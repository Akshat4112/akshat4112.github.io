<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Evaluating LLMs: How Do You Measure a Model's Mind? | Akshat Gupta</title>
<meta name=keywords content="LLM,evaluation,benchmarks,AI,natural-language-processing,deep-learning"><meta name=description content="As large language models (LLMs) become central to search, productivity tools, education, and coding, evaluating them is no longer optional. You have to ask:

Is this model reliable? Accurate? Safe? Biased? Smart enough for my task?
But here&rsquo;s the catch: LLMs are not deterministic functions. They generate free-form text, can be right in one sentence and wrong in the next ‚Äî and vary wildly depending on the prompt.
So how do we evaluate them meaningfully?"><meta name=author content><link rel=canonical href=https://akshat4112.github.io/posts/evaluating-llms/><link crossorigin=anonymous href=/assets/css/stylesheet.e087fd1dc76e73a35ae6d7028ddc1ba41e0131e7f9b3a6e2d019a208e6d6c4b5.css integrity="sha256-4If9Hcduc6Na5tcCjdwbpB4BMef5s6bi0BmiCObWxLU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://akshat4112.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://akshat4112.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://akshat4112.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://akshat4112.github.io/apple-touch-icon.png><link rel=mask-icon href=https://akshat4112.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://akshat4112.github.io/posts/evaluating-llms/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-8YC2E5MW2M"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8YC2E5MW2M")}</script><meta property="og:title" content="Evaluating LLMs: How Do You Measure a Model's Mind?"><meta property="og:description" content="As large language models (LLMs) become central to search, productivity tools, education, and coding, evaluating them is no longer optional. You have to ask:

Is this model reliable? Accurate? Safe? Biased? Smart enough for my task?
But here&rsquo;s the catch: LLMs are not deterministic functions. They generate free-form text, can be right in one sentence and wrong in the next ‚Äî and vary wildly depending on the prompt.
So how do we evaluate them meaningfully?"><meta property="og:type" content="article"><meta property="og:url" content="https://akshat4112.github.io/posts/evaluating-llms/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-15T09:00:00+01:00"><meta property="article:modified_time" content="2024-06-15T09:00:00+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Evaluating LLMs: How Do You Measure a Model's Mind?"><meta name=twitter:description content="As large language models (LLMs) become central to search, productivity tools, education, and coding, evaluating them is no longer optional. You have to ask:

Is this model reliable? Accurate? Safe? Biased? Smart enough for my task?
But here&rsquo;s the catch: LLMs are not deterministic functions. They generate free-form text, can be right in one sentence and wrong in the next ‚Äî and vary wildly depending on the prompt.
So how do we evaluate them meaningfully?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://akshat4112.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Evaluating LLMs: How Do You Measure a Model's Mind?","item":"https://akshat4112.github.io/posts/evaluating-llms/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Evaluating LLMs: How Do You Measure a Model's Mind?","name":"Evaluating LLMs: How Do You Measure a Model\u0027s Mind?","description":"As large language models (LLMs) become central to search, productivity tools, education, and coding, evaluating them is no longer optional. You have to ask:\nIs this model reliable? Accurate? Safe? Biased? Smart enough for my task?\nBut here\u0026rsquo;s the catch: LLMs are not deterministic functions. They generate free-form text, can be right in one sentence and wrong in the next ‚Äî and vary wildly depending on the prompt.\nSo how do we evaluate them meaningfully?\n","keywords":["LLM","evaluation","benchmarks","AI","natural-language-processing","deep-learning"],"articleBody":"As large language models (LLMs) become central to search, productivity tools, education, and coding, evaluating them is no longer optional. You have to ask:\nIs this model reliable? Accurate? Safe? Biased? Smart enough for my task?\nBut here‚Äôs the catch: LLMs are not deterministic functions. They generate free-form text, can be right in one sentence and wrong in the next ‚Äî and vary wildly depending on the prompt.\nSo how do we evaluate them meaningfully?\nüß™ Why Evaluate LLMs? Good evaluation helps answer:\n‚úÖ Is the model aligned with user goals? ‚úÖ Does it generalize to unseen prompts? ‚úÖ Is it factual, helpful, and harmless? ‚úÖ Is it better than baseline or competitor models? Whether you‚Äôre fine-tuning a model, comparing open-source LLMs, or releasing an AI feature ‚Äî you need a systematic way to measure quality.\nüéØ Types of Evaluation There are three main types of evaluation used for LLMs:\n1. Intrinsic Evaluation (automatic) These are computed automatically without human judgment.\nPerplexity: Measures how well a model predicts the next word (lower = better).\nNot ideal for generation tasks, but useful during pretraining.\nBLEU / ROUGE / METEOR: Compare generated output to a reference.\nBest for short-form tasks like translation or summarization.\nBLEU paper\nExact Match / F1 Score: Used in QA tasks with ground truth answers.\nBERTScore: Embedding-based similarity using BERT. Good for semantics.\nüö´ Problem: These scores often fail to capture nuance, creativity, or reasoning.\n2. Extrinsic Evaluation (human-like) This focuses on how LLMs perform in downstream tasks.\nTask success: Did the model complete the task (e.g., booking a flight, answering a tax question)? User satisfaction: Useful in production systems or chatbots. A/B testing: Compare model variants in live usage. Win-rate comparisons: Common in model leaderboards. These are more reflective of real-world performance.\n3. Human Evaluation Still the gold standard for nuanced tasks.\nHuman judges evaluate:\nüåü Relevance üåü Factuality üåü Fluency üåü Helpfulness üåü Harmlessness (toxicity, bias) Usually done via Likert scale or pairwise comparison. Costly, but high-quality.\nüßë‚Äç‚öñÔ∏è Benchmarks for LLMs Some standard benchmarks have emerged:\nMMLU (Massive Multitask Language Understanding)\nCovers math, medicine, law, history ‚Äî tests reasoning over 57 domains.\nHELLASWAG\nCommonsense inference for fill-in-the-blank scenarios.\nTruthfulQA\nMeasures how often LLMs give truthful answers to tricky questions.\nBIG-bench\nCollaborative benchmark of 200+ tasks testing model generalization.\nMT-Bench\nMulti-turn chat evaluation developed by LMSys for Vicuna and Chatbot Arena.\nBonus: Chatbot Arena does live crowd-sourced pairwise model evaluation.\nüìè Common Metrics Metric Use Case Notes Perplexity Pretraining Lower = better BLEU/ROUGE Translation/Summarization Needs reference outputs BERTScore Semantics Works better with long-form tasks Win Rate Pairwise eval Human judges or ranked voting F1 / EM QA tasks Binary metrics, hard to scale GPT-4 Eval Self-evaluation Biased but surprisingly useful üîß Tools for Evaluation OpenAI Evals ‚Äì framework for building evals for GPT lm-eval-harness ‚Äì benchmark open-source LLMs TruLens ‚Äì feedback + eval framework for LLM apps Promptfoo ‚Äì A/B prompt testing tool LangSmith ‚Äì Track, debug, and eval LangChain apps üí¨ My Approach to LLM Evaluation In my own projects (like document Q\u0026A or multi-agent GenAI), I often mix:\nüîç Hard metrics (accuracy, F1) for structured data extraction üß™ Prompt-based unit tests using OpenAI Evals or LangChain üë®‚Äçüë©‚Äçüëß Manual grading for edge cases and critical flows üìä Leaderboards when comparing LLaMA, Mixtral, GPT-4, Claude, etc. For production? Human-in-the-loop testing is key ‚Äî especially for regulated or high-risk domains.\nüß† Final Thoughts Evaluating LLMs isn‚Äôt just a technical problem ‚Äî it‚Äôs a design problem, a UX problem, and a trust problem.\nAs the space matures, we‚Äôll need better automated metrics, transparent benchmarks, and community-driven evaluations.\nUntil then: evaluate early, evaluate often ‚Äî and don‚Äôt trust your LLM until you‚Äôve tested it.\n‚Äî Akshat\n","wordCount":"616","inLanguage":"en","datePublished":"2024-06-15T09:00:00+01:00","dateModified":"2024-06-15T09:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://akshat4112.github.io/posts/evaluating-llms/"},"publisher":{"@type":"Organization","name":"Akshat Gupta","logo":{"@type":"ImageObject","url":"https://akshat4112.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://akshat4112.github.io/ accesskey=h title="Akshat Gupta (Alt + H)">Akshat Gupta</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch></ul></div></div><ul id=menu><li><a href=https://akshat4112.github.io/posts title=Posts><span>Posts</span></a></li><li><a href=https://akshat4112.github.io/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://akshat4112.github.io/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://akshat4112.github.io/events/ title=Events><span>Events</span></a></li><li><a href=https://akshat4112.github.io/about/ title=About><span>About</span></a></li><li><a href="https://drive.google.com/file/d/1Qj6kaZXM40ixUgOAewhZlTj57piWERSw/view?usp=drive_link" title=CV><span>CV</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://akshat4112.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://akshat4112.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Evaluating LLMs: How Do You Measure a Model's Mind?</h1><div class=post-meta><span title='2024-06-15 09:00:00 +0100 +0100'>June 15, 2024</span>&nbsp;¬∑&nbsp;3 min</div></header><div class=post-content><p>As large language models (LLMs) become central to search, productivity tools, education, and coding, <strong>evaluating them</strong> is no longer optional. You <em>have</em> to ask:</p><blockquote><p>Is this model reliable? Accurate? Safe? Biased? Smart enough for my task?</p></blockquote><p>But here&rsquo;s the catch: LLMs are <em>not</em> deterministic functions. They generate free-form text, can be right in one sentence and wrong in the next ‚Äî and vary wildly depending on the prompt.</p><p>So how do we evaluate them meaningfully?</p><hr><h2 id=-why-evaluate-llms>üß™ Why Evaluate LLMs?<a hidden class=anchor aria-hidden=true href=#-why-evaluate-llms>#</a></h2><p>Good evaluation helps answer:</p><ul><li>‚úÖ Is the model <strong>aligned</strong> with user goals?</li><li>‚úÖ Does it <strong>generalize</strong> to unseen prompts?</li><li>‚úÖ Is it <strong>factual</strong>, <strong>helpful</strong>, and <strong>harmless</strong>?</li><li>‚úÖ Is it better than baseline or competitor models?</li></ul><p>Whether you‚Äôre fine-tuning a model, comparing open-source LLMs, or releasing an AI feature ‚Äî you need <strong>a systematic way to measure quality</strong>.</p><hr><h2 id=-types-of-evaluation>üéØ Types of Evaluation<a hidden class=anchor aria-hidden=true href=#-types-of-evaluation>#</a></h2><p>There are three main types of evaluation used for LLMs:</p><h3 id=1-intrinsic-evaluation-automatic>1. <strong>Intrinsic Evaluation</strong> (automatic)<a hidden class=anchor aria-hidden=true href=#1-intrinsic-evaluation-automatic>#</a></h3><p>These are computed automatically without human judgment.</p><ul><li><p><strong>Perplexity</strong>: Measures how well a model predicts the next word (lower = better).<br>Not ideal for generation tasks, but useful during pretraining.</p></li><li><p><strong>BLEU / ROUGE / METEOR</strong>: Compare generated output to a reference.<br>Best for short-form tasks like translation or summarization.<br><a href=https://aclanthology.org/P02-1040/>BLEU paper</a></p></li><li><p><strong>Exact Match / F1 Score</strong>: Used in QA tasks with ground truth answers.</p></li><li><p><strong>BERTScore</strong>: Embedding-based similarity using BERT. Good for semantics.</p></li></ul><blockquote><p>üö´ Problem: These scores often fail to capture nuance, creativity, or reasoning.</p></blockquote><hr><h3 id=2-extrinsic-evaluation-human-like>2. <strong>Extrinsic Evaluation</strong> (human-like)<a hidden class=anchor aria-hidden=true href=#2-extrinsic-evaluation-human-like>#</a></h3><p>This focuses on how LLMs perform in downstream tasks.</p><ul><li><strong>Task success</strong>: Did the model complete the task (e.g., booking a flight, answering a tax question)?</li><li><strong>User satisfaction</strong>: Useful in production systems or chatbots.</li><li><strong>A/B testing</strong>: Compare model variants in live usage.</li><li><strong>Win-rate comparisons</strong>: Common in model leaderboards.</li></ul><p>These are more reflective of real-world performance.</p><hr><h3 id=3-human-evaluation>3. <strong>Human Evaluation</strong><a hidden class=anchor aria-hidden=true href=#3-human-evaluation>#</a></h3><p>Still the gold standard for nuanced tasks.</p><p>Human judges evaluate:</p><ul><li>üåü Relevance</li><li>üåü Factuality</li><li>üåü Fluency</li><li>üåü Helpfulness</li><li>üåü Harmlessness (toxicity, bias)</li></ul><p>Usually done via Likert scale or pairwise comparison. Costly, but high-quality.</p><hr><h2 id=-benchmarks-for-llms>üßë‚Äç‚öñÔ∏è Benchmarks for LLMs<a hidden class=anchor aria-hidden=true href=#-benchmarks-for-llms>#</a></h2><p>Some standard benchmarks have emerged:</p><ul><li><p><a href=https://github.com/hendrycks/test><strong>MMLU</strong></a> (Massive Multitask Language Understanding)<br>Covers math, medicine, law, history ‚Äî tests reasoning over 57 domains.</p></li><li><p><a href=https://rowanzellers.com/hellaswag/><strong>HELLASWAG</strong></a><br>Commonsense inference for fill-in-the-blank scenarios.</p></li><li><p><a href=https://arxiv.org/abs/2109.07958><strong>TruthfulQA</strong></a><br>Measures how often LLMs give <em>truthful</em> answers to tricky questions.</p></li><li><p><a href=https://github.com/google/BIG-bench><strong>BIG-bench</strong></a><br>Collaborative benchmark of 200+ tasks testing model generalization.</p></li><li><p><a href=https://github.com/lm-sys/FastChat/blob/main/docs/evaluation.md#mt-bench><strong>MT-Bench</strong></a><br>Multi-turn chat evaluation developed by LMSys for Vicuna and Chatbot Arena.</p></li></ul><blockquote><p>Bonus: <a href=https://chat.lmsys.org><strong>Chatbot Arena</strong></a> does live <strong>crowd-sourced pairwise</strong> model evaluation.</p></blockquote><hr><h2 id=-common-metrics>üìè Common Metrics<a hidden class=anchor aria-hidden=true href=#-common-metrics>#</a></h2><table><thead><tr><th>Metric</th><th>Use Case</th><th>Notes</th></tr></thead><tbody><tr><td>Perplexity</td><td>Pretraining</td><td>Lower = better</td></tr><tr><td>BLEU/ROUGE</td><td>Translation/Summarization</td><td>Needs reference outputs</td></tr><tr><td>BERTScore</td><td>Semantics</td><td>Works better with long-form tasks</td></tr><tr><td>Win Rate</td><td>Pairwise eval</td><td>Human judges or ranked voting</td></tr><tr><td>F1 / EM</td><td>QA tasks</td><td>Binary metrics, hard to scale</td></tr><tr><td>GPT-4 Eval</td><td>Self-evaluation</td><td>Biased but surprisingly useful</td></tr></tbody></table><hr><h2 id=-tools-for-evaluation>üîß Tools for Evaluation<a hidden class=anchor aria-hidden=true href=#-tools-for-evaluation>#</a></h2><ul><li><a href=https://github.com/openai/evals><strong>OpenAI Evals</strong></a> ‚Äì framework for building evals for GPT</li><li><a href=https://github.com/EleutherAI/lm-evaluation-harness><strong>lm-eval-harness</strong></a> ‚Äì benchmark open-source LLMs</li><li><a href=https://github.com/truera/trulens><strong>TruLens</strong></a> ‚Äì feedback + eval framework for LLM apps</li><li><a href=https://github.com/promptfoo/promptfoo><strong>Promptfoo</strong></a> ‚Äì A/B prompt testing tool</li><li><a href=https://www.langchain.com/langsmith><strong>LangSmith</strong></a> ‚Äì Track, debug, and eval LangChain apps</li></ul><hr><h2 id=-my-approach-to-llm-evaluation>üí¨ My Approach to LLM Evaluation<a hidden class=anchor aria-hidden=true href=#-my-approach-to-llm-evaluation>#</a></h2><p>In my own projects (like document Q&amp;A or multi-agent GenAI), I often mix:</p><ul><li>üîç <strong>Hard metrics</strong> (accuracy, F1) for structured data extraction</li><li>üß™ <strong>Prompt-based unit tests</strong> using <code>OpenAI Evals</code> or <code>LangChain</code></li><li>üë®‚Äçüë©‚Äçüëß <strong>Manual grading</strong> for edge cases and critical flows</li><li>üìä <strong>Leaderboards</strong> when comparing LLaMA, Mixtral, GPT-4, Claude, etc.</li></ul><p>For production? <strong>Human-in-the-loop testing</strong> is key ‚Äî especially for regulated or high-risk domains.</p><hr><h2 id=-final-thoughts>üß† Final Thoughts<a hidden class=anchor aria-hidden=true href=#-final-thoughts>#</a></h2><p>Evaluating LLMs isn‚Äôt just a technical problem ‚Äî it‚Äôs a design problem, a UX problem, and a trust problem.</p><p>As the space matures, we‚Äôll need <strong>better automated metrics</strong>, <strong>transparent benchmarks</strong>, and <strong>community-driven evaluations</strong>.</p><p>Until then: evaluate early, evaluate often ‚Äî and don‚Äôt trust your LLM until you‚Äôve tested it.</p><p>‚Äî <strong>Akshat</strong></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://akshat4112.github.io/tags/llm/>LLM</a></li><li><a href=https://akshat4112.github.io/tags/evaluation/>Evaluation</a></li><li><a href=https://akshat4112.github.io/tags/benchmarks/>Benchmarks</a></li><li><a href=https://akshat4112.github.io/tags/ai/>AI</a></li><li><a href=https://akshat4112.github.io/tags/natural-language-processing/>Natural-Language-Processing</a></li><li><a href=https://akshat4112.github.io/tags/deep-learning/>Deep-Learning</a></li></ul><nav class=paginav><a class=prev href=https://akshat4112.github.io/posts/prompt-engineering/><span class=title>¬´ Prev</span><br><span>Prompt Engineering: The Art of Talking to AI</span>
</a><a class=next href=https://akshat4112.github.io/posts/rag-and-llms/><span class=title>Next ¬ª</span><br><span>RAG and LLMs: Teaching Large Models to Use External Knowledge</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Evaluating LLMs: How Do You Measure a Model's Mind? on x" href="https://x.com/intent/tweet/?text=Evaluating%20LLMs%3a%20How%20Do%20You%20Measure%20a%20Model%27s%20Mind%3f&amp;url=https%3a%2f%2fakshat4112.github.io%2fposts%2fevaluating-llms%2f&amp;hashtags=LLM%2cevaluation%2cbenchmarks%2cAI%2cnatural-language-processing%2cdeep-learning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Evaluating LLMs: How Do You Measure a Model's Mind? on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fakshat4112.github.io%2fposts%2fevaluating-llms%2f&amp;title=Evaluating%20LLMs%3a%20How%20Do%20You%20Measure%20a%20Model%27s%20Mind%3f&amp;summary=Evaluating%20LLMs%3a%20How%20Do%20You%20Measure%20a%20Model%27s%20Mind%3f&amp;source=https%3a%2f%2fakshat4112.github.io%2fposts%2fevaluating-llms%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Evaluating LLMs: How Do You Measure a Model's Mind? on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fakshat4112.github.io%2fposts%2fevaluating-llms%2f&title=Evaluating%20LLMs%3a%20How%20Do%20You%20Measure%20a%20Model%27s%20Mind%3f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Evaluating LLMs: How Do You Measure a Model's Mind? on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fakshat4112.github.io%2fposts%2fevaluating-llms%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Evaluating LLMs: How Do You Measure a Model's Mind? on whatsapp" href="https://api.whatsapp.com/send?text=Evaluating%20LLMs%3a%20How%20Do%20You%20Measure%20a%20Model%27s%20Mind%3f%20-%20https%3a%2f%2fakshat4112.github.io%2fposts%2fevaluating-llms%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Evaluating LLMs: How Do You Measure a Model's Mind? on telegram" href="https://telegram.me/share/url?text=Evaluating%20LLMs%3a%20How%20Do%20You%20Measure%20a%20Model%27s%20Mind%3f&amp;url=https%3a%2f%2fakshat4112.github.io%2fposts%2fevaluating-llms%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Evaluating LLMs: How Do You Measure a Model's Mind? on ycombinator" href="https://news.ycombinator.com/submitlink?t=Evaluating%20LLMs%3a%20How%20Do%20You%20Measure%20a%20Model%27s%20Mind%3f&u=https%3a%2f%2fakshat4112.github.io%2fposts%2fevaluating-llms%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://akshat4112.github.io/>Akshat Gupta</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>