<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Model Extraction Attacks: How Hackers Steal AI Models | Akshat Gupta</title>
<meta name=keywords content="AI-security,model-extraction,machine-learning,cybersecurity,LLMs,deep-learning,AI"><meta name=description content="In the world of machine learning, especially with the rise of large language models (LLMs) and deep neural networks, model extraction attacks are a growing concern. These attacks aim to replicate the behavior of a machine learning model by querying it and then using the responses to reverse-engineer its underlying architecture and parameters.
What is a Model Extraction Attack?
A model extraction attack occurs when an adversary tries to replicate a machine learning model by making repeated queries to it and analyzing its responses. The goal of the attacker is to create a new model that mimics the target model’s functionality, often without direct access to its architecture or parameters."><meta name=author content><link rel=canonical href=https://akshat4112.github.io/posts/model-extraction-attacks/><link crossorigin=anonymous href=/assets/css/stylesheet.e087fd1dc76e73a35ae6d7028ddc1ba41e0131e7f9b3a6e2d019a208e6d6c4b5.css integrity="sha256-4If9Hcduc6Na5tcCjdwbpB4BMef5s6bi0BmiCObWxLU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://akshat4112.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://akshat4112.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://akshat4112.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://akshat4112.github.io/apple-touch-icon.png><link rel=mask-icon href=https://akshat4112.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://akshat4112.github.io/posts/model-extraction-attacks/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-8YC2E5MW2M"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8YC2E5MW2M")}</script><meta property="og:title" content="Model Extraction Attacks: How Hackers Steal AI Models"><meta property="og:description" content="In the world of machine learning, especially with the rise of large language models (LLMs) and deep neural networks, model extraction attacks are a growing concern. These attacks aim to replicate the behavior of a machine learning model by querying it and then using the responses to reverse-engineer its underlying architecture and parameters.
What is a Model Extraction Attack?
A model extraction attack occurs when an adversary tries to replicate a machine learning model by making repeated queries to it and analyzing its responses. The goal of the attacker is to create a new model that mimics the target model’s functionality, often without direct access to its architecture or parameters."><meta property="og:type" content="article"><meta property="og:url" content="https://akshat4112.github.io/posts/model-extraction-attacks/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-15T09:00:00+01:00"><meta property="article:modified_time" content="2024-09-15T09:00:00+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Model Extraction Attacks: How Hackers Steal AI Models"><meta name=twitter:description content="In the world of machine learning, especially with the rise of large language models (LLMs) and deep neural networks, model extraction attacks are a growing concern. These attacks aim to replicate the behavior of a machine learning model by querying it and then using the responses to reverse-engineer its underlying architecture and parameters.
What is a Model Extraction Attack?
A model extraction attack occurs when an adversary tries to replicate a machine learning model by making repeated queries to it and analyzing its responses. The goal of the attacker is to create a new model that mimics the target model’s functionality, often without direct access to its architecture or parameters."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://akshat4112.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Model Extraction Attacks: How Hackers Steal AI Models","item":"https://akshat4112.github.io/posts/model-extraction-attacks/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Model Extraction Attacks: How Hackers Steal AI Models","name":"Model Extraction Attacks: How Hackers Steal AI Models","description":"In the world of machine learning, especially with the rise of large language models (LLMs) and deep neural networks, model extraction attacks are a growing concern. These attacks aim to replicate the behavior of a machine learning model by querying it and then using the responses to reverse-engineer its underlying architecture and parameters.\nWhat is a Model Extraction Attack? A model extraction attack occurs when an adversary tries to replicate a machine learning model by making repeated queries to it and analyzing its responses. The goal of the attacker is to create a new model that mimics the target model’s functionality, often without direct access to its architecture or parameters.\n","keywords":["AI-security","model-extraction","machine-learning","cybersecurity","LLMs","deep-learning","AI"],"articleBody":"In the world of machine learning, especially with the rise of large language models (LLMs) and deep neural networks, model extraction attacks are a growing concern. These attacks aim to replicate the behavior of a machine learning model by querying it and then using the responses to reverse-engineer its underlying architecture and parameters.\nWhat is a Model Extraction Attack? A model extraction attack occurs when an adversary tries to replicate a machine learning model by making repeated queries to it and analyzing its responses. The goal of the attacker is to create a new model that mimics the target model’s functionality, often without direct access to its architecture or parameters.\nOnce the adversary has successfully replicated a model, they can use it for various malicious purposes, including:\nStealing intellectual property: By extracting a proprietary model, attackers can use it to bypass legal or technical protections or even re-sell it. Bypassing security measures: An extracted model might reveal vulnerabilities or ways to exploit the system, allowing attackers to bypass security checks. Creating competitive advantages: Competitors can replicate expensive and sophisticated models without having to invest in training them from scratch. Types of Model Extraction Attacks There are two primary categories of model extraction attacks:\n1. Black-box Attacks In a black-box attack, the adversary only has access to the model’s input-output behavior, without any information about its internals. The attacker can make queries to the model and receive outputs, which they will use to infer the model’s behavior and architecture.\nExample: Suppose an attacker wants to replicate a language model like GPT. They can send in a variety of text prompts to the model, such as “What’s the weather like today?” or “Tell me a joke,” and observe the responses. After making enough queries, the attacker can train their own model on these input-output pairs, essentially trying to reproduce the target model’s performance.\nTechniques used in Black-box Attacks:\nQuerying the model extensively: This is usually the most straightforward approach. The attacker queries the model with diverse inputs to gather enough data to approximate the model’s behavior. Model Distillation: The adversary can train a smaller surrogate model using the same input-output pairs. Although the extracted model will not match the target exactly, it can still replicate much of the functionality. Real-World Example: Researchers from the University of California, Berkeley demonstrated a black-box attack on image classification models. They used a model distillation technique where they queried a black-box image classifier (like Google Vision API) with thousands of images, extracting a surrogate model that performed similarly to the original model on classification tasks.\n2. White-box Attacks In a white-box attack, the adversary has full access to the model’s architecture, weights, and sometimes even its training data. This gives them an advantage in replicating the model, as they can directly inspect its components and behaviors.\nExample: If an attacker gains access to a model’s source code or API endpoint (e.g., through a vulnerable cloud service), they can directly extract information about the model’s structure. This could include its layers, weights, and biases, making it much easier to create a replica.\nTechniques used in White-box Attacks:\nExploiting exposed models: If a company or service exposes their model without adequate protection (like an open-source model or poorly secured API), an attacker can directly replicate it. Model stealing via backdoors: Some attackers try to inject vulnerabilities into the model itself that would allow them to extract its parameters without permission. Real-World Example: In NIPS 2016, researchers successfully conducted a white-box model extraction attack on a neural network model by reverse-engineering the model architecture and retraining a copy of the model on their own data. This demonstrated the feasibility of stealing a model from an exposed API.\nSteps Involved in Model Extraction Attacks Step 1: Querying the Target Model The adversary typically starts by querying the target model. In the case of black-box attacks, they don’t know the internal structure of the model, so they send a variety of queries, often including edge cases and adversarial inputs, to collect a wide range of responses.\nExample Implementation:\nimport requests def query_model(input_text): response = requests.post('https://example.com/predict', data={'input': input_text}) return response.json() queries = [ \"What is 2+2?\", \"Tell me a story about a dragon.\", \"What is the capital of France?\" ] responses = [query_model(query) for query in queries] In this case, the attacker collects responses from the target model and stores them for further analysis.\nStep 2: Analyzing the Responses After gathering enough input-output pairs, the attacker will analyze the responses. They may look for patterns or anomalies that help them understand the model’s decision-making process.\nFor example, the attacker might notice that the model tends to classify certain types of input in a specific way, suggesting a particular feature in the underlying architecture.\nStep 3: Rebuilding the Model In the final step, the attacker will attempt to train a new model using the gathered data. This process involves feeding the input-output pairs into a new model and adjusting its parameters until the model closely replicates the behavior of the target model.\nExample Implementation:\nfrom sklearn.neural_network import MLPClassifier # Example: Train a simple MLP classifier on the extracted data X_train = [[1, 2], [2, 3], [3, 4]] # Example inputs y_train = [0, 1, 1] # Example outputs (targets) model = MLPClassifier(hidden_layer_sizes=(10,)) model.fit(X_train, y_train) # Predict on new data model.predict([[4, 5]]) This new model trained on the attacker’s collected data will likely approximate the behavior of the original model.\nRisks and Implications The risks posed by model extraction attacks are vast:\nIntellectual Property Theft: Large companies invest a lot in developing machine learning models, and model extraction attacks make it easier for malicious actors to replicate their models, potentially leading to loss of competitive advantage. Security Vulnerabilities: Once an attacker has replicated a model, they could use it to exploit weaknesses or gain unauthorized access to sensitive data, especially if the model is used in mission-critical systems like finance or healthcare. Reduction in Trust: If model extraction attacks become more prevalent, users may lose trust in machine learning systems, fearing that adversaries could easily replicate and misuse models. Mitigating Model Extraction Attacks 1. Limit Query Access Limiting the number of queries an external party can make to a model is a simple yet effective measure. Implementing rate limiting, CAPTCHA, or query restrictions can prevent an adversary from gathering enough data to replicate the model.\nImplementation Example:\nimport time from functools import wraps def limit_queries(rate_limit): def decorator(func): last_called = [0.0] @wraps(func) def wrapped(*args, **kwargs): elapsed = time.time() - last_called[0] if elapsed \u003c rate_limit: time.sleep(rate_limit - elapsed) last_called[0] = time.time() return func(*args, **kwargs) return wrapped return decorator @limit_queries(1) # Only allow one query per second def query_model(input_text): response = requests.post('https://example.com/predict', data={'input': input_text}) return response.json() 2. Model Watermarking Watermarking involves embedding unique markers within a model’s behavior. If an attacker replicates the model, these markers can be used to track and prove ownership.\n3. Obfuscating Model Outputs To make it harder for attackers to learn from the model’s behavior, you can introduce noise into the model’s outputs, making them less predictable and more difficult to replicate.\n4. Differential Privacy Differential privacy techniques can be applied to the model to ensure that individual data points cannot be reverse-engineered from the model’s responses. This reduces the effectiveness of model extraction attacks, as the model will not reveal sensitive information about specific data points.\nFinal Thoughts Model extraction attacks are a growing threat in the world of AI and machine learning. As models become more powerful and are deployed at scale, it’s crucial to understand the risks and employ countermeasures to protect intellectual property and secure sensitive systems.\nBy implementing appropriate defenses, including query limitations, model watermarking, and differential privacy, we can reduce the likelihood of successful model extraction and ensure that the benefits of AI are not overshadowed by malicious exploitation.\nStay tuned for more posts where we dive deeper into defense techniques and specific case studies.\n— Akshat\n","wordCount":"1321","inLanguage":"en","datePublished":"2024-09-15T09:00:00+01:00","dateModified":"2024-09-15T09:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://akshat4112.github.io/posts/model-extraction-attacks/"},"publisher":{"@type":"Organization","name":"Akshat Gupta","logo":{"@type":"ImageObject","url":"https://akshat4112.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://akshat4112.github.io/ accesskey=h title="Akshat Gupta (Alt + H)">Akshat Gupta</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch></ul></div></div><ul id=menu><li><a href=https://akshat4112.github.io/posts title=Posts><span>Posts</span></a></li><li><a href=https://akshat4112.github.io/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://akshat4112.github.io/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://akshat4112.github.io/events/ title=Events><span>Events</span></a></li><li><a href=https://akshat4112.github.io/about/ title=About><span>About</span></a></li><li><a href="https://drive.google.com/file/d/1Qj6kaZXM40ixUgOAewhZlTj57piWERSw/view?usp=drive_link" title=CV><span>CV</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://akshat4112.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://akshat4112.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Model Extraction Attacks: How Hackers Steal AI Models</h1><div class=post-meta><span title='2024-09-15 09:00:00 +0100 +0100'>September 15, 2024</span>&nbsp;·&nbsp;7 min</div></header><div class=post-content><p>In the world of machine learning, especially with the rise of large language models (LLMs) and deep neural networks, model extraction attacks are a growing concern. These attacks aim to replicate the behavior of a machine learning model by querying it and then using the responses to reverse-engineer its underlying architecture and parameters.</p><h2 id=what-is-a-model-extraction-attack>What is a Model Extraction Attack?<a hidden class=anchor aria-hidden=true href=#what-is-a-model-extraction-attack>#</a></h2><p>A <strong>model extraction attack</strong> occurs when an adversary tries to replicate a machine learning model by making repeated queries to it and analyzing its responses. The goal of the attacker is to create a new model that mimics the target model’s functionality, often without direct access to its architecture or parameters.</p><p>Once the adversary has successfully replicated a model, they can use it for various malicious purposes, including:</p><ul><li><strong>Stealing intellectual property</strong>: By extracting a proprietary model, attackers can use it to bypass legal or technical protections or even re-sell it.</li><li><strong>Bypassing security measures</strong>: An extracted model might reveal vulnerabilities or ways to exploit the system, allowing attackers to bypass security checks.</li><li><strong>Creating competitive advantages</strong>: Competitors can replicate expensive and sophisticated models without having to invest in training them from scratch.</li></ul><h2 id=types-of-model-extraction-attacks>Types of Model Extraction Attacks<a hidden class=anchor aria-hidden=true href=#types-of-model-extraction-attacks>#</a></h2><p>There are two primary categories of model extraction attacks:</p><h3 id=1-black-box-attacks>1. <strong>Black-box Attacks</strong><a hidden class=anchor aria-hidden=true href=#1-black-box-attacks>#</a></h3><p>In a <strong>black-box</strong> attack, the adversary only has access to the model’s input-output behavior, without any information about its internals. The attacker can make queries to the model and receive outputs, which they will use to infer the model’s behavior and architecture.</p><h4 id=example>Example:<a hidden class=anchor aria-hidden=true href=#example>#</a></h4><p>Suppose an attacker wants to replicate a language model like GPT. They can send in a variety of text prompts to the model, such as &ldquo;What’s the weather like today?&rdquo; or &ldquo;Tell me a joke,&rdquo; and observe the responses. After making enough queries, the attacker can train their own model on these input-output pairs, essentially trying to reproduce the target model&rsquo;s performance.</p><p><strong>Techniques used in Black-box Attacks</strong>:</p><ul><li><strong>Querying the model extensively</strong>: This is usually the most straightforward approach. The attacker queries the model with diverse inputs to gather enough data to approximate the model’s behavior.</li><li><strong>Model Distillation</strong>: The adversary can train a smaller surrogate model using the same input-output pairs. Although the extracted model will not match the target exactly, it can still replicate much of the functionality.</li></ul><h4 id=real-world-example>Real-World Example:<a hidden class=anchor aria-hidden=true href=#real-world-example>#</a></h4><p>Researchers from the <strong>University of California, Berkeley</strong> demonstrated a black-box attack on image classification models. They used a model distillation technique where they queried a black-box image classifier (like Google Vision API) with thousands of images, extracting a surrogate model that performed similarly to the original model on classification tasks.</p><h3 id=2-white-box-attacks>2. <strong>White-box Attacks</strong><a hidden class=anchor aria-hidden=true href=#2-white-box-attacks>#</a></h3><p>In a <strong>white-box</strong> attack, the adversary has full access to the model’s architecture, weights, and sometimes even its training data. This gives them an advantage in replicating the model, as they can directly inspect its components and behaviors.</p><h4 id=example-1>Example:<a hidden class=anchor aria-hidden=true href=#example-1>#</a></h4><p>If an attacker gains access to a model&rsquo;s source code or API endpoint (e.g., through a vulnerable cloud service), they can directly extract information about the model’s structure. This could include its layers, weights, and biases, making it much easier to create a replica.</p><p><strong>Techniques used in White-box Attacks</strong>:</p><ul><li><strong>Exploiting exposed models</strong>: If a company or service exposes their model without adequate protection (like an open-source model or poorly secured API), an attacker can directly replicate it.</li><li><strong>Model stealing via backdoors</strong>: Some attackers try to inject vulnerabilities into the model itself that would allow them to extract its parameters without permission.</li></ul><h4 id=real-world-example-1>Real-World Example:<a hidden class=anchor aria-hidden=true href=#real-world-example-1>#</a></h4><p>In <strong>NIPS 2016</strong>, researchers successfully conducted a white-box model extraction attack on a neural network model by reverse-engineering the model architecture and retraining a copy of the model on their own data. This demonstrated the feasibility of stealing a model from an exposed API.</p><h2 id=steps-involved-in-model-extraction-attacks>Steps Involved in Model Extraction Attacks<a hidden class=anchor aria-hidden=true href=#steps-involved-in-model-extraction-attacks>#</a></h2><h3 id=step-1-querying-the-target-model>Step 1: Querying the Target Model<a hidden class=anchor aria-hidden=true href=#step-1-querying-the-target-model>#</a></h3><p>The adversary typically starts by querying the target model. In the case of black-box attacks, they don’t know the internal structure of the model, so they send a variety of queries, often including edge cases and adversarial inputs, to collect a wide range of responses.</p><p><strong>Example Implementation</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>query_model</span><span class=p>(</span><span class=n>input_text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>post</span><span class=p>(</span><span class=s1>&#39;https://example.com/predict&#39;</span><span class=p>,</span> <span class=n>data</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;input&#39;</span><span class=p>:</span> <span class=n>input_text</span><span class=p>})</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>response</span><span class=o>.</span><span class=n>json</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>queries</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;What is 2+2?&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;Tell me a story about a dragon.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;What is the capital of France?&#34;</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>responses</span> <span class=o>=</span> <span class=p>[</span><span class=n>query_model</span><span class=p>(</span><span class=n>query</span><span class=p>)</span> <span class=k>for</span> <span class=n>query</span> <span class=ow>in</span> <span class=n>queries</span><span class=p>]</span>
</span></span></code></pre></div><p>In this case, the attacker collects responses from the target model and stores them for further analysis.</p><h3 id=step-2-analyzing-the-responses>Step 2: Analyzing the Responses<a hidden class=anchor aria-hidden=true href=#step-2-analyzing-the-responses>#</a></h3><p>After gathering enough input-output pairs, the attacker will analyze the responses. They may look for patterns or anomalies that help them understand the model’s decision-making process.</p><p>For example, the attacker might notice that the model tends to classify certain types of input in a specific way, suggesting a particular feature in the underlying architecture.</p><h3 id=step-3-rebuilding-the-model>Step 3: Rebuilding the Model<a hidden class=anchor aria-hidden=true href=#step-3-rebuilding-the-model>#</a></h3><p>In the final step, the attacker will attempt to train a new model using the gathered data. This process involves feeding the input-output pairs into a new model and adjusting its parameters until the model closely replicates the behavior of the target model.</p><p><strong>Example Implementation</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.neural_network</span> <span class=kn>import</span> <span class=n>MLPClassifier</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Example: Train a simple MLP classifier on the extracted data</span>
</span></span><span class=line><span class=cl><span class=n>X_train</span> <span class=o>=</span> <span class=p>[[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>]]</span>  <span class=c1># Example inputs</span>
</span></span><span class=line><span class=cl><span class=n>y_train</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span>  <span class=c1># Example outputs (targets)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>MLPClassifier</span><span class=p>(</span><span class=n>hidden_layer_sizes</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,))</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Predict on new data</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>predict</span><span class=p>([[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>]])</span>
</span></span></code></pre></div><p>This new model trained on the attacker’s collected data will likely approximate the behavior of the original model.</p><h2 id=risks-and-implications>Risks and Implications<a hidden class=anchor aria-hidden=true href=#risks-and-implications>#</a></h2><p>The risks posed by model extraction attacks are vast:</p><ul><li><strong>Intellectual Property Theft</strong>: Large companies invest a lot in developing machine learning models, and model extraction attacks make it easier for malicious actors to replicate their models, potentially leading to loss of competitive advantage.</li><li><strong>Security Vulnerabilities</strong>: Once an attacker has replicated a model, they could use it to exploit weaknesses or gain unauthorized access to sensitive data, especially if the model is used in mission-critical systems like finance or healthcare.</li><li><strong>Reduction in Trust</strong>: If model extraction attacks become more prevalent, users may lose trust in machine learning systems, fearing that adversaries could easily replicate and misuse models.</li></ul><h2 id=mitigating-model-extraction-attacks>Mitigating Model Extraction Attacks<a hidden class=anchor aria-hidden=true href=#mitigating-model-extraction-attacks>#</a></h2><h3 id=1-limit-query-access>1. <strong>Limit Query Access</strong><a hidden class=anchor aria-hidden=true href=#1-limit-query-access>#</a></h3><p>Limiting the number of queries an external party can make to a model is a simple yet effective measure. Implementing rate limiting, CAPTCHA, or query restrictions can prevent an adversary from gathering enough data to replicate the model.</p><p><strong>Implementation Example</strong>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>functools</span> <span class=kn>import</span> <span class=n>wraps</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>limit_queries</span><span class=p>(</span><span class=n>rate_limit</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>decorator</span><span class=p>(</span><span class=n>func</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>last_called</span> <span class=o>=</span> <span class=p>[</span><span class=mf>0.0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nd>@wraps</span><span class=p>(</span><span class=n>func</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>wrapped</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>elapsed</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=n>last_called</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>elapsed</span> <span class=o>&lt;</span> <span class=n>rate_limit</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>time</span><span class=o>.</span><span class=n>sleep</span><span class=p>(</span><span class=n>rate_limit</span> <span class=o>-</span> <span class=n>elapsed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>last_called</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>func</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>wrapped</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>decorator</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@limit_queries</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># Only allow one query per second</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>query_model</span><span class=p>(</span><span class=n>input_text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>post</span><span class=p>(</span><span class=s1>&#39;https://example.com/predict&#39;</span><span class=p>,</span> <span class=n>data</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;input&#39;</span><span class=p>:</span> <span class=n>input_text</span><span class=p>})</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>response</span><span class=o>.</span><span class=n>json</span><span class=p>()</span>
</span></span></code></pre></div><h3 id=2-model-watermarking>2. <strong>Model Watermarking</strong><a hidden class=anchor aria-hidden=true href=#2-model-watermarking>#</a></h3><p>Watermarking involves embedding unique markers within a model’s behavior. If an attacker replicates the model, these markers can be used to track and prove ownership.</p><h3 id=3-obfuscating-model-outputs>3. <strong>Obfuscating Model Outputs</strong><a hidden class=anchor aria-hidden=true href=#3-obfuscating-model-outputs>#</a></h3><p>To make it harder for attackers to learn from the model’s behavior, you can introduce noise into the model’s outputs, making them less predictable and more difficult to replicate.</p><h3 id=4-differential-privacy>4. <strong>Differential Privacy</strong><a hidden class=anchor aria-hidden=true href=#4-differential-privacy>#</a></h3><p>Differential privacy techniques can be applied to the model to ensure that individual data points cannot be reverse-engineered from the model’s responses. This reduces the effectiveness of model extraction attacks, as the model will not reveal sensitive information about specific data points.</p><h2 id=final-thoughts>Final Thoughts<a hidden class=anchor aria-hidden=true href=#final-thoughts>#</a></h2><p>Model extraction attacks are a growing threat in the world of AI and machine learning. As models become more powerful and are deployed at scale, it’s crucial to understand the risks and employ countermeasures to protect intellectual property and secure sensitive systems.</p><p>By implementing appropriate defenses, including query limitations, model watermarking, and differential privacy, we can reduce the likelihood of successful model extraction and ensure that the benefits of AI are not overshadowed by malicious exploitation.</p><p>Stay tuned for more posts where we dive deeper into defense techniques and specific case studies.</p><p>— Akshat</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://akshat4112.github.io/tags/ai-security/>AI-Security</a></li><li><a href=https://akshat4112.github.io/tags/model-extraction/>Model-Extraction</a></li><li><a href=https://akshat4112.github.io/tags/machine-learning/>Machine-Learning</a></li><li><a href=https://akshat4112.github.io/tags/cybersecurity/>Cybersecurity</a></li><li><a href=https://akshat4112.github.io/tags/llms/>LLMs</a></li><li><a href=https://akshat4112.github.io/tags/deep-learning/>Deep-Learning</a></li><li><a href=https://akshat4112.github.io/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://akshat4112.github.io/posts/attention-in-transformers/><span class=title>« Prev</span><br><span>Understanding Attention in Transformers: The Core of Modern NLP</span>
</a><a class=next href=https://akshat4112.github.io/posts/speaker-anonymization/><span class=title>Next »</span><br><span>Speaker Anonymization: Protecting Voice Identity in the AI Era</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Model Extraction Attacks: How Hackers Steal AI Models on x" href="https://x.com/intent/tweet/?text=Model%20Extraction%20Attacks%3a%20How%20Hackers%20Steal%20AI%20Models&amp;url=https%3a%2f%2fakshat4112.github.io%2fposts%2fmodel-extraction-attacks%2f&amp;hashtags=AI-security%2cmodel-extraction%2cmachine-learning%2ccybersecurity%2cLLMs%2cdeep-learning%2cAI"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Model Extraction Attacks: How Hackers Steal AI Models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fakshat4112.github.io%2fposts%2fmodel-extraction-attacks%2f&amp;title=Model%20Extraction%20Attacks%3a%20How%20Hackers%20Steal%20AI%20Models&amp;summary=Model%20Extraction%20Attacks%3a%20How%20Hackers%20Steal%20AI%20Models&amp;source=https%3a%2f%2fakshat4112.github.io%2fposts%2fmodel-extraction-attacks%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Model Extraction Attacks: How Hackers Steal AI Models on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fakshat4112.github.io%2fposts%2fmodel-extraction-attacks%2f&title=Model%20Extraction%20Attacks%3a%20How%20Hackers%20Steal%20AI%20Models"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Model Extraction Attacks: How Hackers Steal AI Models on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fakshat4112.github.io%2fposts%2fmodel-extraction-attacks%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Model Extraction Attacks: How Hackers Steal AI Models on whatsapp" href="https://api.whatsapp.com/send?text=Model%20Extraction%20Attacks%3a%20How%20Hackers%20Steal%20AI%20Models%20-%20https%3a%2f%2fakshat4112.github.io%2fposts%2fmodel-extraction-attacks%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Model Extraction Attacks: How Hackers Steal AI Models on telegram" href="https://telegram.me/share/url?text=Model%20Extraction%20Attacks%3a%20How%20Hackers%20Steal%20AI%20Models&amp;url=https%3a%2f%2fakshat4112.github.io%2fposts%2fmodel-extraction-attacks%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Model Extraction Attacks: How Hackers Steal AI Models on ycombinator" href="https://news.ycombinator.com/submitlink?t=Model%20Extraction%20Attacks%3a%20How%20Hackers%20Steal%20AI%20Models&u=https%3a%2f%2fakshat4112.github.io%2fposts%2fmodel-extraction-attacks%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://akshat4112.github.io/>Akshat Gupta</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>