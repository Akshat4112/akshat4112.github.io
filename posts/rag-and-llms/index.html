<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>RAG and LLMs: Teaching Large Models to Use External Knowledge | Akshat Gupta</title>
<meta name=keywords content="RAG,LLM,vector-databases,embeddings,AI,information-retrieval,generative-ai"><meta name=description content="Large Language Models (LLMs) like GPT or LLaMA are great at generating text. But there&rsquo;s a catch:
They only know what they were trained on, and that knowledge is frozen at training time.
So what happens when you ask them something from after their training cutoff? Or something super niche, like a policy from your internal HR docs?
Enter RAG â€“ Retrieval-Augmented Generation.
A technique that combines LLMs with a search engine, enabling them to look up facts on the fly."><meta name=author content><link rel=canonical href=https://akshat4112.github.io/posts/rag-and-llms/><link crossorigin=anonymous href=/assets/css/stylesheet.e087fd1dc76e73a35ae6d7028ddc1ba41e0131e7f9b3a6e2d019a208e6d6c4b5.css integrity="sha256-4If9Hcduc6Na5tcCjdwbpB4BMef5s6bi0BmiCObWxLU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://akshat4112.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://akshat4112.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://akshat4112.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://akshat4112.github.io/apple-touch-icon.png><link rel=mask-icon href=https://akshat4112.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://akshat4112.github.io/posts/rag-and-llms/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-8YC2E5MW2M"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8YC2E5MW2M")}</script><meta property="og:title" content="RAG and LLMs: Teaching Large Models to Use External Knowledge"><meta property="og:description" content="Large Language Models (LLMs) like GPT or LLaMA are great at generating text. But there&rsquo;s a catch:
They only know what they were trained on, and that knowledge is frozen at training time.
So what happens when you ask them something from after their training cutoff? Or something super niche, like a policy from your internal HR docs?
Enter RAG â€“ Retrieval-Augmented Generation.
A technique that combines LLMs with a search engine, enabling them to look up facts on the fly."><meta property="og:type" content="article"><meta property="og:url" content="https://akshat4112.github.io/posts/rag-and-llms/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-15T09:00:00+01:00"><meta property="article:modified_time" content="2024-07-15T09:00:00+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="RAG and LLMs: Teaching Large Models to Use External Knowledge"><meta name=twitter:description content="Large Language Models (LLMs) like GPT or LLaMA are great at generating text. But there&rsquo;s a catch:
They only know what they were trained on, and that knowledge is frozen at training time.
So what happens when you ask them something from after their training cutoff? Or something super niche, like a policy from your internal HR docs?
Enter RAG â€“ Retrieval-Augmented Generation.
A technique that combines LLMs with a search engine, enabling them to look up facts on the fly."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://akshat4112.github.io/posts/"},{"@type":"ListItem","position":2,"name":"RAG and LLMs: Teaching Large Models to Use External Knowledge","item":"https://akshat4112.github.io/posts/rag-and-llms/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"RAG and LLMs: Teaching Large Models to Use External Knowledge","name":"RAG and LLMs: Teaching Large Models to Use External Knowledge","description":"Large Language Models (LLMs) like GPT or LLaMA are great at generating text. But there\u0026rsquo;s a catch:\nThey only know what they were trained on, and that knowledge is frozen at training time.\nSo what happens when you ask them something from after their training cutoff? Or something super niche, like a policy from your internal HR docs?\nEnter RAG â€“ Retrieval-Augmented Generation.\nA technique that combines LLMs with a search engine, enabling them to look up facts on the fly.\n","keywords":["RAG","LLM","vector-databases","embeddings","AI","information-retrieval","generative-ai"],"articleBody":"Large Language Models (LLMs) like GPT or LLaMA are great at generating text. But thereâ€™s a catch:\nThey only know what they were trained on, and that knowledge is frozen at training time.\nSo what happens when you ask them something from after their training cutoff? Or something super niche, like a policy from your internal HR docs?\nEnter RAG â€“ Retrieval-Augmented Generation.\nA technique that combines LLMs with a search engine, enabling them to look up facts on the fly.\nğŸ§  What is RAG? RAG (Retrieval-Augmented Generation) is a framework that augments the input to an LLM with retrieved documents or chunks from an external knowledge source.\nInstead of relying solely on the modelâ€™s internal weights, RAG pulls in real, current, or domain-specific content to ground its responses.\nThink of it like this:\nâ€œBefore answering, the model Googles the topic â€” and then responds.â€\nğŸ› ï¸ How Does RAG Work? The RAG architecture typically has three stages:\nQuery Understanding\nUser asks a question (e.g., â€œWhat are the leave policies at Acme Corp?â€) Retrieval\nThe system converts the query into an embedding (via a model like all-MiniLM). Searches a vector database (like FAISS, Weaviate, Qdrant) for top relevant chunks. Generation\nThe LLM gets the original query + retrieved context. It generates a grounded, coherent answer. This creates a dynamic pipeline where the LLM can â€œlook upâ€ facts in real time.\nğŸ” Why RAG is Important Without RAG With RAG âŒ Hallucinations âœ… Grounded answers âŒ Outdated knowledge âœ… Real-time / up-to-date info âŒ Model retraining needed for updates âœ… Just update documents âŒ Doesnâ€™t know your internal data âœ… Custom knowledge injected dynamically Itâ€™s the backbone of many enterprise AI apps, chat-with-your-PDF, code assistants, and AI copilots.\nğŸ§° Tools and Frameworks for RAG LangChain â€“ End-to-end pipelines with retrieval and LLM chaining (docs) Haystack â€“ Search-native RAG framework from deepset LlamaIndex â€“ Lightweight RAG with document loaders and query engines Pinecone / Weaviate / Qdrant â€“ Vector DBs to store and retrieve embeddings FAISS â€“ Facebook AI similarity search, blazing fast and open source Bonus: Use sentence-transformers to embed documents.\nğŸ“„ RAG for Custom Documents RAG is ideal for Q\u0026A over:\nğŸ“ Internal policies ğŸ“š Academic PDFs ğŸ§¾ Tax or legal docs ğŸ’» Codebases ğŸ¢ HR manuals You chunk the docs (e.g., into 500-word segments), embed them, and index in a vector DB. When the user asks a question, the system retrieves relevant chunks and passes them to the LLM for answering.\nğŸ§  When to Use RAG vs Fine-Tuning Situation Technique Need accurate info from private docs âœ… RAG Need tone/style/domain adaptation ğŸ” LoRA or finetune Need dynamic updates (e.g., news) âœ… RAG Have small structured data ğŸ”„ Toolformer / APIs Want to reduce hallucinations âœ… RAG + prompt tuning Often, combining RAG + LoRA fine-tuning gives the best of both worlds.\nâš ï¸ RAG Challenges Chunking strategy matters a lot (sentence, paragraph, or overlap-based?) Embedding quality impacts retrieval quality Long documents can lead to token limits â†’ need summarization or re-ranking LLM may still hallucinate within retrieved context (e.g., wrong interpretation) Tip: Always show the source in the final answer to improve trust.\nğŸ”„ Variants of RAG Hybrid RAG: Combines semantic + keyword search Multi-hop RAG: Chain multiple retrieval steps for complex reasoning Self-RAG: LLM rewrites query before retrieval to improve results (Meta AI, 2023) Agentic RAG: Agents explore document tree and reason before answering ğŸ§  Final Thoughts RAG is one of the most practical, scalable, and production-ready ways to supercharge LLMs with real-world knowledge.\nInstead of hoping your LLM â€œremembersâ€ something from training, just tell it what it needs to know.\nThe future of LLM applications isnâ€™t just smarter models, itâ€™s smarter context. And RAG is the backbone of that.\nâ€” Akshat\n","wordCount":"622","inLanguage":"en","datePublished":"2024-07-15T09:00:00+01:00","dateModified":"2024-07-15T09:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://akshat4112.github.io/posts/rag-and-llms/"},"publisher":{"@type":"Organization","name":"Akshat Gupta","logo":{"@type":"ImageObject","url":"https://akshat4112.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://akshat4112.github.io/ accesskey=h title="Akshat Gupta (Alt + H)">Akshat Gupta</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch></ul></div></div><ul id=menu><li><a href=https://akshat4112.github.io/posts title=Posts><span>Posts</span></a></li><li><a href=https://akshat4112.github.io/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://akshat4112.github.io/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://akshat4112.github.io/events/ title=Events><span>Events</span></a></li><li><a href=https://akshat4112.github.io/about/ title=About><span>About</span></a></li><li><a href="https://drive.google.com/file/d/1Qj6kaZXM40ixUgOAewhZlTj57piWERSw/view?usp=drive_link" title=CV><span>CV</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://akshat4112.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://akshat4112.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">RAG and LLMs: Teaching Large Models to Use External Knowledge</h1><div class=post-meta><span title='2024-07-15 09:00:00 +0100 +0100'>July 15, 2024</span>&nbsp;Â·&nbsp;3 min</div></header><div class=post-content><p>Large Language Models (LLMs) like GPT or LLaMA are great at generating text. But there&rsquo;s a catch:<br>They <strong>only know what they were trained on</strong>, and that knowledge is frozen at training time.</p><p>So what happens when you ask them something from after their training cutoff? Or something super niche, like a policy from your internal HR docs?</p><p>Enter <strong>RAG</strong> â€“ Retrieval-Augmented Generation.<br>A technique that combines LLMs with a <strong>search engine</strong>, enabling them to look up facts on the fly.</p><hr><h2 id=-what-is-rag>ğŸ§  What is RAG?<a hidden class=anchor aria-hidden=true href=#-what-is-rag>#</a></h2><p><strong>RAG</strong> (Retrieval-Augmented Generation) is a framework that augments the input to an LLM with <strong>retrieved documents or chunks</strong> from an external knowledge source.</p><p>Instead of relying solely on the modelâ€™s internal weights, RAG pulls in real, current, or domain-specific content to ground its responses.</p><blockquote><p>Think of it like this:<br>&ldquo;Before answering, the model Googles the topic â€” and <em>then</em> responds.&rdquo;</p></blockquote><hr><h2 id=-how-does-rag-work>ğŸ› ï¸ How Does RAG Work?<a hidden class=anchor aria-hidden=true href=#-how-does-rag-work>#</a></h2><p>The RAG architecture typically has three stages:</p><ol><li><p><strong>Query Understanding</strong></p><ul><li>User asks a question (e.g., <em>&ldquo;What are the leave policies at Acme Corp?&rdquo;</em>)</li></ul></li><li><p><strong>Retrieval</strong></p><ul><li>The system converts the query into an embedding (via a model like <code>all-MiniLM</code>).</li><li>Searches a vector database (like <strong>FAISS</strong>, <strong>Weaviate</strong>, <strong>Qdrant</strong>) for top relevant chunks.</li></ul></li><li><p><strong>Generation</strong></p><ul><li>The LLM gets the <strong>original query + retrieved context</strong>.</li><li>It generates a grounded, coherent answer.</li></ul></li></ol><p>This creates a dynamic pipeline where the LLM can â€œlook upâ€ facts in real time.</p><hr><h2 id=-why-rag-is-important>ğŸ” Why RAG is Important<a hidden class=anchor aria-hidden=true href=#-why-rag-is-important>#</a></h2><table><thead><tr><th>Without RAG</th><th>With RAG</th></tr></thead><tbody><tr><td>âŒ Hallucinations</td><td>âœ… Grounded answers</td></tr><tr><td>âŒ Outdated knowledge</td><td>âœ… Real-time / up-to-date info</td></tr><tr><td>âŒ Model retraining needed for updates</td><td>âœ… Just update documents</td></tr><tr><td>âŒ Doesnâ€™t know your internal data</td><td>âœ… Custom knowledge injected dynamically</td></tr></tbody></table><p>Itâ€™s the backbone of many <strong>enterprise AI apps</strong>, <strong>chat-with-your-PDF</strong>, <strong>code assistants</strong>, and <strong>AI copilots</strong>.</p><hr><h2 id=-tools-and-frameworks-for-rag>ğŸ§° Tools and Frameworks for RAG<a hidden class=anchor aria-hidden=true href=#-tools-and-frameworks-for-rag>#</a></h2><ul><li><strong>LangChain</strong> â€“ End-to-end pipelines with retrieval and LLM chaining (<a href=https://docs.langchain.com/>docs</a>)</li><li><strong>Haystack</strong> â€“ Search-native RAG framework from deepset</li><li><strong>LlamaIndex</strong> â€“ Lightweight RAG with document loaders and query engines</li><li><strong>Pinecone / Weaviate / Qdrant</strong> â€“ Vector DBs to store and retrieve embeddings</li><li><strong>FAISS</strong> â€“ Facebook AI similarity search, blazing fast and open source</li></ul><blockquote><p>Bonus: Use <a href=https://www.sbert.net/>sentence-transformers</a> to embed documents.</p></blockquote><hr><h2 id=-rag-for-custom-documents>ğŸ“„ RAG for Custom Documents<a hidden class=anchor aria-hidden=true href=#-rag-for-custom-documents>#</a></h2><p>RAG is ideal for Q&amp;A over:</p><ul><li>ğŸ“ Internal policies</li><li>ğŸ“š Academic PDFs</li><li>ğŸ§¾ Tax or legal docs</li><li>ğŸ’» Codebases</li><li>ğŸ¢ HR manuals</li></ul><p>You chunk the docs (e.g., into 500-word segments), embed them, and index in a vector DB. When the user asks a question, the system retrieves relevant chunks and passes them to the LLM for answering.</p><hr><h2 id=-when-to-use-rag-vs-fine-tuning>ğŸ§  When to Use RAG vs Fine-Tuning<a hidden class=anchor aria-hidden=true href=#-when-to-use-rag-vs-fine-tuning>#</a></h2><table><thead><tr><th>Situation</th><th>Technique</th></tr></thead><tbody><tr><td>Need accurate info from private docs</td><td>âœ… RAG</td></tr><tr><td>Need tone/style/domain adaptation</td><td>ğŸ” LoRA or finetune</td></tr><tr><td>Need dynamic updates (e.g., news)</td><td>âœ… RAG</td></tr><tr><td>Have small structured data</td><td>ğŸ”„ Toolformer / APIs</td></tr><tr><td>Want to reduce hallucinations</td><td>âœ… RAG + prompt tuning</td></tr></tbody></table><p>Often, combining <strong>RAG + LoRA fine-tuning</strong> gives the best of both worlds.</p><hr><h2 id=-rag-challenges>âš ï¸ RAG Challenges<a hidden class=anchor aria-hidden=true href=#-rag-challenges>#</a></h2><ul><li><strong>Chunking strategy</strong> matters a lot (sentence, paragraph, or overlap-based?)</li><li>Embedding quality impacts retrieval quality</li><li>Long documents can lead to token limits â†’ need summarization or re-ranking</li><li>LLM may still hallucinate <em>within</em> retrieved context (e.g., wrong interpretation)</li></ul><blockquote><p>Tip: Always show the <strong>source</strong> in the final answer to improve trust.</p></blockquote><hr><h2 id=-variants-of-rag>ğŸ”„ Variants of RAG<a hidden class=anchor aria-hidden=true href=#-variants-of-rag>#</a></h2><ul><li><strong>Hybrid RAG</strong>: Combines semantic + keyword search</li><li><strong>Multi-hop RAG</strong>: Chain multiple retrieval steps for complex reasoning</li><li><strong>Self-RAG</strong>: LLM rewrites query before retrieval to improve results (<a href=https://arxiv.org/abs/2302.07296>Meta AI, 2023</a>)</li><li><strong>Agentic RAG</strong>: Agents explore document tree and reason before answering</li></ul><hr><h2 id=-final-thoughts>ğŸ§  Final Thoughts<a hidden class=anchor aria-hidden=true href=#-final-thoughts>#</a></h2><p>RAG is one of the most <strong>practical, scalable, and production-ready</strong> ways to supercharge LLMs with real-world knowledge.</p><p>Instead of hoping your LLM &ldquo;remembers&rdquo; something from training, just <strong>tell it what it needs to know</strong>.</p><p>The future of LLM applications isnâ€™t just <em>smarter models</em>, itâ€™s <em>smarter context</em>. And RAG is the backbone of that.</p><p>â€” <strong>Akshat</strong></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://akshat4112.github.io/tags/rag/>RAG</a></li><li><a href=https://akshat4112.github.io/tags/llm/>LLM</a></li><li><a href=https://akshat4112.github.io/tags/vector-databases/>Vector-Databases</a></li><li><a href=https://akshat4112.github.io/tags/embeddings/>Embeddings</a></li><li><a href=https://akshat4112.github.io/tags/ai/>AI</a></li><li><a href=https://akshat4112.github.io/tags/information-retrieval/>Information-Retrieval</a></li><li><a href=https://akshat4112.github.io/tags/generative-ai/>Generative-Ai</a></li></ul><nav class=paginav><a class=prev href=https://akshat4112.github.io/posts/evaluating-llms/><span class=title>Â« Prev</span><br><span>Evaluating LLMs: How Do You Measure a Model's Mind?</span>
</a><a class=next href=https://akshat4112.github.io/posts/attention-in-transformers/><span class=title>Next Â»</span><br><span>Understanding Attention in Transformers: The Core of Modern NLP</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share RAG and LLMs: Teaching Large Models to Use External Knowledge on x" href="https://x.com/intent/tweet/?text=RAG%20and%20LLMs%3a%20Teaching%20Large%20Models%20to%20Use%20External%20Knowledge&amp;url=https%3a%2f%2fakshat4112.github.io%2fposts%2frag-and-llms%2f&amp;hashtags=RAG%2cLLM%2cvector-databases%2cembeddings%2cAI%2cinformation-retrieval%2cgenerative-ai"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RAG and LLMs: Teaching Large Models to Use External Knowledge on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fakshat4112.github.io%2fposts%2frag-and-llms%2f&amp;title=RAG%20and%20LLMs%3a%20Teaching%20Large%20Models%20to%20Use%20External%20Knowledge&amp;summary=RAG%20and%20LLMs%3a%20Teaching%20Large%20Models%20to%20Use%20External%20Knowledge&amp;source=https%3a%2f%2fakshat4112.github.io%2fposts%2frag-and-llms%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RAG and LLMs: Teaching Large Models to Use External Knowledge on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fakshat4112.github.io%2fposts%2frag-and-llms%2f&title=RAG%20and%20LLMs%3a%20Teaching%20Large%20Models%20to%20Use%20External%20Knowledge"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RAG and LLMs: Teaching Large Models to Use External Knowledge on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fakshat4112.github.io%2fposts%2frag-and-llms%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RAG and LLMs: Teaching Large Models to Use External Knowledge on whatsapp" href="https://api.whatsapp.com/send?text=RAG%20and%20LLMs%3a%20Teaching%20Large%20Models%20to%20Use%20External%20Knowledge%20-%20https%3a%2f%2fakshat4112.github.io%2fposts%2frag-and-llms%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RAG and LLMs: Teaching Large Models to Use External Knowledge on telegram" href="https://telegram.me/share/url?text=RAG%20and%20LLMs%3a%20Teaching%20Large%20Models%20to%20Use%20External%20Knowledge&amp;url=https%3a%2f%2fakshat4112.github.io%2fposts%2frag-and-llms%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share RAG and LLMs: Teaching Large Models to Use External Knowledge on ycombinator" href="https://news.ycombinator.com/submitlink?t=RAG%20and%20LLMs%3a%20Teaching%20Large%20Models%20to%20Use%20External%20Knowledge&u=https%3a%2f%2fakshat4112.github.io%2fposts%2frag-and-llms%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://akshat4112.github.io/>Akshat Gupta</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>