<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LLM Fine-Tuning and LoRA: Making Large Models Work for You | Akshat Gupta</title>
<meta name=keywords content="LLM,fine-tuning,LoRA,AI,transformers"><meta name=description content="As powerful as large language models (LLMs) like GPT, LLaMA, and Mistral are, they‚Äôre still general-purpose. If you want to make them truly useful for your domain‚Äîwhether it‚Äôs legal documents, financial analysis, or German tax law‚Äîyou need to fine-tune them.
And thanks to a technique called LoRA (Low-Rank Adaptation), you can now fine-tune LLMs with a fraction of the data, compute, and cost.

üîß What is Fine-Tuning?
Fine-tuning is the process of continuing the training of a pre-trained LLM on your own dataset so that it learns domain-specific patterns, vocabulary, tone, or tasks."><meta name=author content><link rel=canonical href=https://akshat4112.github.io/posts/llm-fine-tuning-lora/><link crossorigin=anonymous href=/assets/css/stylesheet.e087fd1dc76e73a35ae6d7028ddc1ba41e0131e7f9b3a6e2d019a208e6d6c4b5.css integrity="sha256-4If9Hcduc6Na5tcCjdwbpB4BMef5s6bi0BmiCObWxLU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://akshat4112.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://akshat4112.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://akshat4112.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://akshat4112.github.io/apple-touch-icon.png><link rel=mask-icon href=https://akshat4112.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://akshat4112.github.io/posts/llm-fine-tuning-lora/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-8YC2E5MW2M"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8YC2E5MW2M")}</script><meta property="og:title" content="LLM Fine-Tuning and LoRA: Making Large Models Work for You"><meta property="og:description" content="As powerful as large language models (LLMs) like GPT, LLaMA, and Mistral are, they‚Äôre still general-purpose. If you want to make them truly useful for your domain‚Äîwhether it‚Äôs legal documents, financial analysis, or German tax law‚Äîyou need to fine-tune them.
And thanks to a technique called LoRA (Low-Rank Adaptation), you can now fine-tune LLMs with a fraction of the data, compute, and cost.

üîß What is Fine-Tuning?
Fine-tuning is the process of continuing the training of a pre-trained LLM on your own dataset so that it learns domain-specific patterns, vocabulary, tone, or tasks."><meta property="og:type" content="article"><meta property="og:url" content="https://akshat4112.github.io/posts/llm-fine-tuning-lora/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-20T17:30:00+02:00"><meta property="article:modified_time" content="2025-04-20T17:30:00+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="LLM Fine-Tuning and LoRA: Making Large Models Work for You"><meta name=twitter:description content="As powerful as large language models (LLMs) like GPT, LLaMA, and Mistral are, they‚Äôre still general-purpose. If you want to make them truly useful for your domain‚Äîwhether it‚Äôs legal documents, financial analysis, or German tax law‚Äîyou need to fine-tune them.
And thanks to a technique called LoRA (Low-Rank Adaptation), you can now fine-tune LLMs with a fraction of the data, compute, and cost.

üîß What is Fine-Tuning?
Fine-tuning is the process of continuing the training of a pre-trained LLM on your own dataset so that it learns domain-specific patterns, vocabulary, tone, or tasks."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://akshat4112.github.io/posts/"},{"@type":"ListItem","position":2,"name":"LLM Fine-Tuning and LoRA: Making Large Models Work for You","item":"https://akshat4112.github.io/posts/llm-fine-tuning-lora/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LLM Fine-Tuning and LoRA: Making Large Models Work for You","name":"LLM Fine-Tuning and LoRA: Making Large Models Work for You","description":"As powerful as large language models (LLMs) like GPT, LLaMA, and Mistral are, they‚Äôre still general-purpose. If you want to make them truly useful for your domain‚Äîwhether it‚Äôs legal documents, financial analysis, or German tax law‚Äîyou need to fine-tune them.\nAnd thanks to a technique called LoRA (Low-Rank Adaptation), you can now fine-tune LLMs with a fraction of the data, compute, and cost.\nüîß What is Fine-Tuning? Fine-tuning is the process of continuing the training of a pre-trained LLM on your own dataset so that it learns domain-specific patterns, vocabulary, tone, or tasks.\n","keywords":["LLM","fine-tuning","LoRA","AI","transformers"],"articleBody":"As powerful as large language models (LLMs) like GPT, LLaMA, and Mistral are, they‚Äôre still general-purpose. If you want to make them truly useful for your domain‚Äîwhether it‚Äôs legal documents, financial analysis, or German tax law‚Äîyou need to fine-tune them.\nAnd thanks to a technique called LoRA (Low-Rank Adaptation), you can now fine-tune LLMs with a fraction of the data, compute, and cost.\nüîß What is Fine-Tuning? Fine-tuning is the process of continuing the training of a pre-trained LLM on your own dataset so that it learns domain-specific patterns, vocabulary, tone, or tasks.\nFor example:\nWant an LLM that answers only insurance questions? ‚Üí Fine-tune it on your policy docs and claims. Need a medical assistant? ‚Üí Fine-tune it on clinical notes and patient Q\u0026A. Want it to follow instructions better? ‚Üí Fine-tune on curated instruction-response pairs. Fine-tuning adjusts the internal weights of the model, helping it generalize better to your specific use case.\nü§Ø The Challenge The problem? Full fine-tuning of LLMs is expensive.\nA 7B parameter model might need hundreds of GBs of VRAM. You‚Äôll need thousands of samples and multiple epochs. It‚Äôs easy to overfit, and hard to iterate fast. Enter LoRA.\nüí° What is LoRA? LoRA, short for Low-Rank Adaptation of Large Language Models, is a technique introduced by Microsoft Research (paper here) that makes fine-tuning cheaper and modular.\nInstead of updating all the parameters of the model, LoRA:\nFreezes the original weights of the model Adds trainable rank-decomposed matrices (adapters) to specific layers (usually attention projections) Trains only these lightweight matrices (~0.1% of the original model size) This drastically reduces GPU memory and training time.\n‚öôÔ∏è How LoRA Works (Simplified) Mathematically, instead of updating weight matrix W, LoRA adds two low-rank matrices A and B such that:\nW‚Äô = W + A * B\nWhere:\nW = original frozen weight A, B = small trainable matrices (e.g. rank 4 or 8) During inference, the adapted weights are used as if they were part of the model.\nüß™ Benefits of LoRA üí∏ Low cost: Train on consumer GPUs or Colab ‚ö° Fast: Fewer trainable params = quicker epochs üîÅ Composable: Mix and match adapters (e.g., domain A + domain B) üéØ Targeted: Focus adaptation on just a few layers Perfect for startups, researchers, and builders who want domain-specific LLMs without full-scale infra.\nüõ†Ô∏è When to Use Fine-Tuning or LoRA Use Case Fine-Tuning Type Model refuses valid queries Full fine-tune / LoRA Needs to match company tone LoRA Custom document Q\u0026A RAG or LoRA Domain-specific language or symbols Fine-tuning Instruction-following improvements LoRA or full fine-tune For general Q\u0026A or document tasks, combine LoRA with a RAG pipeline to get best results.\nüß∞ Popular Libraries for LoRA PEFT ‚Äì Hugging Face‚Äôs library for Parameter-Efficient Fine-Tuning QLoRA ‚Äì Quantized LoRA (8-bit/4-bit) for even more memory savings Axolotl ‚Äì Powerful config-based trainer LLaMA-Factory ‚Äì Quick setup for finetuning LLaMA and Mistral models üß™ Example: Fine-Tuning Mistral with LoRA Prepare dataset (e.g. Alpaca or your own instruction set) Choose base model (e.g. mistralai/Mistral-7B-Instruct-v0.2) Use peft.LoraConfig to configure adapter Train with transformers.Trainer or SFTTrainer Save and deploy LoRA adapter with model You now have your own lightweight LLM variant!\nüß† Final Thoughts Fine-tuning LLMs is no longer reserved for big labs and billion-parameter budgets. With LoRA, anyone can personalize a model for their task, brand, or niche.\nWant your own German-speaking travel planner? Or a legal assistant that understands Indian property law? LoRA gets you there‚Äîfast, cheap, and modular.\nAnd best of all: you keep the base model untouched and can reuse adapters across projects.\n‚Äî Akshat\n","wordCount":"595","inLanguage":"en","datePublished":"2025-04-20T17:30:00+02:00","dateModified":"2025-04-20T17:30:00+02:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://akshat4112.github.io/posts/llm-fine-tuning-lora/"},"publisher":{"@type":"Organization","name":"Akshat Gupta","logo":{"@type":"ImageObject","url":"https://akshat4112.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://akshat4112.github.io/ accesskey=h title="Akshat Gupta (Alt + H)">Akshat Gupta</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch></ul></div></div><ul id=menu><li><a href=https://akshat4112.github.io/posts title=Posts><span>Posts</span></a></li><li><a href=https://akshat4112.github.io/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://akshat4112.github.io/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://akshat4112.github.io/events/ title=Events><span>Events</span></a></li><li><a href=https://akshat4112.github.io/about/ title=About><span>About</span></a></li><li><a href="https://drive.google.com/file/d/1Qj6kaZXM40ixUgOAewhZlTj57piWERSw/view?usp=drive_link" title=CV><span>CV</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://akshat4112.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://akshat4112.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">LLM Fine-Tuning and LoRA: Making Large Models Work for You</h1><div class=post-meta><span title='2025-04-20 17:30:00 +0200 +0200'>April 20, 2025</span>&nbsp;¬∑&nbsp;3 min</div></header><div class=post-content><p>As powerful as large language models (LLMs) like GPT, LLaMA, and Mistral are, they‚Äôre still <em>general-purpose</em>. If you want to make them truly useful for your domain‚Äîwhether it‚Äôs legal documents, financial analysis, or German tax law‚Äîyou need to <strong>fine-tune</strong> them.</p><p>And thanks to a technique called <strong>LoRA (Low-Rank Adaptation)</strong>, you can now fine-tune LLMs with a fraction of the data, compute, and cost.</p><hr><h2 id=-what-is-fine-tuning>üîß What is Fine-Tuning?<a hidden class=anchor aria-hidden=true href=#-what-is-fine-tuning>#</a></h2><p>Fine-tuning is the process of <strong>continuing the training</strong> of a pre-trained LLM on your own dataset so that it learns domain-specific patterns, vocabulary, tone, or tasks.</p><p>For example:</p><ul><li>Want an LLM that answers <em>only</em> insurance questions? ‚Üí Fine-tune it on your policy docs and claims.</li><li>Need a medical assistant? ‚Üí Fine-tune it on clinical notes and patient Q&amp;A.</li><li>Want it to follow instructions better? ‚Üí Fine-tune on curated instruction-response pairs.</li></ul><p>Fine-tuning adjusts the internal weights of the model, helping it generalize better to your specific use case.</p><hr><h2 id=-the-challenge>ü§Ø The Challenge<a hidden class=anchor aria-hidden=true href=#-the-challenge>#</a></h2><p>The problem? Full fine-tuning of LLMs is expensive.</p><ul><li>A 7B parameter model might need <strong>hundreds of GBs</strong> of VRAM.</li><li>You‚Äôll need <strong>thousands of samples</strong> and <strong>multiple epochs</strong>.</li><li>It‚Äôs easy to <strong>overfit</strong>, and hard to iterate fast.</li></ul><p>Enter <strong>LoRA</strong>.</p><hr><h2 id=-what-is-lora>üí° What is LoRA?<a hidden class=anchor aria-hidden=true href=#-what-is-lora>#</a></h2><p>LoRA, short for <strong>Low-Rank Adaptation of Large Language Models</strong>, is a technique introduced by Microsoft Research (<a href=https://arxiv.org/abs/2106.09685>paper here</a>) that makes fine-tuning cheaper and modular.</p><p>Instead of updating <em>all</em> the parameters of the model, LoRA:</p><ul><li><strong>Freezes</strong> the original weights of the model</li><li>Adds <strong>trainable rank-decomposed matrices</strong> (adapters) to specific layers (usually attention projections)</li><li>Trains only these lightweight matrices (~0.1% of the original model size)</li></ul><p>This drastically reduces GPU memory and training time.</p><hr><h2 id=-how-lora-works-simplified>‚öôÔ∏è How LoRA Works (Simplified)<a hidden class=anchor aria-hidden=true href=#-how-lora-works-simplified>#</a></h2><p>Mathematically, instead of updating weight matrix <strong>W</strong>, LoRA adds two low-rank matrices <strong>A</strong> and <strong>B</strong> such that:</p><p>W&rsquo; = W + A * B</p><p>Where:</p><ul><li><strong>W</strong> = original frozen weight</li><li><strong>A</strong>, <strong>B</strong> = small trainable matrices (e.g. rank 4 or 8)</li></ul><p>During inference, the adapted weights are used as if they were part of the model.</p><hr><h2 id=-benefits-of-lora>üß™ Benefits of LoRA<a hidden class=anchor aria-hidden=true href=#-benefits-of-lora>#</a></h2><ul><li>üí∏ <strong>Low cost</strong>: Train on consumer GPUs or Colab</li><li>‚ö° <strong>Fast</strong>: Fewer trainable params = quicker epochs</li><li>üîÅ <strong>Composable</strong>: Mix and match adapters (e.g., domain A + domain B)</li><li>üéØ <strong>Targeted</strong>: Focus adaptation on just a few layers</li></ul><p>Perfect for startups, researchers, and builders who want <strong>domain-specific LLMs</strong> without full-scale infra.</p><hr><h2 id=-when-to-use-fine-tuning-or-lora>üõ†Ô∏è When to Use Fine-Tuning or LoRA<a hidden class=anchor aria-hidden=true href=#-when-to-use-fine-tuning-or-lora>#</a></h2><table><thead><tr><th>Use Case</th><th>Fine-Tuning Type</th></tr></thead><tbody><tr><td>Model refuses valid queries</td><td>Full fine-tune / LoRA</td></tr><tr><td>Needs to match company tone</td><td>LoRA</td></tr><tr><td>Custom document Q&amp;A</td><td>RAG or LoRA</td></tr><tr><td>Domain-specific language or symbols</td><td>Fine-tuning</td></tr><tr><td>Instruction-following improvements</td><td>LoRA or full fine-tune</td></tr></tbody></table><p>For general Q&amp;A or document tasks, <strong>combine LoRA with a RAG pipeline</strong> to get best results.</p><hr><h2 id=-popular-libraries-for-lora>üß∞ Popular Libraries for LoRA<a hidden class=anchor aria-hidden=true href=#-popular-libraries-for-lora>#</a></h2><ul><li><a href=https://huggingface.co/docs/peft/index><strong>PEFT</strong></a> ‚Äì Hugging Face‚Äôs library for Parameter-Efficient Fine-Tuning</li><li><a href=https://huggingface.co/blog/4bit-transformers-bitsandbytes><strong>QLoRA</strong></a> ‚Äì Quantized LoRA (8-bit/4-bit) for even more memory savings</li><li><a href=https://github.com/OpenAccess-AI-Collective/axolotl><strong>Axolotl</strong></a> ‚Äì Powerful config-based trainer</li><li><a href=https://github.com/hiyouga/LLaMA-Factory><strong>LLaMA-Factory</strong></a> ‚Äì Quick setup for finetuning LLaMA and Mistral models</li></ul><hr><h2 id=-example-fine-tuning-mistral-with-lora>üß™ Example: Fine-Tuning Mistral with LoRA<a hidden class=anchor aria-hidden=true href=#-example-fine-tuning-mistral-with-lora>#</a></h2><ol><li>Prepare dataset (e.g. Alpaca or your own instruction set)</li><li>Choose base model (e.g. <code>mistralai/Mistral-7B-Instruct-v0.2</code>)</li><li>Use <code>peft.LoraConfig</code> to configure adapter</li><li>Train with <code>transformers.Trainer</code> or <code>SFTTrainer</code></li><li>Save and deploy LoRA adapter with model</li></ol><p>You now have your own lightweight LLM variant!</p><hr><h2 id=-final-thoughts>üß† Final Thoughts<a hidden class=anchor aria-hidden=true href=#-final-thoughts>#</a></h2><p>Fine-tuning LLMs is no longer reserved for big labs and billion-parameter budgets. With <strong>LoRA</strong>, anyone can personalize a model for their task, brand, or niche.</p><p>Want your own German-speaking travel planner? Or a legal assistant that understands Indian property law? LoRA gets you there‚Äîfast, cheap, and modular.</p><p>And best of all: you keep the base model untouched and can reuse adapters across projects.</p><p>‚Äî <strong>Akshat</strong></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://akshat4112.github.io/tags/llm/>LLM</a></li><li><a href=https://akshat4112.github.io/tags/fine-tuning/>Fine-Tuning</a></li><li><a href=https://akshat4112.github.io/tags/lora/>LoRA</a></li><li><a href=https://akshat4112.github.io/tags/ai/>AI</a></li><li><a href=https://akshat4112.github.io/tags/transformers/>Transformers</a></li></ul><nav class=paginav><a class=prev href=https://akshat4112.github.io/posts/ontology-in-knowledge-graphs/><span class=title>¬´ Prev</span><br><span>What is an Ontology in a Knowledge Graph?</span>
</a><a class=next href=https://akshat4112.github.io/posts/what-are-knowledge-graphs/><span class=title>Next ¬ª</span><br><span>What Are Knowledge Graphs?</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share LLM Fine-Tuning and LoRA: Making Large Models Work for You on x" href="https://x.com/intent/tweet/?text=LLM%20Fine-Tuning%20and%20LoRA%3a%20Making%20Large%20Models%20Work%20for%20You&amp;url=https%3a%2f%2fakshat4112.github.io%2fposts%2fllm-fine-tuning-lora%2f&amp;hashtags=LLM%2cfine-tuning%2cLoRA%2cAI%2ctransformers"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLM Fine-Tuning and LoRA: Making Large Models Work for You on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fakshat4112.github.io%2fposts%2fllm-fine-tuning-lora%2f&amp;title=LLM%20Fine-Tuning%20and%20LoRA%3a%20Making%20Large%20Models%20Work%20for%20You&amp;summary=LLM%20Fine-Tuning%20and%20LoRA%3a%20Making%20Large%20Models%20Work%20for%20You&amp;source=https%3a%2f%2fakshat4112.github.io%2fposts%2fllm-fine-tuning-lora%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLM Fine-Tuning and LoRA: Making Large Models Work for You on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fakshat4112.github.io%2fposts%2fllm-fine-tuning-lora%2f&title=LLM%20Fine-Tuning%20and%20LoRA%3a%20Making%20Large%20Models%20Work%20for%20You"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLM Fine-Tuning and LoRA: Making Large Models Work for You on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fakshat4112.github.io%2fposts%2fllm-fine-tuning-lora%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLM Fine-Tuning and LoRA: Making Large Models Work for You on whatsapp" href="https://api.whatsapp.com/send?text=LLM%20Fine-Tuning%20and%20LoRA%3a%20Making%20Large%20Models%20Work%20for%20You%20-%20https%3a%2f%2fakshat4112.github.io%2fposts%2fllm-fine-tuning-lora%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLM Fine-Tuning and LoRA: Making Large Models Work for You on telegram" href="https://telegram.me/share/url?text=LLM%20Fine-Tuning%20and%20LoRA%3a%20Making%20Large%20Models%20Work%20for%20You&amp;url=https%3a%2f%2fakshat4112.github.io%2fposts%2fllm-fine-tuning-lora%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share LLM Fine-Tuning and LoRA: Making Large Models Work for You on ycombinator" href="https://news.ycombinator.com/submitlink?t=LLM%20Fine-Tuning%20and%20LoRA%3a%20Making%20Large%20Models%20Work%20for%20You&u=https%3a%2f%2fakshat4112.github.io%2fposts%2fllm-fine-tuning-lora%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://akshat4112.github.io/>Akshat Gupta</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>