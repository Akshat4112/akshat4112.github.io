<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Understanding Attention in Transformers: The Core of Modern NLP | Akshat Gupta</title>
<meta name=keywords content="transformers,attention,deep-learning,NLP,self-attention,neural-networks,AI"><meta name=description content="When people say &ldquo;Transformers revolutionized NLP,&rdquo; what they really mean is:

Attention revolutionized NLP.
From GPT and BERT to LLaMA and Claude, attention mechanisms are the beating heart of modern large language models.
But what exactly is attention? Why is it so powerful? And how many types are there?
Let&rsquo;s dive in.

üß† What is Attention?
In the simplest sense, attention is a way for a model to focus on the most relevant parts of the input when generating output."><meta name=author content><link rel=canonical href=https://akshat4112.github.io/posts/attention-in-transformers/><link crossorigin=anonymous href=/assets/css/stylesheet.e087fd1dc76e73a35ae6d7028ddc1ba41e0131e7f9b3a6e2d019a208e6d6c4b5.css integrity="sha256-4If9Hcduc6Na5tcCjdwbpB4BMef5s6bi0BmiCObWxLU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://akshat4112.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://akshat4112.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://akshat4112.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://akshat4112.github.io/apple-touch-icon.png><link rel=mask-icon href=https://akshat4112.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://akshat4112.github.io/posts/attention-in-transformers/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-8YC2E5MW2M"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-8YC2E5MW2M")}</script><meta property="og:title" content="Understanding Attention in Transformers: The Core of Modern NLP"><meta property="og:description" content="When people say &ldquo;Transformers revolutionized NLP,&rdquo; what they really mean is:

Attention revolutionized NLP.
From GPT and BERT to LLaMA and Claude, attention mechanisms are the beating heart of modern large language models.
But what exactly is attention? Why is it so powerful? And how many types are there?
Let&rsquo;s dive in.

üß† What is Attention?
In the simplest sense, attention is a way for a model to focus on the most relevant parts of the input when generating output."><meta property="og:type" content="article"><meta property="og:url" content="https://akshat4112.github.io/posts/attention-in-transformers/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-15T09:00:00+01:00"><meta property="article:modified_time" content="2024-08-15T09:00:00+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Understanding Attention in Transformers: The Core of Modern NLP"><meta name=twitter:description content="When people say &ldquo;Transformers revolutionized NLP,&rdquo; what they really mean is:

Attention revolutionized NLP.
From GPT and BERT to LLaMA and Claude, attention mechanisms are the beating heart of modern large language models.
But what exactly is attention? Why is it so powerful? And how many types are there?
Let&rsquo;s dive in.

üß† What is Attention?
In the simplest sense, attention is a way for a model to focus on the most relevant parts of the input when generating output."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://akshat4112.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Understanding Attention in Transformers: The Core of Modern NLP","item":"https://akshat4112.github.io/posts/attention-in-transformers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding Attention in Transformers: The Core of Modern NLP","name":"Understanding Attention in Transformers: The Core of Modern NLP","description":"When people say \u0026ldquo;Transformers revolutionized NLP,\u0026rdquo; what they really mean is:\nAttention revolutionized NLP.\nFrom GPT and BERT to LLaMA and Claude, attention mechanisms are the beating heart of modern large language models.\nBut what exactly is attention? Why is it so powerful? And how many types are there?\nLet\u0026rsquo;s dive in.\nüß† What is Attention? In the simplest sense, attention is a way for a model to focus on the most relevant parts of the input when generating output.\n","keywords":["transformers","attention","deep-learning","NLP","self-attention","neural-networks","AI"],"articleBody":"When people say ‚ÄúTransformers revolutionized NLP,‚Äù what they really mean is:\nAttention revolutionized NLP.\nFrom GPT and BERT to LLaMA and Claude, attention mechanisms are the beating heart of modern large language models.\nBut what exactly is attention? Why is it so powerful? And how many types are there?\nLet‚Äôs dive in.\nüß† What is Attention? In the simplest sense, attention is a way for a model to focus on the most relevant parts of the input when generating output.\nIt answers:\n‚ÄúGiven this word, which other words should I pay attention to ‚Äî and how much?‚Äù\nüî¢ The Scaled Dot-Product Attention Let‚Äôs break it down mathematically.\nGiven:\nQuery matrix $Q \\in \\mathbb{R}^{n \\times d_k}$ Key matrix $K \\in \\mathbb{R}^{n \\times d_k}$ Value matrix $V \\in \\mathbb{R}^{n \\times d_v}$ The attention output is:\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V$$\n$QK^\\top$: Dot product measures similarity $\\sqrt{d_k}$: Scaling factor to avoid large softmax values softmax: Turns similarity into attention weights $V$: Weighted sum of value vectors üìñ Citation: Vaswani et al., 2017 (Attention is All You Need)\nüîÅ Multi-Head Attention Instead of applying one attention function, we apply it multiple times in parallel with different projections.\n$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ‚Ä¶, \\text{head}_h)W^O$$ $$\\text{where head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$\nEach head learns different types of relationships (e.g., syntactic, semantic).\nüß© Types of Attention in Transformers Let‚Äôs look at the key attention variations used in different transformer architectures.\n1. Self-Attention Query, Key, and Value come from the same input. Used in encoder and decoder blocks. Each token attends to every other token (or just previous ones in causal attention). $$\\text{SelfAttention}(X) = \\text{Attention}(X, X, X)$$\n2. Cross-Attention Used in encoder-decoder models like T5 or BART. Query comes from decoder, Key \u0026 Value come from encoder output. $$\\text{CrossAttention}(Q_{\\text{decoder}}, K_{\\text{encoder}}, V_{\\text{encoder}})$$\n3. Masked (Causal) Attention Used in autoregressive models like GPT. Prevents tokens from attending to future tokens. Enforced using a triangular mask. $$\\text{MaskedAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + M\\right)V$$ where $M$ is a mask with $-\\infty$ in upper triangle.\n4. Local Attention Each token only attends to a local window (e.g., ¬±128 tokens). Reduces compute from $O(n^2)$ to $O(nw)$ Used in models like Longformer (Beltagy et al., 2020)\n5. Sparse Attention Instead of full attention, use pre-defined patterns (e.g., strided, global). Reduces memory usage. Examples:\nBigBird (Zaheer et al., 2020) Reformer (Kitaev et al., 2020) 6. Linear / Kernelized Attention Approximates attention with linear complexity. Replace softmax with kernel function: $$\\text{Attention}(Q, K, V) = \\phi(Q)(\\phi(K)^\\top V)$$ Used in Performer (Choromanski et al., 2020)\n7. Memory-Augmented Attention Adds external memory vectors (e.g., key-value cache, documents). Popular in RAG and MoE systems. üèóÔ∏è Attention Block in Transformers Each Transformer layer consists of:\nMulti-head Attention Add \u0026 Layer Norm Feed Forward Network Add \u0026 Layer Norm Input ‚Üí [Multi-head Attention] ‚Üí Add \u0026 Norm ‚Üí [FFN] ‚Üí Add \u0026 Norm ‚Üí Output Transformers stack these layers 12‚Äì96 times depending on size (BERT-base vs GPT-4 scale).\nüß™ Why Attention Works Position-invariant: Doesn‚Äôt care where a word is, just what it‚Äôs related to Parallelizable: Unlike RNNs Interpretable: You can visualize what the model is ‚Äúlooking at‚Äù üß† Final Thoughts Attention isn‚Äôt just a component ‚Äî it is the innovation that powers modern LLMs.\nFrom GPT‚Äôs self-attention to BERT‚Äôs bidirectional masking, every major NLP breakthrough builds on this core idea:\n‚ÄúPay attention to what matters ‚Äî and learn how to pay attention.‚Äù\nIn upcoming posts, I‚Äôll dive into positional encodings, attention visualization, and how LoRA modifies attention layers.\n‚Äî Akshat\n","wordCount":"579","inLanguage":"en","datePublished":"2024-08-15T09:00:00+01:00","dateModified":"2024-08-15T09:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://akshat4112.github.io/posts/attention-in-transformers/"},"publisher":{"@type":"Organization","name":"Akshat Gupta","logo":{"@type":"ImageObject","url":"https://akshat4112.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://akshat4112.github.io/ accesskey=h title="Akshat Gupta (Alt + H)">Akshat Gupta</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch></ul></div></div><ul id=menu><li><a href=https://akshat4112.github.io/posts title=Posts><span>Posts</span></a></li><li><a href=https://akshat4112.github.io/publications/ title=Publications><span>Publications</span></a></li><li><a href=https://akshat4112.github.io/talks/ title=Talks><span>Talks</span></a></li><li><a href=https://akshat4112.github.io/events/ title=Events><span>Events</span></a></li><li><a href=https://akshat4112.github.io/about/ title=About><span>About</span></a></li><li><a href="https://drive.google.com/file/d/1Qj6kaZXM40ixUgOAewhZlTj57piWERSw/view?usp=drive_link" title=CV><span>CV</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://akshat4112.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://akshat4112.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Understanding Attention in Transformers: The Core of Modern NLP</h1><div class=post-meta><span title='2024-08-15 09:00:00 +0100 +0100'>August 15, 2024</span>&nbsp;¬∑&nbsp;3 min</div></header><div class=post-content><p>When people say &ldquo;Transformers revolutionized NLP,&rdquo; what they <em>really</em> mean is:</p><blockquote><p><strong>Attention</strong> revolutionized NLP.</p></blockquote><p>From GPT and BERT to LLaMA and Claude, <strong>attention mechanisms</strong> are the beating heart of modern large language models.</p><p>But what exactly is attention? Why is it so powerful? And how many types are there?</p><p>Let&rsquo;s dive in.</p><hr><h2 id=-what-is-attention>üß† What is Attention?<a hidden class=anchor aria-hidden=true href=#-what-is-attention>#</a></h2><p>In the simplest sense, <strong>attention is a way for a model to focus on the most relevant parts of the input when generating output</strong>.</p><p>It answers:</p><blockquote><p>&ldquo;Given this word, which other words should I pay attention to ‚Äî and how much?&rdquo;</p></blockquote><hr><h2 id=-the-scaled-dot-product-attention>üî¢ The Scaled Dot-Product Attention<a hidden class=anchor aria-hidden=true href=#-the-scaled-dot-product-attention>#</a></h2><p>Let&rsquo;s break it down mathematically.</p><p>Given:</p><ul><li>Query matrix $Q \in \mathbb{R}^{n \times d_k}$</li><li>Key matrix $K \in \mathbb{R}^{n \times d_k}$</li><li>Value matrix $V \in \mathbb{R}^{n \times d_v}$</li></ul><p>The attention output is:</p><p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V$$</p><ul><li>$QK^\top$: Dot product measures similarity</li><li>$\sqrt{d_k}$: Scaling factor to avoid large softmax values</li><li>softmax: Turns similarity into attention weights</li><li>$V$: Weighted sum of value vectors</li></ul><p>üìñ Citation: <a href=https://arxiv.org/abs/1706.03762>Vaswani et al., 2017</a> (Attention is All You Need)</p><hr><h2 id=-multi-head-attention>üîÅ Multi-Head Attention<a hidden class=anchor aria-hidden=true href=#-multi-head-attention>#</a></h2><p>Instead of applying one attention function, we apply it <strong>multiple times in parallel</strong> with different projections.</p><p>$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, &mldr;, \text{head}_h)W^O$$
$$\text{where head}_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$</p><p>Each head learns different types of relationships (e.g., syntactic, semantic).</p><hr><h2 id=-types-of-attention-in-transformers>üß© Types of Attention in Transformers<a hidden class=anchor aria-hidden=true href=#-types-of-attention-in-transformers>#</a></h2><p>Let&rsquo;s look at the key attention variations used in different transformer architectures.</p><h3 id=1-self-attention>1. <strong>Self-Attention</strong><a hidden class=anchor aria-hidden=true href=#1-self-attention>#</a></h3><ul><li>Query, Key, and Value come from the <strong>same input</strong>.</li><li>Used in encoder and decoder blocks.</li><li>Each token attends to <strong>every other token</strong> (or just previous ones in causal attention).</li></ul><p>$$\text{SelfAttention}(X) = \text{Attention}(X, X, X)$$</p><h3 id=2-cross-attention>2. <strong>Cross-Attention</strong><a hidden class=anchor aria-hidden=true href=#2-cross-attention>#</a></h3><ul><li>Used in encoder-decoder models like T5 or BART.</li><li>Query comes from decoder, Key & Value come from encoder output.</li></ul><p>$$\text{CrossAttention}(Q_{\text{decoder}}, K_{\text{encoder}}, V_{\text{encoder}})$$</p><h3 id=3-masked-causal-attention>3. <strong>Masked (Causal) Attention</strong><a hidden class=anchor aria-hidden=true href=#3-masked-causal-attention>#</a></h3><ul><li>Used in autoregressive models like GPT.</li><li>Prevents tokens from attending to future tokens.</li><li>Enforced using a <strong>triangular mask</strong>.</li></ul><p>$$\text{MaskedAttention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + M\right)V$$
where $M$ is a mask with $-\infty$ in upper triangle.</p><h3 id=4-local-attention>4. <strong>Local Attention</strong><a hidden class=anchor aria-hidden=true href=#4-local-attention>#</a></h3><ul><li>Each token only attends to a <strong>local window</strong> (e.g., ¬±128 tokens).</li><li>Reduces compute from $O(n^2)$ to $O(nw)$</li></ul><p>Used in models like Longformer (<a href=https://arxiv.org/abs/2004.05150>Beltagy et al., 2020</a>)</p><h3 id=5-sparse-attention>5. <strong>Sparse Attention</strong><a hidden class=anchor aria-hidden=true href=#5-sparse-attention>#</a></h3><ul><li>Instead of full attention, use pre-defined patterns (e.g., strided, global).</li><li>Reduces memory usage.</li></ul><p>Examples:</p><ul><li><strong>BigBird</strong> (<a href=https://arxiv.org/abs/2007.14062>Zaheer et al., 2020</a>)</li><li><strong>Reformer</strong> (<a href=https://arxiv.org/abs/2001.04451>Kitaev et al., 2020</a>)</li></ul><h3 id=6-linear--kernelized-attention>6. <strong>Linear / Kernelized Attention</strong><a hidden class=anchor aria-hidden=true href=#6-linear--kernelized-attention>#</a></h3><ul><li>Approximates attention with linear complexity.</li><li>Replace softmax with kernel function:
$$\text{Attention}(Q, K, V) = \phi(Q)(\phi(K)^\top V)$$</li></ul><p>Used in <strong>Performer</strong> (<a href=https://arxiv.org/abs/2009.14794>Choromanski et al., 2020</a>)</p><h3 id=7-memory-augmented-attention>7. <strong>Memory-Augmented Attention</strong><a hidden class=anchor aria-hidden=true href=#7-memory-augmented-attention>#</a></h3><ul><li>Adds <strong>external memory vectors</strong> (e.g., key-value cache, documents).</li><li>Popular in <strong>RAG</strong> and <strong>MoE</strong> systems.</li></ul><hr><h2 id=-attention-block-in-transformers>üèóÔ∏è Attention Block in Transformers<a hidden class=anchor aria-hidden=true href=#-attention-block-in-transformers>#</a></h2><p>Each Transformer layer consists of:</p><ol><li><strong>Multi-head Attention</strong></li><li><strong>Add & Layer Norm</strong></li><li><strong>Feed Forward Network</strong></li><li><strong>Add & Layer Norm</strong></li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>Input ‚Üí [Multi-head Attention] ‚Üí Add &amp; Norm ‚Üí [FFN] ‚Üí Add &amp; Norm ‚Üí Output
</span></span></code></pre></div><p>Transformers stack these layers 12‚Äì96 times depending on size (BERT-base vs GPT-4 scale).</p><h2 id=-why-attention-works>üß™ Why Attention Works<a hidden class=anchor aria-hidden=true href=#-why-attention-works>#</a></h2><ul><li><strong>Position-invariant</strong>: Doesn&rsquo;t care where a word is, just what it&rsquo;s related to</li><li><strong>Parallelizable</strong>: Unlike RNNs</li><li><strong>Interpretable</strong>: You can visualize what the model is &ldquo;looking at&rdquo;</li></ul><h2 id=-final-thoughts>üß† Final Thoughts<a hidden class=anchor aria-hidden=true href=#-final-thoughts>#</a></h2><p>Attention isn&rsquo;t just a component ‚Äî it is the innovation that powers modern LLMs.</p><p>From GPT&rsquo;s self-attention to BERT&rsquo;s bidirectional masking, every major NLP breakthrough builds on this core idea:</p><blockquote><p>&ldquo;Pay attention to what matters ‚Äî and learn how to pay attention.&rdquo;</p></blockquote><p>In upcoming posts, I&rsquo;ll dive into positional encodings, attention visualization, and how LoRA modifies attention layers.</p><p>‚Äî Akshat</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://akshat4112.github.io/tags/transformers/>Transformers</a></li><li><a href=https://akshat4112.github.io/tags/attention/>Attention</a></li><li><a href=https://akshat4112.github.io/tags/deep-learning/>Deep-Learning</a></li><li><a href=https://akshat4112.github.io/tags/nlp/>NLP</a></li><li><a href=https://akshat4112.github.io/tags/self-attention/>Self-Attention</a></li><li><a href=https://akshat4112.github.io/tags/neural-networks/>Neural-Networks</a></li><li><a href=https://akshat4112.github.io/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://akshat4112.github.io/posts/rag-and-llms/><span class=title>¬´ Prev</span><br><span>RAG and LLMs: Teaching Large Models to Use External Knowledge</span>
</a><a class=next href=https://akshat4112.github.io/posts/model-extraction-attacks/><span class=title>Next ¬ª</span><br><span>Model Extraction Attacks: How Hackers Steal AI Models</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Attention in Transformers: The Core of Modern NLP on x" href="https://x.com/intent/tweet/?text=Understanding%20Attention%20in%20Transformers%3a%20The%20Core%20of%20Modern%20NLP&amp;url=https%3a%2f%2fakshat4112.github.io%2fposts%2fattention-in-transformers%2f&amp;hashtags=transformers%2cattention%2cdeep-learning%2cNLP%2cself-attention%2cneural-networks%2cAI"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Attention in Transformers: The Core of Modern NLP on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fakshat4112.github.io%2fposts%2fattention-in-transformers%2f&amp;title=Understanding%20Attention%20in%20Transformers%3a%20The%20Core%20of%20Modern%20NLP&amp;summary=Understanding%20Attention%20in%20Transformers%3a%20The%20Core%20of%20Modern%20NLP&amp;source=https%3a%2f%2fakshat4112.github.io%2fposts%2fattention-in-transformers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Attention in Transformers: The Core of Modern NLP on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fakshat4112.github.io%2fposts%2fattention-in-transformers%2f&title=Understanding%20Attention%20in%20Transformers%3a%20The%20Core%20of%20Modern%20NLP"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Attention in Transformers: The Core of Modern NLP on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fakshat4112.github.io%2fposts%2fattention-in-transformers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Attention in Transformers: The Core of Modern NLP on whatsapp" href="https://api.whatsapp.com/send?text=Understanding%20Attention%20in%20Transformers%3a%20The%20Core%20of%20Modern%20NLP%20-%20https%3a%2f%2fakshat4112.github.io%2fposts%2fattention-in-transformers%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Attention in Transformers: The Core of Modern NLP on telegram" href="https://telegram.me/share/url?text=Understanding%20Attention%20in%20Transformers%3a%20The%20Core%20of%20Modern%20NLP&amp;url=https%3a%2f%2fakshat4112.github.io%2fposts%2fattention-in-transformers%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Attention in Transformers: The Core of Modern NLP on ycombinator" href="https://news.ycombinator.com/submitlink?t=Understanding%20Attention%20in%20Transformers%3a%20The%20Core%20of%20Modern%20NLP&u=https%3a%2f%2fakshat4112.github.io%2fposts%2fattention-in-transformers%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://akshat4112.github.io/>Akshat Gupta</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>