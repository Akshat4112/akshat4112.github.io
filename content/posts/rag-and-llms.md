---
title: "RAG and LLMs: Teaching Large Models to Use External Knowledge"
date: 2024-07-15T09:00:00+01:00
draft: false
tags: ["RAG", "LLM", "vector-databases", "embeddings", "AI", "information-retrieval", "generative-ai"]
weight: 111
math: true
---

Large Language Models (LLMs) like GPT or LLaMA are great at generating text. But there's a catch:  
They **only know what they were trained on**, and that knowledge is frozen at training time.

So what happens when you ask them something from after their training cutoff? Or something super niche, like a policy from your internal HR docs?

Enter **RAG** â€“ Retrieval-Augmented Generation.  
A technique that combines LLMs with a **search engine**, enabling them to look up facts on the fly.

---

## ðŸ§  What is RAG?

**RAG** (Retrieval-Augmented Generation) is a framework that augments the input to an LLM with **retrieved documents or chunks** from an external knowledge source.

Instead of relying solely on the modelâ€™s internal weights, RAG pulls in real, current, or domain-specific content to ground its responses.

> Think of it like this:  
> "Before answering, the model Googles the topic â€” and *then* responds."

---

## ðŸ› ï¸ How Does RAG Work?

The RAG architecture typically has three stages:

1. **Query Understanding**  
   - User asks a question (e.g., _"What are the leave policies at Acme Corp?"_)

2. **Retrieval**  
   - The system converts the query into an embedding (via a model like `all-MiniLM`).
   - Searches a vector database (like **FAISS**, **Weaviate**, **Qdrant**) for top relevant chunks.

3. **Generation**  
   - The LLM gets the **original query + retrieved context**.
   - It generates a grounded, coherent answer.

This creates a dynamic pipeline where the LLM can â€œlook upâ€ facts in real time.

---

## ðŸ” Why RAG is Important

| Without RAG                         | With RAG                                |
|------------------------------------|------------------------------------------|
| âŒ Hallucinations                   | âœ… Grounded answers                       |
| âŒ Outdated knowledge               | âœ… Real-time / up-to-date info            |
| âŒ Model retraining needed for updates | âœ… Just update documents                 |
| âŒ Doesnâ€™t know your internal data | âœ… Custom knowledge injected dynamically |

Itâ€™s the backbone of many **enterprise AI apps**, **chat-with-your-PDF**, **code assistants**, and **AI copilots**.

---

## ðŸ§° Tools and Frameworks for RAG

- **LangChain** â€“ End-to-end pipelines with retrieval and LLM chaining ([docs](https://docs.langchain.com/))
- **Haystack** â€“ Search-native RAG framework from deepset
- **LlamaIndex** â€“ Lightweight RAG with document loaders and query engines
- **Pinecone / Weaviate / Qdrant** â€“ Vector DBs to store and retrieve embeddings
- **FAISS** â€“ Facebook AI similarity search, blazing fast and open source

> Bonus: Use [sentence-transformers](https://www.sbert.net/) to embed documents.

---

## ðŸ“„ RAG for Custom Documents

RAG is ideal for Q&A over:

- ðŸ“ Internal policies
- ðŸ“š Academic PDFs
- ðŸ§¾ Tax or legal docs
- ðŸ’» Codebases
- ðŸ¢ HR manuals

You chunk the docs (e.g., into 500-word segments), embed them, and index in a vector DB. When the user asks a question, the system retrieves relevant chunks and passes them to the LLM for answering.

---

## ðŸ§  When to Use RAG vs Fine-Tuning

| Situation                              | Technique       |
|----------------------------------------|------------------|
| Need accurate info from private docs   | âœ… RAG            |
| Need tone/style/domain adaptation      | ðŸ” LoRA or finetune |
| Need dynamic updates (e.g., news)      | âœ… RAG            |
| Have small structured data             | ðŸ”„ Toolformer / APIs |
| Want to reduce hallucinations          | âœ… RAG + prompt tuning |

Often, combining **RAG + LoRA fine-tuning** gives the best of both worlds.

---

## âš ï¸ RAG Challenges

- **Chunking strategy** matters a lot (sentence, paragraph, or overlap-based?)
- Embedding quality impacts retrieval quality
- Long documents can lead to token limits â†’ need summarization or re-ranking
- LLM may still hallucinate *within* retrieved context (e.g., wrong interpretation)

> Tip: Always show the **source** in the final answer to improve trust.

---

## ðŸ”„ Variants of RAG

- **Hybrid RAG**: Combines semantic + keyword search
- **Multi-hop RAG**: Chain multiple retrieval steps for complex reasoning
- **Self-RAG**: LLM rewrites query before retrieval to improve results ([Meta AI, 2023](https://arxiv.org/abs/2302.07296))
- **Agentic RAG**: Agents explore document tree and reason before answering

---

## ðŸ§  Final Thoughts

RAG is one of the most **practical, scalable, and production-ready** ways to supercharge LLMs with real-world knowledge.

Instead of hoping your LLM "remembers" something from training, just **tell it what it needs to know**.

The future of LLM applications isnâ€™t just *smarter models*, itâ€™s *smarter context*. And RAG is the backbone of that.

â€” **Akshat**
