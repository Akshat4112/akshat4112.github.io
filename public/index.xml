<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Akshat Gupta</title>
    <link>https://akshat4112.github.io/</link>
    <description>Recent content on Akshat Gupta</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 05 May 2025 09:00:00 +0100</lastBuildDate>
    <atom:link href="https://akshat4112.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Cyber Valley AI Startup Bootcamp</title>
      <link>https://akshat4112.github.io/events/cyber_valley_ai_bootcamp_stuttgart_tubingen_zurich_2023/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akshat4112.github.io/events/cyber_valley_ai_bootcamp_stuttgart_tubingen_zurich_2023/</guid>
      <description>Stuttgart, Tubingen, Zurich</description>
    </item>
    <item>
      <title>What are Diffusion Models?</title>
      <link>https://akshat4112.github.io/posts/what_are_diffusion_models/</link>
      <pubDate>Thu, 15 Feb 2024 09:00:00 +0100</pubDate>
      <guid>https://akshat4112.github.io/posts/what_are_diffusion_models/</guid>
      <description>Generative modeling is currently one of the most thrilling domains in deep learning research. Traditional models like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) have already demonstrated impressive capabilities in synthetically generating realistic data, such as images and text. However, diffusion models is swiftly gaining prominence as a powerful model in the arena of high-quality and stable generative modeling. This blog explores diffusion models, examining their operational mechanisms, architectural designs, training processes, sampling methods, and the key advantages that position them at the forefront of generative AI.</description>
    </item>
    <item>
      <title>Fairness in Machine Learning</title>
      <link>https://akshat4112.github.io/posts/fairness_in_machine_learning_/</link>
      <pubDate>Sun, 15 Oct 2023 09:00:00 +0100</pubDate>
      <guid>https://akshat4112.github.io/posts/fairness_in_machine_learning_/</guid>
      <description>As machine learning systems are increasingly used in critical areas like finance, employment, and criminal justice, it&amp;rsquo;s essential to ensure these models are fair and do not discriminate against certain groups. In this post, I will explore the concept of fairness in machine learning.
Defining Fairness Fairness in machine learning can be understood in several ways:
Group Fairness: This implies equal treatment or outcomes for different groups categorized by sensitive attributes like race or gender.</description>
    </item>
    <item>
      <title>GlyphNet: Homoglyph domains dataset and detection using attention-based Convolutional Neural Networks</title>
      <link>https://akshat4112.github.io/publications/glyphnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akshat4112.github.io/publications/glyphnet/</guid>
      <description>Paper
Authors: Akshat Gupta, Laxman Singh Tomar, Ridhima Garg
Abstract Cyber attacks deceive machines into believing something that does not exist in the first place. However, there are some to which even humans fall prey. One such famous attack that attackers have used over the years to exploit the vulnerability of vision is known to be a Homoglyph attack. It employs a primary yet effective mechanism to create illegitimate domains that are hard to differentiate from legit ones.</description>
    </item>
    <item>
      <title>MESH Hackathon</title>
      <link>https://akshat4112.github.io/events/mesh_hackathon_stuttgart_2023/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akshat4112.github.io/events/mesh_hackathon_stuttgart_2023/</guid>
      <description>Stuttgart, Germany</description>
    </item>
    <item>
      <title>Recap of Intel&#39;s Machine Learning Workshop at Dr. Akhilesh Das Gupta Institute</title>
      <link>https://akshat4112.github.io/talks/intel_machine_learning_workshop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akshat4112.github.io/talks/intel_machine_learning_workshop/</guid>
      <description>Delhi, India</description>
    </item>
    <item>
      <title>Hands-on Deep Learning with Tensorflow 2.0</title>
      <link>https://akshat4112.github.io/publications/deep_learning_with_tensorflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akshat4112.github.io/publications/deep_learning_with_tensorflow/</guid>
      <description></description>
    </item>
    <item>
      <title>Role of Natural Language Processing in Healthcare</title>
      <link>https://akshat4112.github.io/talks/nlp_in_healthcare/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akshat4112.github.io/talks/nlp_in_healthcare/</guid>
      <description>Poornima University, Jaipur, Rajasthan, India</description>
    </item>
    <item>
      <title>Smart India Hackathon 2017: Minitry of Earth Sciences</title>
      <link>https://akshat4112.github.io/events/smart_india_hackathon_2017/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akshat4112.github.io/events/smart_india_hackathon_2017/</guid>
      <description>Chennai, Tamil Nadu, India</description>
    </item>
    <item>
      <title>What is a Vector Database?</title>
      <link>https://akshat4112.github.io/posts/vector-databases/</link>
      <pubDate>Fri, 15 Dec 2023 09:00:00 +0100</pubDate>
      <guid>https://akshat4112.github.io/posts/vector-databases/</guid>
      <description>If you&amp;rsquo;ve been working with modern AI systems ‚Äî particularly in the realm of Large Language Models (LLMs), image embeddings, or recommendation engines ‚Äî you&amp;rsquo;ve probably heard of vector databases. But what are they really? And why is everyone in the ML community suddenly so excited about them?
Let me break it down in simple terms, along with how I&amp;rsquo;ve been exploring them in my own projects.
üîç The Problem: Why Traditional Databases Fall Short Traditional databases (like PostgreSQL or MongoDB) are great when you&amp;rsquo;re dealing with exact matches or relational queries:</description>
    </item>
    <item>
      <title>Introduction to Artificial Intelligence Workshop in Ujjain</title>
      <link>https://akshat4112.github.io/talks/kips_artificial_intelligence_workshop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akshat4112.github.io/talks/kips_artificial_intelligence_workshop/</guid>
      <description>Ujjain, Madhya Pradesh, India</description>
    </item>
    <item>
      <title>What is an Ontology in a Knowledge Graph?</title>
      <link>https://akshat4112.github.io/posts/ontology-in-knowledge-graphs/</link>
      <pubDate>Mon, 15 Jan 2024 09:00:00 +0100</pubDate>
      <guid>https://akshat4112.github.io/posts/ontology-in-knowledge-graphs/</guid>
      <description>If you&amp;rsquo;re working with knowledge graphs, one term that keeps popping up is ontology. Sounds academic, right? Like something you&amp;rsquo;d find buried in a philosophy textbook.
But in the world of AI, data science, and search engines, an ontology is far from abstract ‚Äî it&amp;rsquo;s the blueprint that gives your knowledge graph meaning. Let&amp;rsquo;s break it down and explore how it all fits together.
üß† What Is an Ontology (in AI)?</description>
    </item>
    <item>
      <title>LLM Fine-Tuning and LoRA: Making Large Models Work for You</title>
      <link>https://akshat4112.github.io/posts/llm-fine-tuning-lora/</link>
      <pubDate>Sun, 20 Apr 2025 17:30:00 +0200</pubDate>
      <guid>https://akshat4112.github.io/posts/llm-fine-tuning-lora/</guid>
      <description>As powerful as large language models (LLMs) like GPT, LLaMA, and Mistral are, they‚Äôre still general-purpose. If you want to make them truly useful for your domain‚Äîwhether it‚Äôs legal documents, financial analysis, or German tax law‚Äîyou need to fine-tune them.
And thanks to a technique called LoRA (Low-Rank Adaptation), you can now fine-tune LLMs with a fraction of the data, compute, and cost.
üîß What is Fine-Tuning? Fine-tuning is the process of continuing the training of a pre-trained LLM on your own dataset so that it learns domain-specific patterns, vocabulary, tone, or tasks.</description>
    </item>
    <item>
      <title>What Are Knowledge Graphs?</title>
      <link>https://akshat4112.github.io/posts/what-are-knowledge-graphs/</link>
      <pubDate>Fri, 15 Mar 2024 09:00:00 +0100</pubDate>
      <guid>https://akshat4112.github.io/posts/what-are-knowledge-graphs/</guid>
      <description>We hear the term knowledge graph everywhere now ‚Äî from Google Search to enterprise AI to GenAI apps. But what exactly is a knowledge graph, and why is everyone suddenly obsessed with it?
In this post, I&amp;rsquo;ll break down knowledge graphs in plain language: what they are, how they work, and how I use them in my own projects.
üß± The Basics: What Is a Knowledge Graph? At its core, a knowledge graph is a network of real-world entities (people, places, things) and the relationships between them.</description>
    </item>
    <item>
      <title>Prompt Engineering: The Art of Talking to AI</title>
      <link>https://akshat4112.github.io/posts/prompt-engineering/</link>
      <pubDate>Mon, 15 Apr 2024 09:00:00 +0100</pubDate>
      <guid>https://akshat4112.github.io/posts/prompt-engineering/</guid>
      <description>We&amp;rsquo;ve all played with ChatGPT, Copilot, or Claude ‚Äî typing in questions and marveling at their responses. But behind the scenes, there&amp;rsquo;s a powerful craft at play: prompt engineering.
It&amp;rsquo;s not just about &amp;ldquo;asking a question.&amp;rdquo; It&amp;rsquo;s about how you phrase it, structure it, and guide the model. Prompt engineering is the new programming skill ‚Äî and it&amp;rsquo;s transforming how we interact with AI.
üß† What Is Prompt Engineering? Prompt engineering is the process of designing effective input prompts that guide large language models (LLMs) like GPT-4 to produce accurate, helpful, or creative outputs.</description>
    </item>
    <item>
      <title>Evaluating LLMs: How Do You Measure a Model&#39;s Mind?</title>
      <link>https://akshat4112.github.io/posts/evaluating-llms/</link>
      <pubDate>Sat, 15 Jun 2024 09:00:00 +0100</pubDate>
      <guid>https://akshat4112.github.io/posts/evaluating-llms/</guid>
      <description>As large language models (LLMs) become central to search, productivity tools, education, and coding, evaluating them is no longer optional. You have to ask:
Is this model reliable? Accurate? Safe? Biased? Smart enough for my task?
But here&amp;rsquo;s the catch: LLMs are not deterministic functions. They generate free-form text, can be right in one sentence and wrong in the next ‚Äî and vary wildly depending on the prompt.
So how do we evaluate them meaningfully?</description>
    </item>
    <item>
      <title>RAG and LLMs: Teaching Large Models to Use External Knowledge</title>
      <link>https://akshat4112.github.io/posts/rag-and-llms/</link>
      <pubDate>Mon, 15 Jul 2024 09:00:00 +0100</pubDate>
      <guid>https://akshat4112.github.io/posts/rag-and-llms/</guid>
      <description>Large Language Models (LLMs) like GPT or LLaMA are great at generating text. But there&amp;rsquo;s a catch:
They only know what they were trained on, and that knowledge is frozen at training time.
So what happens when you ask them something from after their training cutoff? Or something super niche, like a policy from your internal HR docs?
Enter RAG ‚Äì Retrieval-Augmented Generation.
A technique that combines LLMs with a search engine, enabling them to look up facts on the fly.</description>
    </item>
    <item>
      <title>Understanding Attention in Transformers: The Core of Modern NLP</title>
      <link>https://akshat4112.github.io/posts/attention-in-transformers/</link>
      <pubDate>Thu, 15 Aug 2024 09:00:00 +0100</pubDate>
      <guid>https://akshat4112.github.io/posts/attention-in-transformers/</guid>
      <description>When people say &amp;ldquo;Transformers revolutionized NLP,&amp;rdquo; what they really mean is:
Attention revolutionized NLP.
From GPT and BERT to LLaMA and Claude, attention mechanisms are the beating heart of modern large language models.
But what exactly is attention? Why is it so powerful? And how many types are there?
Let&amp;rsquo;s dive in.
üß† What is Attention? In the simplest sense, attention is a way for a model to focus on the most relevant parts of the input when generating output.</description>
    </item>
    <item>
      <title>Model Extraction Attacks: How Hackers Steal AI Models</title>
      <link>https://akshat4112.github.io/posts/model-extraction-attacks/</link>
      <pubDate>Sun, 15 Sep 2024 09:00:00 +0100</pubDate>
      <guid>https://akshat4112.github.io/posts/model-extraction-attacks/</guid>
      <description>In the world of machine learning, especially with the rise of large language models (LLMs) and deep neural networks, model extraction attacks are a growing concern. These attacks aim to replicate the behavior of a machine learning model by querying it and then using the responses to reverse-engineer its underlying architecture and parameters.
What is a Model Extraction Attack? A model extraction attack occurs when an adversary tries to replicate a machine learning model by making repeated queries to it and analyzing its responses.</description>
    </item>
    <item>
      <title>Speaker Anonymization: Protecting Voice Identity in the AI Era</title>
      <link>https://akshat4112.github.io/posts/speaker-anonymization/</link>
      <pubDate>Tue, 15 Oct 2024 09:00:00 +0100</pubDate>
      <guid>https://akshat4112.github.io/posts/speaker-anonymization/</guid>
      <description>Speaker anonymization refers to the process of modifying the characteristics of a speaker&amp;rsquo;s voice so that the speaker&amp;rsquo;s identity cannot be easily determined while preserving the speech&amp;rsquo;s intelligibility. With the increasing usage of speech data in virtual assistants, surveillance systems, and other applications, ensuring privacy in speech data has become critical.
In this post, we&amp;rsquo;ll dive into the technical details of speaker anonymization techniques, including implementation approaches using machine learning, deep learning models, and popular libraries.</description>
    </item>
    <item>
      <title>LLM Agents: Building AI Systems That Can Reason and Act</title>
      <link>https://akshat4112.github.io/posts/llm-agents/</link>
      <pubDate>Mon, 05 May 2025 09:00:00 +0100</pubDate>
      <guid>https://akshat4112.github.io/posts/llm-agents/</guid>
      <description>Understanding LLM Agents: The Future of Autonomous AI Systems Large Language Models (LLMs), like GPT-3, GPT-4, and others, have taken the world by storm due to their impressive language generation and understanding capabilities. However, when these models are augmented with decision-making capabilities, memory, and actions in specific environments, they become even more powerful. Enter LLM Agents ‚Äî autonomous systems built on top of large language models to perform tasks, make decisions, and act autonomously based on user instructions.</description>
    </item>
    <item>
      <title>About</title>
      <link>https://akshat4112.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akshat4112.github.io/about/</guid>
      <description>I&amp;rsquo;m a machine learning engineer and my research interests are in generative modelling, speech processing, text analysis, machine learning, and deep learning. Currently pursuing a master&amp;rsquo;s from the University of Stuttgart, my major is in computational linguistics.
Current work Research work includes denoising diffusion on speaker embeddings, explainability of CRF-LSTM-based NER models, temporal and spatial word embeddings, and homoglyph detection using attention-based CNNs.
My work includes Before this, I worked at Quantiphi, Scanta, and Mobile Programming LLC as a machine learning engineer and worked on speaker diarization, multimodal sentiment analysis, NLP augmentor, clinical ner, chatbot using AWS Lex and Dialogflow, customer care call analytics, neural machine translation.</description>
    </item>
    <item>
      <title>GlyphNet: Homoglyph domains dataset and detection using attention-based Convolutional Neural Networks</title>
      <link>https://akshat4112.github.io/publication/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akshat4112.github.io/publication/</guid>
      <description></description>
    </item>
  </channel>
</rss>
