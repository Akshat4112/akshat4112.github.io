<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformers on Akshat Gupta</title>
    <link>http://localhost:50281/tags/transformers/</link>
    <description>Recent content in Transformers on Akshat Gupta</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 20 Apr 2025 17:30:00 +0200</lastBuildDate>
    <atom:link href="http://localhost:50281/tags/transformers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM Fine-Tuning and LoRA: Making Large Models Work for You</title>
      <link>http://localhost:50281/posts/llm-fine-tuning-lora/</link>
      <pubDate>Sun, 20 Apr 2025 17:30:00 +0200</pubDate>
      <guid>http://localhost:50281/posts/llm-fine-tuning-lora/</guid>
      <description>As powerful as large language models (LLMs) like GPT, LLaMA, and Mistral are, theyâ€™re still general-purpose. If you want to make them truly useful for your domainâ€”whether itâ€™s legal documents, financial analysis, or German tax lawâ€”you need to fine-tune them.
And thanks to a technique called LoRA (Low-Rank Adaptation), you can now fine-tune LLMs with a fraction of the data, compute, and cost.
ðŸ”§ What is Fine-Tuning? Fine-tuning is the process of continuing the training of a pre-trained LLM on your own dataset so that it learns domain-specific patterns, vocabulary, tone, or tasks.</description>
    </item>
    <item>
      <title>Understanding Attention in Transformers: The Core of Modern NLP</title>
      <link>http://localhost:50281/posts/attention-in-transformers/</link>
      <pubDate>Thu, 15 Aug 2024 09:00:00 +0100</pubDate>
      <guid>http://localhost:50281/posts/attention-in-transformers/</guid>
      <description>When people say &amp;ldquo;Transformers revolutionized NLP,&amp;rdquo; what they really mean is:
Attention revolutionized NLP.
From GPT and BERT to LLaMA and Claude, attention mechanisms are the beating heart of modern large language models.
But what exactly is attention? Why is it so powerful? And how many types are there?
Let&amp;rsquo;s dive in.
ðŸ§  What is Attention? In the simplest sense, attention is a way for a model to focus on the most relevant parts of the input when generating output.</description>
    </item>
  </channel>
</rss>
