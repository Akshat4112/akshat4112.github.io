<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine-Learning on Akshat Gupta</title>
    <link>https://akshat4112.github.io/tags/machine-learning/</link>
    <description>Recent content in Machine-Learning on Akshat Gupta</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 15 Sep 2024 09:00:00 +0100</lastBuildDate>
    <atom:link href="https://akshat4112.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Fairness in Machine Learning</title>
      <link>https://akshat4112.github.io/posts/fairness_in_machine_learning_/</link>
      <pubDate>Sun, 15 Oct 2023 09:00:00 +0100</pubDate>
      <guid>https://akshat4112.github.io/posts/fairness_in_machine_learning_/</guid>
      <description>As machine learning systems are increasingly used in critical areas like finance, employment, and criminal justice, it&amp;rsquo;s essential to ensure these models are fair and do not discriminate against certain groups. In this post, I will explore the concept of fairness in machine learning.
Defining Fairness Fairness in machine learning can be understood in several ways:
Group Fairness: This implies equal treatment or outcomes for different groups categorized by sensitive attributes like race or gender.</description>
    </item>
    <item>
      <title>GlyphNet: Homoglyph domains dataset and detection using attention-based Convolutional Neural Networks</title>
      <link>https://akshat4112.github.io/publications/glyphnet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akshat4112.github.io/publications/glyphnet/</guid>
      <description>Paper
Authors: Akshat Gupta, Laxman Singh Tomar, Ridhima Garg
Abstract Cyber attacks deceive machines into believing something that does not exist in the first place. However, there are some to which even humans fall prey. One such famous attack that attackers have used over the years to exploit the vulnerability of vision is known to be a Homoglyph attack. It employs a primary yet effective mechanism to create illegitimate domains that are hard to differentiate from legit ones.</description>
    </item>
    <item>
      <title>Recap of Intel&#39;s Machine Learning Workshop at Dr. Akhilesh Das Gupta Institute</title>
      <link>https://akshat4112.github.io/talks/intel_machine_learning_workshop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akshat4112.github.io/talks/intel_machine_learning_workshop/</guid>
      <description>Delhi, India</description>
    </item>
    <item>
      <title>Hands-on Deep Learning with Tensorflow 2.0</title>
      <link>https://akshat4112.github.io/publications/deep_learning_with_tensorflow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akshat4112.github.io/publications/deep_learning_with_tensorflow/</guid>
      <description></description>
    </item>
    <item>
      <title>Role of Natural Language Processing in Healthcare</title>
      <link>https://akshat4112.github.io/talks/nlp_in_healthcare/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akshat4112.github.io/talks/nlp_in_healthcare/</guid>
      <description>Poornima University, Jaipur, Rajasthan, India</description>
    </item>
    <item>
      <title>Introduction to Artificial Intelligence Workshop in Ujjain</title>
      <link>https://akshat4112.github.io/talks/kips_artificial_intelligence_workshop/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://akshat4112.github.io/talks/kips_artificial_intelligence_workshop/</guid>
      <description>Ujjain, Madhya Pradesh, India</description>
    </item>
    <item>
      <title>Model Extraction Attacks: How Hackers Steal AI Models</title>
      <link>https://akshat4112.github.io/posts/model-extraction-attacks/</link>
      <pubDate>Sun, 15 Sep 2024 09:00:00 +0100</pubDate>
      <guid>https://akshat4112.github.io/posts/model-extraction-attacks/</guid>
      <description>In the world of machine learning, especially with the rise of large language models (LLMs) and deep neural networks, model extraction attacks are a growing concern. These attacks aim to replicate the behavior of a machine learning model by querying it and then using the responses to reverse-engineer its underlying architecture and parameters.
What is a Model Extraction Attack? A model extraction attack occurs when an adversary tries to replicate a machine learning model by making repeated queries to it and analyzing its responses.</description>
    </item>
  </channel>
</rss>
