[{"content":"","permalink":"https://akshat4112.github.io/events/cyber_valley_ai_bootcamp_stuttgart_tubingen_zurich_2023/","summary":"","title":"Cyber Valley AI Startup Bootcamp"},{"content":"Generative modeling is currently one of the most thrilling domains in deep learning research. Traditional models like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) have already demonstrated impressive capabilities in synthetically generating realistic data, such as images and text. However, diffusion models is swiftly gaining prominence as a powerful model in the arena of high-quality and stable generative modeling. This blog explores diffusion models, examining their operational mechanisms, architectural designs, training processes, sampling methods, and the key advantages that position them at the forefront of generative AI.\nThe foundations of diffusion models were introduced in papers by Sohl-Dickstein et al. and Ho et al.\nHow Do Diffusion Models Work? Diffusion models function on a principal strategy of training a model to reverse a gradual process of data corruption, transitioning from a clean data point to pure noise and then back to clean data point.\nImage Source: Ho et al.\nForward Diffusion Process This corruption is executed through the forward diffusion process. It is characterized by the iterative addition of small quantities of Gaussian noise to the data \\(x_0\\) over \\(T\\) discrete time steps. This process is mathematically expressed as:\n\\[ x_t = \\sqrt{1 - \\beta_t} \\times x_{t-1} + \\sqrt{\\beta_t} \\times \\epsilon \\]\nIn this equation, \\(x_t\\) represents the partially corrupted data at time \\(t\\), \\(\\beta_t\\) is the noise parameter that controls variance, and \\(\\epsilon\\) signifies Gaussian noise. As \\(t\\) increases, \\(\\beta_t\\) is progressively annealed from a lower to higher value, methodically erasing the structure in \\(x_0\\) until it becomes unrecognizable noise \\(x_T\\).\nReverse Diffusion Process The core of the diffusion model is a neural network trained to precisely reverse this forward process. It accepts \\(x_t\\) as input and predicts \\(x_{t-1}\\), the less noisy version from the preceding step. By consecutively predicting each step in reverse, the model systematically denoises \\(x_T\\) back into a clear sample \\(x_0\\).\nA critical aspect of this training involves denoising score matching, where the model aims to predict the gradient of the log-probability of \\(x_{t-1}\\) given \\(x_t\\). This approach, relying on noise data from the forward process, is key to the model\u0026rsquo;s stable training.\nDiffusion Model Architectures Diffusion models commonly utilize convolutional neural networks, particularly U-Net architectures. The contracting and expanding paths in U-Nets facilitate both local and global attention to the noise, yielding high-quality outputs.\nConditional variants of these models integrate class embeddings at intermediate layers, allowing for controlled sampling of specific classes of data, like images of diverse objects.\nTraining Process Diffusion models are distinct in that they are trained using a denoising score matching loss rather than a standard likelihood loss. During training, the model is exposed only to artificially noised data from the forward process, never to clean data. This approach enables large-scale stable training and mitigates issues like mode collapse, common in likelihood-based models such as GANs, especially on diverse datasets. The iterative training on noise data endows diffusion models with robust generative capabilities.\nSampling Methods To generate samples from a trained diffusion model, one starts with pure noise \\(x_T\\) and progressively predicts \\(x_{T-1}\\), \\(x_{T-2}\\), and so on, using the model\u0026rsquo;s predicted denoising score at each step.\nThe number of sampling steps is crucial; approximately 1000 steps can recover fine details clearly, whereas fewer steps might result in distorted outputs. Post-processing techniques, such as upscaling, can further enhance the quality of the samples.\nAdvantages Over Other Models Diffusion models offer several advantages over existing generative models, making them a promising new direction in the field:\nSample Quality: The iterative denoising process facilitates the creation of high-resolution, clear samples that effectively capture the complexity of data. Training Stability: Exposure to only artificial noise data prevents collapse issues and enables scalable training. Flexible Control: Class-conditional variants offer significant control over sampling specific data types. Parallelizable Sampling: Each step in the sampling process can be efficiently parallelized across GPUs. Current Limitations Despite their advantages, diffusion models do face certain limitations:\nSampling is computationally intensive, requiring hundreds of passes. Numerous hyperparameters related to the noise schedule needs careful tuning. Class-conditional guidance is limited compared to the desired level of control. The Future of Generative AI Diffusion models are exceptionally promising as generative models due to both sample quality and training stability. With rapid innovations in architecture, hyperparameters, and sampling techniques, they represent the new frontier in generative modeling with vast potentials still to be unlocked. As research continues, we can expect diffusion models to become even more powerful and flexible generative tools.\n","permalink":"https://akshat4112.github.io/posts/what_are_diffusion_models/","summary":"Generative modeling is currently one of the most thrilling domains in deep learning research. Traditional models like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) have already demonstrated impressive capabilities in synthetically generating realistic data, such as images and text. However, diffusion models is swiftly gaining prominence as a powerful model in the arena of high-quality and stable generative modeling. This blog explores diffusion models, examining their operational mechanisms, architectural designs, training processes, sampling methods, and the key advantages that position them at the forefront of generative AI.","title":"What are Diffusion Models?"},{"content":" As machine learning systems are increasingly used in critical areas like finance, employment, and criminal justice, it\u0026rsquo;s essential to ensure these models are fair and do not discriminate against certain groups. In this post, I will explore the concept of fairness in machine learning.\nDefining Fairness Fairness in machine learning can be understood in several ways:\nGroup Fairness: This implies equal treatment or outcomes for different groups categorized by sensitive attributes like race or gender. For instance, ensuring a loan application system doesn\u0026rsquo;t have a higher false rejection rate for one gender compared to another.\nIndividual Fairness: This means that similar individuals should receive similar predictions or decisions, irrespective of their group membership. Two individuals with comparable financial backgrounds should get similar credit scores, regardless of their ethnicity or gender.\nCausal Fairness: Defined using causal modeling, it ensures similar predictions for individuals who would exhibit similar outcomes under different treatments. For example, a person\u0026rsquo;s chances of getting a job should not be influenced by their gender.\nSources of Unfairness Unfairness in machine learning models can arise from several factors:\nBiased Training Data: If the training data reflects historical human biases, the model will likely inherit these biases. Using Protected Variables: Direct use of attributes like race or gender in models can lead to disparate treatment. Proxy Variables: Models may learn to discriminate using variables correlated with protected attributes, like zip codes. Skewed Test Performance: Poor model performance on minority groups due to imbalanced datasets. Incorrect Similarity Metrics: Discriminatory definitions of similarity between individuals can introduce bias. Techniques to Improve Fairness Addressing unfairness involves strategies across the ML pipeline:\nPre-processing: Removing biases in training data and identifying proxy variables. In-processing: Modifying the model training process to incorporate fairness constraints. Post-processing: Applying techniques post-training to correct biases. Improved Evaluation: Using specific metrics to assess fairness in different contexts. Causal Modeling: Employing causal inference techniques to understand and mitigate biases. Real-World Example: Bias in Digital Recruitment Advertising A notable instance highlighting the need for fairness in AI was observed in digital recruitment advertising. An algorithm disproportionately showed high-salary job ads to men over women, influenced by biased historical data that reflected existing employment trends. This case underscores the importance of evaluating training data for biases and the necessity for ongoing algorithmic assessment to avoid reinforcing social inequalities.\nConclusion Achieving fairness in machine learning is a complex yet vital endeavor, requiring collaboration across various fields. With careful consideration and appropriate techniques, we can develop AI systems that are both ethical and equitable.\n","permalink":"https://akshat4112.github.io/posts/fairness_in_machine_learning_/","summary":"As machine learning systems are increasingly used in critical areas like finance, employment, and criminal justice, it\u0026rsquo;s essential to ensure these models are fair and do not discriminate against certain groups. In this post, I will explore the concept of fairness in machine learning.\nDefining Fairness Fairness in machine learning can be understood in several ways:\nGroup Fairness: This implies equal treatment or outcomes for different groups categorized by sensitive attributes like race or gender.","title":"Fairness in Machine Learning"},{"content":"Paper\nAuthors: Akshat Gupta, Laxman Singh Tomar, Ridhima Garg\nAbstract Cyber attacks deceive machines into believing something that does not exist in the first place. However, there are some to which even humans fall prey. One such famous attack that attackers have used over the years to exploit the vulnerability of vision is known to be a Homoglyph attack. It employs a primary yet effective mechanism to create illegitimate domains that are hard to differentiate from legit ones. Moreover, as the difference is pretty indistinguishable for a user to notice, they cannot stop themselves from clicking on these homoglyph domain names.\nIn our work, we created GlyphNet, an image dataset that contains 4M domains, both real and homoglyphs. Additionally, we introduce a baseline method for homoglyph attack detection system using an attention-based convolutional Neural Network. We show that our model can reach state-of-the-art accuracy in detecting homoglyph attacks with a 0.93 AUC on our dataset.\nIntroduction In cyber security, attackers employ different attacks to infiltrate our systems and networks, with the objective varying from stealing crucial information to inflicting system damage. One such deceptive attack is the homoglyph attack, which involves an attacker trying to fool humans and computer systems by using characters and symbols that may appear visually similar to characters used in real domain and process names but are different.\nDataset Proposed Dataset We have proposed a dataset consisting of real and homoglyph domains. We obtained domains from the Domains Project, comprising 500M domains, restricting our work to 2M domains due to hardware restrictions.\nHomoglyph Creation Algorithm We created a novel algorithm for the generation of homoglyph domains to ensure that real homoglyphs are generated with randomness and closeness. To achieve this, we sample homoglyph noise characters using Gaussian sampling from the glyph pool.\nImage Generation Homoglyph attacks exploit the weakness of human vision to differentiate real from homoglyph domain names. We rendered images from the real and homoglyph strings generated via our algorithm.\nMethodology Experimentation Dataset and Metrics We split our dataset into train, validation, and test, with a ratio of 70:20:10, respectively. We use accuracy, precision, recall, and F1 score as our evaluation metrics, along with the AUC score.\nExperimental Settings For training, we used binary cross-entropy as a Loss Function and RMSProp Optimizer. The network is trained for 30 epochs with early stopping, using a batch size of 256.\nResults We evaluated our model on two unpaired datasets for domain names, achieving an accuracy of 0.93 and an F1-score of 0.93, outperforming other models.\nReferences Bahdanau, D.; Cho, K.; and Bengio, Y. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. Boor, V.; Overmars, M. H.; and Van Der Stappen, A. F. 1999. The Gaussian sampling strategy for probabilistic roadmap planners. In Proceedings 1999 IEEE International Conference on Robotics and Automation (Cat. No. 99CH36288C), volume 2, 1018–1023. IEEE. Cheng, L.; Liu, F.; and Yao, D. 2017. Enterprise data breach: causes, challenges, prevention, and future directions. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 7(5): e1211. Chollet, F.; et al. 2015. Keras. Damerau, F. J. 1964. A technique for computer detection and correction of spelling errors. Communications of the ACM, 7(3): 171–176. Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and FeiFei, L. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, 248–255. Ieee. Ginsberg, A.; and Yu, C. 2018. Rapid homoglyph prediction and detection. In 2018 1st International Conference on Data Intelligence and Security (ICDIS), 17–23. IEEE. Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2014. Generative adversarial nets. Advances in neural information processing systems, 27. He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, 770–778. Helms, M. M.; Ettkin, L. P.; and Morris, D. J. 2000. The risk of information compromise and approaches to prevention. The Journal of Strategic Information Systems, 9(1): 5–15. Hoffer, E.; and Ailon, N. 2015. Deep metric learning using triplet network. In International workshop on similarity-based pattern recognition, 84–92. Springer. Hong, J. 2012. The state of phishing attacks. Communications of the ACM, 55(1): 74–81. Isola, P.; Zhu, J.-Y.; Zhou, T.; and Efros, A. A. 2017. Imageto-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 1125–1134. Citation @article{gupta2023glyphnet, title={GlyphNet: Homoglyph domains dataset and detection using attention-based Convolutional Neural Networks}, author={Gupta, Akshat and Tomar, Laxman Singh and Garg, Ridhima}, journal={arXiv preprint arXiv:2306.10392}, year={2023} } ","permalink":"https://akshat4112.github.io/publications/glyphnet/","summary":"Paper\nAuthors: Akshat Gupta, Laxman Singh Tomar, Ridhima Garg\nAbstract Cyber attacks deceive machines into believing something that does not exist in the first place. However, there are some to which even humans fall prey. One such famous attack that attackers have used over the years to exploit the vulnerability of vision is known to be a Homoglyph attack. It employs a primary yet effective mechanism to create illegitimate domains that are hard to differentiate from legit ones.","title":"GlyphNet: Homoglyph domains dataset and detection using attention-based Convolutional Neural Networks"},{"content":"","permalink":"https://akshat4112.github.io/events/mesh_hackathon_stuttgart_2023/","summary":"","title":"MESH Hackathon"},{"content":"Date of Event: September 6, 2019\nVenue: Dr. Akhilesh Das Gupta Institute of Technology \u0026amp; Management\nEvent Overview: In an exhilarating day filled with insights and hands-on activities, the Machine Learning workshop organized by Intel turned out to be a landmark event for aspiring ML enthusiasts. Our auditorium, buzzing with the energy of keen learners, became a crucible for innovation and deep understanding in the rapidly evolving field of Machine Learning.\nHighlights of the Session: Practical Learning Approach: The workshop kicked off by laying a solid foundation of Machine Learning concepts, employing a top-down teaching methodology. This approach, tailored to enhance student comprehension, was supplemented by easy-to-grasp visualizations, making complex theories accessible to all attendees.\nInteractive Sessions: A significant highlight was the interactive segment where students applied ML concepts in real-time. This \u0026rsquo;learning by doing\u0026rsquo; approach not only solidified their understanding but also sparked a curiosity to explore more.\nFocus on Modern AI Challenges: Beyond the basics, the workshop delved into the broader spectrum and future challenges of AI, presenting a realistic picture of the field today.\nHigh Engagement and Enthusiasm: The packed auditorium was a testament to the success of the workshop, with students showing a blend of contentment and eagerness to learn more.\nReflections: The day\u0026rsquo;s success was evident not just in the packed hall and the enthusiastic participation, but also in the heartwarming responses received on platforms like LinkedIn. This positive feedback has left us more energized and committed to organizing similar enriching events in the future.\nLooking Forward: The workshop has set a precedent for future sessions, highlighting the importance of practical, interactive learning in the field of Machine Learning and AI. We are excited about continuing this journey and contributing to the growth of aspiring Machine Learning professionals. Stay tuned for more updates and upcoming events in this space!\n","permalink":"https://akshat4112.github.io/talks/intel_machine_learning_workshop/","summary":"Date of Event: September 6, 2019\nVenue: Dr. Akhilesh Das Gupta Institute of Technology \u0026amp; Management\nEvent Overview: In an exhilarating day filled with insights and hands-on activities, the Machine Learning workshop organized by Intel turned out to be a landmark event for aspiring ML enthusiasts. Our auditorium, buzzing with the energy of keen learners, became a crucible for innovation and deep understanding in the rapidly evolving field of Machine Learning.","title":"Recap of Intel's Machine Learning Workshop at Dr. Akhilesh Das Gupta Institute"},{"content":"","permalink":"https://akshat4112.github.io/publications/deep_learning_with_tensorflow/","summary":"","title":"Hands-on Deep Learning with Tensorflow 2.0"},{"content":"Date of Event: May 05, 2020\nVenue: Online\nEvent Overview: In May 2020, Poornima University in Rajasthan hosted a groundbreaking workshop focusing on the Role of Natural Language Processing (NLP) in Healthcare. This event provided a unique platform for healthcare professionals, technologists, and students to explore the intersection of advanced linguistic technology and healthcare applications.\nHighlights of the Workshop: Introduction to NLP in Healthcare: The workshop started with an overview of NLP and its evolving role in the healthcare sector, emphasizing how it transforms patient care and medical data analysis.\nHands-On Sessions: Participants engaged in hands-on training, where they learned to apply NLP tools in various healthcare scenarios, such as patient data interpretation, medical record analysis, and more.\nExpert-Led Discussions: Leading experts in NLP and healthcare IT shared their insights on the latest trends, challenges, and advancements in the field, fostering a deeper understanding among attendees.\nCase Studies and Applications: The event featured several case studies demonstrating successful NLP applications in healthcare, highlighting its potential in improving diagnosis, treatment, and patient engagement.\nEthical Considerations: A key part of the workshop was dedicated to discussing the ethical implications of using NLP in healthcare, including data privacy and the importance of unbiased algorithms.\nParticipant Engagement: The workshop saw active participation from attendees, who were keen to understand how NLP can revolutionize healthcare delivery. The interactive format allowed for an exchange of ideas and stimulated discussions on future innovations in this space.\nConclusion: The workshop on the Role of NLP in Healthcare at Poornima University marked an important step towards bridging the gap between technology and healthcare. It opened new avenues for innovation and collaboration, setting the stage for future breakthroughs that can significantly enhance the quality and efficiency of healthcare services.\n","permalink":"https://akshat4112.github.io/talks/nlp_in_healthcare/","summary":"Date of Event: May 05, 2020\nVenue: Online\nEvent Overview: In May 2020, Poornima University in Rajasthan hosted a groundbreaking workshop focusing on the Role of Natural Language Processing (NLP) in Healthcare. This event provided a unique platform for healthcare professionals, technologists, and students to explore the intersection of advanced linguistic technology and healthcare applications.\nHighlights of the Workshop: Introduction to NLP in Healthcare: The workshop started with an overview of NLP and its evolving role in the healthcare sector, emphasizing how it transforms patient care and medical data analysis.","title":"Role of Natural Language Processing in Healthcare"},{"content":"","permalink":"https://akshat4112.github.io/events/smart_india_hackathon_2017/","summary":"","title":"Smart India Hackathon 2017: Minitry of Earth Sciences"},{"content":"If you\u0026rsquo;ve been working with modern AI systems — particularly in the realm of Large Language Models (LLMs), image embeddings, or recommendation engines — you\u0026rsquo;ve probably heard of vector databases. But what are they really? And why is everyone in the ML community suddenly so excited about them?\nLet me break it down in simple terms, along with how I\u0026rsquo;ve been exploring them in my own projects.\n🔍 The Problem: Why Traditional Databases Fall Short Traditional databases (like PostgreSQL or MongoDB) are great when you\u0026rsquo;re dealing with exact matches or relational queries:\n\u0026ldquo;Find all users from Stuttgart\u0026rdquo; \u0026ldquo;Show me orders placed in the last 30 days\u0026rdquo; But AI doesn\u0026rsquo;t speak in exact matches. For example:\n\u0026ldquo;Images similar to a cat\u0026rdquo; \u0026ldquo;Documents related to GDPR compliance\u0026rdquo; \u0026ldquo;People with similar resume embeddings\u0026rdquo; These are all semantic queries — and you need a system that understands similarity, not just exact matches. That\u0026rsquo;s where vector databases come in.\n🧭 What Is a Vector Database? A vector database is a specialized type of database designed to store and retrieve high-dimensional vectors — the kind you get from neural network embeddings.\nFor instance:\nAn image processed by a CNN might become a 512-dimensional vector. A sentence embedding from BERT might be a 768-dimensional vector. A product recommendation engine might embed user behavior in 128 dimensions. These aren\u0026rsquo;t human-readable, but they carry meaning in a latent space. A vector database allows you to store, index, and search those vectors efficiently.\n⚙️ How Do They Work? Here\u0026rsquo;s a simplified flow:\nGenerate Embeddings: Use a model like OpenAI\u0026rsquo;s embedding API, Hugging Face Transformers, or CLIP to convert your input (text/image/etc.) into a vector. Store the Vector: Save this vector along with metadata (e.g. document ID, title, tags) in the vector DB. Perform Similarity Search: When querying, your input is also converted into a vector, and the DB finds the closest vectors using metrics like cosine similarity or Euclidean distance. This is called Approximate Nearest Neighbor (ANN) search — the core engine behind vector DBs.\n📦 Popular Vector Databases Here are a few tools I\u0026rsquo;ve worked with or explored:\nPinecone: Fully managed and cloud-native, great for production LLM workflows. Weaviate: Open-source with hybrid search (keyword + vector). FAISS (Facebook AI Similarity Search): A C++/Python library for fast similarity search. Milvus: Industrial-grade open-source vector DB built for scale. Qdrant: Rust-based, developer-friendly, with REST and gRPC APIs. Chroma: Lightweight and ideal for quick local experiments or prototyping. 🚀 Real-World Use Cases Some practical examples I\u0026rsquo;ve seen or built:\nRAG (Retrieval-Augmented Generation) pipelines: Retrieving the most relevant documents before feeding them to an LLM. Image Search: Finding visually similar images using CLIP embeddings. Voiceprint Matching: In a speaker diarization project, I embedded speaker audio and searched for similar embeddings. Semantic QA: Matching a question against a corpus of answers using dense embeddings instead of keywords. 🧪 My Learnings \u0026amp; Tips Start Small: Use FAISS or Chroma locally before scaling to managed solutions like Pinecone. Hybrid Search Rocks: Combining vector similarity with keyword search (like in Weaviate or Elasticsearch) often yields better results. Fine-Tune Embeddings: Pretrained models work well, but fine-tuning with libraries like SentenceTransformers can significantly improve relevance. Storage + Speed Tradeoffs: ANN methods sacrifice some accuracy for speed — you\u0026rsquo;ll need to balance these based on your use case. 🧩 Final Thoughts Vector databases are not just a hype — they\u0026rsquo;re a foundational layer in any serious GenAI system. From semantic search to recommendation and RAG, they enable the kind of \u0026ldquo;intelligent recall\u0026rdquo; that was previously hard to build at scale.\nIf you\u0026rsquo;re building anything involving embeddings, I strongly recommend giving one of these tools a try. Feel free to reach out if you\u0026rsquo;re stuck or want to nerd out about vector indexing strategies 😄\nThanks for reading! I\u0026rsquo;ll be posting more about building scalable GenAI pipelines and multimodal systems — stay tuned.\n— Akshat\n","permalink":"https://akshat4112.github.io/posts/vector-databases/","summary":"If you\u0026rsquo;ve been working with modern AI systems — particularly in the realm of Large Language Models (LLMs), image embeddings, or recommendation engines — you\u0026rsquo;ve probably heard of vector databases. But what are they really? And why is everyone in the ML community suddenly so excited about them?\nLet me break it down in simple terms, along with how I\u0026rsquo;ve been exploring them in my own projects.\n🔍 The Problem: Why Traditional Databases Fall Short Traditional databases (like PostgreSQL or MongoDB) are great when you\u0026rsquo;re dealing with exact matches or relational queries:","title":"What is a Vector Database?"},{"content":"Date of Event: December 17, 2019\nVenue: Ujjain\nEvent Overview: Ujjain witnessed an enriching educational event in December 2019, with a workshop dedicated to introducing the fundamentals of Artificial Intelligence (AI). Designed for beginners and enthusiasts alike, this event served as a primer to the world of AI, attracting a diverse audience from students to professionals keen on understanding this cutting-edge technology.\nHighlights of the Workshop: Fundamentals of AI: The workshop focused on the core principles of AI, providing a comprehensive introduction to its basic concepts and terminologies.\nInteractive Learning Approach: To ensure a solid grasp of AI fundamentals, the workshop included interactive sessions and practical demonstrations, making complex topics accessible to all attendees.\nExpert Speakers: Knowledgeable speakers from the field of AI shared insights and real-world applications, offering a well-rounded perspective on the impact of AI in various industries.\nCollaborative Activities: Attendees participated in group activities designed to stimulate creative thinking and problem-solving in AI contexts.\nResource Sharing: The workshop also provided resources for further learning, encouraging participants to continue their AI education beyond the event.\nParticipant Experiences: The workshop was met with enthusiasm and curiosity, as attendees expressed their appreciation for the clear and engaging presentation of AI basics. Many participants noted how the workshop demystified AI and sparked their interest in exploring the field further.\nLooking Ahead: This introductory AI workshop has set the stage for more advanced discussions and learning opportunities in Ujjain. The positive response has paved the way for future events that will delve deeper into AI technologies and their applications.\nConclusion: The \u0026lsquo;Introduction to Artificial Intelligence\u0026rsquo; workshop in Ujjain was a significant step towards fostering a knowledgeable and skilled community ready to embrace AI advancements. As AI continues to evolve, Ujjain\u0026rsquo;s tech community is well-placed to be part of this transformative journey, equipped with the foundational knowledge imparted by this successful event.\n","permalink":"https://akshat4112.github.io/talks/kips_artificial_intelligence_workshop/","summary":"Date of Event: December 17, 2019\nVenue: Ujjain\nEvent Overview: Ujjain witnessed an enriching educational event in December 2019, with a workshop dedicated to introducing the fundamentals of Artificial Intelligence (AI). Designed for beginners and enthusiasts alike, this event served as a primer to the world of AI, attracting a diverse audience from students to professionals keen on understanding this cutting-edge technology.\nHighlights of the Workshop: Fundamentals of AI: The workshop focused on the core principles of AI, providing a comprehensive introduction to its basic concepts and terminologies.","title":"Introduction to Artificial Intelligence Workshop in Ujjain"},{"content":"If you\u0026rsquo;re working with knowledge graphs, one term that keeps popping up is ontology. Sounds academic, right? Like something you\u0026rsquo;d find buried in a philosophy textbook.\nBut in the world of AI, data science, and search engines, an ontology is far from abstract — it\u0026rsquo;s the blueprint that gives your knowledge graph meaning. Let\u0026rsquo;s break it down and explore how it all fits together.\n🧠 What Is an Ontology (in AI)? In the simplest terms:\nAn ontology is a formal representation of concepts, relationships, and rules within a domain.\nIt tells your system:\nWhat things exist (like Person, Company, Product) What types of relationships they can have (worksFor, locatedIn, foundedBy) What rules govern those entities and their connections (e.g. \u0026ldquo;A Person can only work for a Company\u0026rdquo;) Think of it like a schema, but more expressive and logical — like SQL schema meets logic programming.\n🔗 How It Relates to Knowledge Graphs A knowledge graph is a collection of entities and their relationships, usually represented as: (subject) —[predicate]→ (object)\nExample: \u0026ldquo;Elon Musk\u0026rdquo; —[CEO of]→ \u0026ldquo;Tesla\u0026rdquo;\nBut how does the system know that \u0026ldquo;CEO of\u0026rdquo; is a valid relationship? Or that \u0026ldquo;Elon Musk\u0026rdquo; is a Person and \u0026ldquo;Tesla\u0026rdquo; is a Company?\n👉 That\u0026rsquo;s where the ontology comes in.\nWithout an ontology, a knowledge graph is just a spaghetti mess of nodes and edges. The ontology gives it structure, semantics, and logic.\n📦 Example: Simple Ontology for a Business Graph Here\u0026rsquo;s a micro-ontology in plain English:\nClasses: Person, Company, Product Properties: worksFor(Person → Company) foundedBy(Company → Person) makes(Company → Product) Rules: A Person can work for only one company. A Company must have at least one Product. Now, when you build your graph, this ontology acts as a guardrail. If someone tries to say a Product works for a Person, the system throws a semantic red flag 🚩\n🧰 Common Ontology Languages \u0026amp; Tools If you\u0026rsquo;re building real-world ontologies, you\u0026rsquo;ll likely run into these tools and standards:\nOWL (Web Ontology Language) RDFS (RDF Schema) Protégé SHACL SPARQL These standards let you define your ontology and query your knowledge graph in ways that are both machine-readable and semantically rich.\n🧭 Why Ontologies Matter Here\u0026rsquo;s why you should care about them if you\u0026rsquo;re working in AI or data science:\nSemantic Search: Understand user queries beyond keywords — e.g. knowing that \u0026ldquo;Barack Obama\u0026rsquo;s wife\u0026rdquo; implies spouseOf. Data Integration: Merge messy, heterogeneous data using a shared structure. Explainability: Ontologies help machines reason about data — e.g., infer that someone is a leader if they are a CEO of a Company. Interoperability: Use a global standard (like schema.org) to make your data web-friendly and machine-readable. 🧪 In My Own Projects I\u0026rsquo;ve used ontologies in:\nA healthcare project, where patient symptoms, diagnoses, and treatments were modeled using the SNOMED CT ontology. A personal finance KG, where Income, Expense, and Account were tightly defined — enabling automated categorization and reasoning. Integrating RAG pipelines with structured knowledge graphs to improve retrieval precision using typed entity constraints. It\u0026rsquo;s honestly been a game-changer for building explainable AI systems.\n🧩 Final Thoughts Ontologies are the brain behind a knowledge graph\u0026rsquo;s structure. They bring order to the chaos of data and let machines \u0026ldquo;understand\u0026rdquo; concepts and their context. If you\u0026rsquo;re venturing into semantic search, personalized recommendations, RAG systems, or even smart assistants — investing time in ontology design is absolutely worth it.\nFeel free to ping me if you\u0026rsquo;re designing your first ontology or need help wrangling one into your GenAI pipeline. Happy graphing! 🔍🧠\nMore posts on knowledge graphs, vector search, and GenAI systems coming soon.\n— Akshat\n","permalink":"https://akshat4112.github.io/posts/ontology-in-knowledge-graphs/","summary":"If you\u0026rsquo;re working with knowledge graphs, one term that keeps popping up is ontology. Sounds academic, right? Like something you\u0026rsquo;d find buried in a philosophy textbook.\nBut in the world of AI, data science, and search engines, an ontology is far from abstract — it\u0026rsquo;s the blueprint that gives your knowledge graph meaning. Let\u0026rsquo;s break it down and explore how it all fits together.\n🧠 What Is an Ontology (in AI)?","title":"What is an Ontology in a Knowledge Graph?"},{"content":"As powerful as large language models (LLMs) like GPT, LLaMA, and Mistral are, they’re still general-purpose. If you want to make them truly useful for your domain—whether it’s legal documents, financial analysis, or German tax law—you need to fine-tune them.\nAnd thanks to a technique called LoRA (Low-Rank Adaptation), you can now fine-tune LLMs with a fraction of the data, compute, and cost.\n🔧 What is Fine-Tuning? Fine-tuning is the process of continuing the training of a pre-trained LLM on your own dataset so that it learns domain-specific patterns, vocabulary, tone, or tasks.\nFor example:\nWant an LLM that answers only insurance questions? → Fine-tune it on your policy docs and claims. Need a medical assistant? → Fine-tune it on clinical notes and patient Q\u0026amp;A. Want it to follow instructions better? → Fine-tune on curated instruction-response pairs. Fine-tuning adjusts the internal weights of the model, helping it generalize better to your specific use case.\n🤯 The Challenge The problem? Full fine-tuning of LLMs is expensive.\nA 7B parameter model might need hundreds of GBs of VRAM. You’ll need thousands of samples and multiple epochs. It’s easy to overfit, and hard to iterate fast. Enter LoRA.\n💡 What is LoRA? LoRA, short for Low-Rank Adaptation of Large Language Models, is a technique introduced by Microsoft Research (paper here) that makes fine-tuning cheaper and modular.\nInstead of updating all the parameters of the model, LoRA:\nFreezes the original weights of the model Adds trainable rank-decomposed matrices (adapters) to specific layers (usually attention projections) Trains only these lightweight matrices (~0.1% of the original model size) This drastically reduces GPU memory and training time.\n⚙️ How LoRA Works (Simplified) Mathematically, instead of updating weight matrix W, LoRA adds two low-rank matrices A and B such that:\nW\u0026rsquo; = W + A * B\nWhere:\nW = original frozen weight A, B = small trainable matrices (e.g. rank 4 or 8) During inference, the adapted weights are used as if they were part of the model.\n🧪 Benefits of LoRA 💸 Low cost: Train on consumer GPUs or Colab ⚡ Fast: Fewer trainable params = quicker epochs 🔁 Composable: Mix and match adapters (e.g., domain A + domain B) 🎯 Targeted: Focus adaptation on just a few layers Perfect for startups, researchers, and builders who want domain-specific LLMs without full-scale infra.\n🛠️ When to Use Fine-Tuning or LoRA Use Case Fine-Tuning Type Model refuses valid queries Full fine-tune / LoRA Needs to match company tone LoRA Custom document Q\u0026amp;A RAG or LoRA Domain-specific language or symbols Fine-tuning Instruction-following improvements LoRA or full fine-tune For general Q\u0026amp;A or document tasks, combine LoRA with a RAG pipeline to get best results.\n🧰 Popular Libraries for LoRA PEFT – Hugging Face’s library for Parameter-Efficient Fine-Tuning QLoRA – Quantized LoRA (8-bit/4-bit) for even more memory savings Axolotl – Powerful config-based trainer LLaMA-Factory – Quick setup for finetuning LLaMA and Mistral models 🧪 Example: Fine-Tuning Mistral with LoRA Prepare dataset (e.g. Alpaca or your own instruction set) Choose base model (e.g. mistralai/Mistral-7B-Instruct-v0.2) Use peft.LoraConfig to configure adapter Train with transformers.Trainer or SFTTrainer Save and deploy LoRA adapter with model You now have your own lightweight LLM variant!\n🧠 Final Thoughts Fine-tuning LLMs is no longer reserved for big labs and billion-parameter budgets. With LoRA, anyone can personalize a model for their task, brand, or niche.\nWant your own German-speaking travel planner? Or a legal assistant that understands Indian property law? LoRA gets you there—fast, cheap, and modular.\nAnd best of all: you keep the base model untouched and can reuse adapters across projects.\n— Akshat\n","permalink":"https://akshat4112.github.io/posts/llm-fine-tuning-lora/","summary":"As powerful as large language models (LLMs) like GPT, LLaMA, and Mistral are, they’re still general-purpose. If you want to make them truly useful for your domain—whether it’s legal documents, financial analysis, or German tax law—you need to fine-tune them.\nAnd thanks to a technique called LoRA (Low-Rank Adaptation), you can now fine-tune LLMs with a fraction of the data, compute, and cost.\n🔧 What is Fine-Tuning? Fine-tuning is the process of continuing the training of a pre-trained LLM on your own dataset so that it learns domain-specific patterns, vocabulary, tone, or tasks.","title":"LLM Fine-Tuning and LoRA: Making Large Models Work for You"},{"content":"We hear the term knowledge graph everywhere now — from Google Search to enterprise AI to GenAI apps. But what exactly is a knowledge graph, and why is everyone suddenly obsessed with it?\nIn this post, I\u0026rsquo;ll break down knowledge graphs in plain language: what they are, how they work, and how I use them in my own projects.\n🧱 The Basics: What Is a Knowledge Graph? At its core, a knowledge graph is a network of real-world entities (people, places, things) and the relationships between them. It\u0026rsquo;s how machines can represent, understand, and reason about the world — kind of like a human brain, but for structured data.\nIn technical terms:\nA knowledge graph is a graph-based data structure that encodes entities as nodes and relationships as edges, enriched with semantics via an ontology.\nHere\u0026rsquo;s a simple example:\n\u0026ldquo;Elon Musk\u0026rdquo; —[CEO of]→ \u0026ldquo;Tesla\u0026rdquo; \u0026ldquo;Tesla\u0026rdquo; —[makes]→ \u0026ldquo;Cybertruck\u0026rdquo; \u0026ldquo;Cybertruck\u0026rdquo; —[type]→ \u0026ldquo;Electric Vehicle\u0026rdquo;\nEvery node is an entity. Every edge is a predicate (relationship). And the whole structure is queryable, explorable, and often inferable.\n🕸️ Why Use a Graph? Traditional databases work with rows and tables. But in real life, information is messy and connected.\nA person can work for multiple companies. A product can belong to many categories. Concepts can be linked across domains. Graphs handle this interconnectedness naturally. That\u0026rsquo;s why big tech uses them:\nGoogle\u0026rsquo;s Knowledge Graph for better search answers. Facebook\u0026rsquo;s social graph to model user relationships. Amazon\u0026rsquo;s product graph for recommendations. 💡 How Is It Different from a Database? Feature Relational DB Knowledge Graph Data model Tables Nodes \u0026amp; edges Relationships Joins (explicit) First-class citizens (edges) Schema Rigid Flexible (ontology-driven) Query language SQL SPARQL / Cypher Semantics Implicit Explicit \u0026amp; machine-readable Graphs are schema-light and flexible, which makes them perfect for AI, NLP, and dynamic domains.\n🧠 Where Are Knowledge Graphs Used? Here\u0026rsquo;s where knowledge graphs really shine:\n🔍 Semantic Search\nEnables intent-based results (e.g., \u0026ldquo;founder of Tesla\u0026rdquo; → Elon Musk)\n🧠 LLM Context Injection (RAG)\nUse a knowledge graph to retrieve precise facts and inject them into prompts — improving GenAI accuracy.\n🏥 Healthcare \u0026amp; Life Sciences\nModel relationships between diseases, symptoms, genes, drugs.\n💼 Enterprise Intelligence\nUnify data silos across CRM, HR, finance, support.\n🔗 Data Integration \u0026amp; Interoperability\nLink structured + unstructured data through common semantics.\n⚙️ Key Components of a Knowledge Graph Here\u0026rsquo;s what goes into a real-world knowledge graph:\nEntities: The nodes — people, places, products, concepts. Relationships: The edges — how entities are connected. Ontology: Defines classes, properties, and constraints. (Read my ontology post for details!) Identifiers: Unique URIs to refer to each concept. Query Layer: Languages like SPARQL or Cypher to retrieve insights. 🧪 My Use Cases I\u0026rsquo;ve worked with knowledge graphs in several domains:\nInsurance Claims Automation: Extract structured facts from documents using OpenAI + Neo4j to speed up FNOL (First Notice of Loss). RAG Pipelines: Create mini knowledge graphs from PDFs and inject triples into prompts for better LLM accuracy. German Tax Assistant: Model deductions, expenses, and income types as nodes to generate explainable tax advice. Whether it\u0026rsquo;s documents, chatbots, or graphs powering LLMs — KGs make AI smarter and explainable.\n🧰 Popular Tools for Building KGs Neo4j: Popular graph database (uses Cypher). RDF \u0026amp; SPARQL: W3C standards for linked data. Stardog: Enterprise-grade knowledge graph platform. GraphDB: Great for RDF-based graphs. LangChain: Integrates LLMs with KG-based retrievers. 🔍 Final Thoughts If you\u0026rsquo;re working with LLMs, messy data, or just want your system to understand things better, knowledge graphs are a superpower. They\u0026rsquo;re the connective tissue between raw data and semantic meaning.\nIn a world where AI often hallucinates, knowledge graphs ground your models in truth, logic, and explainability.\nGot a use case you\u0026rsquo;re working on? Feel free to reach out — happy to jam on graph ideas!\n— Akshat\n","permalink":"https://akshat4112.github.io/posts/what-are-knowledge-graphs/","summary":"We hear the term knowledge graph everywhere now — from Google Search to enterprise AI to GenAI apps. But what exactly is a knowledge graph, and why is everyone suddenly obsessed with it?\nIn this post, I\u0026rsquo;ll break down knowledge graphs in plain language: what they are, how they work, and how I use them in my own projects.\n🧱 The Basics: What Is a Knowledge Graph? At its core, a knowledge graph is a network of real-world entities (people, places, things) and the relationships between them.","title":"What Are Knowledge Graphs?"},{"content":"We\u0026rsquo;ve all played with ChatGPT, Copilot, or Claude — typing in questions and marveling at their responses. But behind the scenes, there\u0026rsquo;s a powerful craft at play: prompt engineering.\nIt\u0026rsquo;s not just about \u0026ldquo;asking a question.\u0026rdquo; It\u0026rsquo;s about how you phrase it, structure it, and guide the model. Prompt engineering is the new programming skill — and it\u0026rsquo;s transforming how we interact with AI.\n🧠 What Is Prompt Engineering? Prompt engineering is the process of designing effective input prompts that guide large language models (LLMs) like GPT-4 to produce accurate, helpful, or creative outputs.\nIt\u0026rsquo;s half science, half art. A good prompt can mean the difference between:\n\u0026ldquo;Write a summary of this text\u0026rdquo;\nvs.\n\u0026ldquo;Summarize the following policy document in three bullet points, focusing on eligibility criteria and deadlines.\u0026rdquo;\nThe second one gives more context, structure, and constraints — and typically leads to much better results.\n⚙️ Why Prompt Engineering Matters LLMs are general-purpose models. They\u0026rsquo;re trained on everything from Shakespeare to StackOverflow. But they rely heavily on your prompt to figure out what you want.\nHere\u0026rsquo;s why good prompts matter:\n🎯 Precision: Reduce hallucinations and get to the point. 🧩 Context control: Inject the right background info. 🧠 Reasoning: Get step-by-step logic, not just surface-level answers. 📦 Structured output: Useful for coding, data extraction, APIs. Especially in RAG (Retrieval-Augmented Generation) or enterprise systems, a well-crafted prompt is mission-critical.\n✍️ Common Prompt Patterns Here are a few templates I\u0026rsquo;ve used successfully:\n1. Role-based prompting You are a tax advisor. Explain the deductions available to a German student who earned €2,000 in 2023. Tells the model who to \u0026ldquo;act\u0026rdquo; like. Great for voice, style, and expertise.\n2. Few-shot prompting Q: What\u0026#39;s the capital of France? A: Paris Q: What\u0026#39;s the capital of Italy? A: Providing examples before your actual question increases accuracy, especially for classification or logic tasks.\n3. Chain-of-thought prompting Solve the math problem step by step: 12 + (4 × 2) - 3 = 4. Instruction-tuned prompts Summarize the following PDF in JSON format with keys: title, summary, and top_3_keywords. 💥 Tips to Improve Your Prompts Here are some battle-tested ideas I\u0026rsquo;ve used in production:\n✅ Be specific: Vagueness = unpredictable output ✅ Define format: JSON, markdown, table, list? Ask for it explicitly ✅ Use delimiters: Use \u0026quot;\u0026quot;\u0026quot; or \u0026lt;data\u0026gt; to isolate input context ✅ Give examples: One example can double your accuracy ✅ Iterate: Prompt engineering is experimental — test and tweak. 🧪 🧠 My Use Cases In my work with document intelligence and GenAI platforms, prompt engineering is everywhere:\nPDF to JSON Extraction: Prompts that extract structured data from invoices, policies, etc. RAG Pipelines: Combine vector similarity + prompt tuning for better fact-grounding. Multi-agent Systems: Prompts define how different agents (planner, retriever, answerer) talk to each other. LTX Studio Scripts: Use role-play prompts + styles for travel vlog voiceovers and scene scripting. Each use case demands its own set of refined prompts — no one-size-fits-all.\n🛠️ Tools \u0026amp; Frameworks Here are some tools to experiment with prompt engineering:\nOpenAI Playground PromptLayer LangChain Promptable LlamaIndex 🚫 Common Prompt Pitfalls Avoid these if you want consistent output:\n❌ Ambiguity: \u0026ldquo;Summarize\u0026rdquo; vs. \u0026ldquo;Summarize for 12-year-olds\u0026rdquo; ❌ Overloading: Too much context = token overflow ❌ No instructions: The model isn\u0026rsquo;t psychic. Tell it what you want. 🔍 Final Thoughts Prompt engineering is the new interface between humans and machines.\nIt\u0026rsquo;s how we teach, guide, and collaborate with AI.\nAnd like any skill, it improves with practice.\nWhether you\u0026rsquo;re building a chatbot, a legal assistant, or a full-blown AI product —\nprompts are your steering wheel.\nLearn to drive well, and the LLM will take you far.\nLet me know if you\u0026rsquo;re working on something that could use a prompt tune-up — happy to help!\n— Akshat\n","permalink":"https://akshat4112.github.io/posts/prompt-engineering/","summary":"We\u0026rsquo;ve all played with ChatGPT, Copilot, or Claude — typing in questions and marveling at their responses. But behind the scenes, there\u0026rsquo;s a powerful craft at play: prompt engineering.\nIt\u0026rsquo;s not just about \u0026ldquo;asking a question.\u0026rdquo; It\u0026rsquo;s about how you phrase it, structure it, and guide the model. Prompt engineering is the new programming skill — and it\u0026rsquo;s transforming how we interact with AI.\n🧠 What Is Prompt Engineering? Prompt engineering is the process of designing effective input prompts that guide large language models (LLMs) like GPT-4 to produce accurate, helpful, or creative outputs.","title":"Prompt Engineering: The Art of Talking to AI"},{"content":"As large language models (LLMs) become central to search, productivity tools, education, and coding, evaluating them is no longer optional. You have to ask:\nIs this model reliable? Accurate? Safe? Biased? Smart enough for my task?\nBut here\u0026rsquo;s the catch: LLMs are not deterministic functions. They generate free-form text, can be right in one sentence and wrong in the next — and vary wildly depending on the prompt.\nSo how do we evaluate them meaningfully?\n🧪 Why Evaluate LLMs? Good evaluation helps answer:\n✅ Is the model aligned with user goals? ✅ Does it generalize to unseen prompts? ✅ Is it factual, helpful, and harmless? ✅ Is it better than baseline or competitor models? Whether you’re fine-tuning a model, comparing open-source LLMs, or releasing an AI feature — you need a systematic way to measure quality.\n🎯 Types of Evaluation There are three main types of evaluation used for LLMs:\n1. Intrinsic Evaluation (automatic) These are computed automatically without human judgment.\nPerplexity: Measures how well a model predicts the next word (lower = better).\nNot ideal for generation tasks, but useful during pretraining.\nBLEU / ROUGE / METEOR: Compare generated output to a reference.\nBest for short-form tasks like translation or summarization.\nBLEU paper\nExact Match / F1 Score: Used in QA tasks with ground truth answers.\nBERTScore: Embedding-based similarity using BERT. Good for semantics.\n🚫 Problem: These scores often fail to capture nuance, creativity, or reasoning.\n2. Extrinsic Evaluation (human-like) This focuses on how LLMs perform in downstream tasks.\nTask success: Did the model complete the task (e.g., booking a flight, answering a tax question)? User satisfaction: Useful in production systems or chatbots. A/B testing: Compare model variants in live usage. Win-rate comparisons: Common in model leaderboards. These are more reflective of real-world performance.\n3. Human Evaluation Still the gold standard for nuanced tasks.\nHuman judges evaluate:\n🌟 Relevance 🌟 Factuality 🌟 Fluency 🌟 Helpfulness 🌟 Harmlessness (toxicity, bias) Usually done via Likert scale or pairwise comparison. Costly, but high-quality.\n🧑‍⚖️ Benchmarks for LLMs Some standard benchmarks have emerged:\nMMLU (Massive Multitask Language Understanding)\nCovers math, medicine, law, history — tests reasoning over 57 domains.\nHELLASWAG\nCommonsense inference for fill-in-the-blank scenarios.\nTruthfulQA\nMeasures how often LLMs give truthful answers to tricky questions.\nBIG-bench\nCollaborative benchmark of 200+ tasks testing model generalization.\nMT-Bench\nMulti-turn chat evaluation developed by LMSys for Vicuna and Chatbot Arena.\nBonus: Chatbot Arena does live crowd-sourced pairwise model evaluation.\n📏 Common Metrics Metric Use Case Notes Perplexity Pretraining Lower = better BLEU/ROUGE Translation/Summarization Needs reference outputs BERTScore Semantics Works better with long-form tasks Win Rate Pairwise eval Human judges or ranked voting F1 / EM QA tasks Binary metrics, hard to scale GPT-4 Eval Self-evaluation Biased but surprisingly useful 🔧 Tools for Evaluation OpenAI Evals – framework for building evals for GPT lm-eval-harness – benchmark open-source LLMs TruLens – feedback + eval framework for LLM apps Promptfoo – A/B prompt testing tool LangSmith – Track, debug, and eval LangChain apps 💬 My Approach to LLM Evaluation In my own projects (like document Q\u0026amp;A or multi-agent GenAI), I often mix:\n🔍 Hard metrics (accuracy, F1) for structured data extraction 🧪 Prompt-based unit tests using OpenAI Evals or LangChain 👨‍👩‍👧 Manual grading for edge cases and critical flows 📊 Leaderboards when comparing LLaMA, Mixtral, GPT-4, Claude, etc. For production? Human-in-the-loop testing is key — especially for regulated or high-risk domains.\n🧠 Final Thoughts Evaluating LLMs isn’t just a technical problem — it’s a design problem, a UX problem, and a trust problem.\nAs the space matures, we’ll need better automated metrics, transparent benchmarks, and community-driven evaluations.\nUntil then: evaluate early, evaluate often — and don’t trust your LLM until you’ve tested it.\n— Akshat\n","permalink":"https://akshat4112.github.io/posts/evaluating-llms/","summary":"As large language models (LLMs) become central to search, productivity tools, education, and coding, evaluating them is no longer optional. You have to ask:\nIs this model reliable? Accurate? Safe? Biased? Smart enough for my task?\nBut here\u0026rsquo;s the catch: LLMs are not deterministic functions. They generate free-form text, can be right in one sentence and wrong in the next — and vary wildly depending on the prompt.\nSo how do we evaluate them meaningfully?","title":"Evaluating LLMs: How Do You Measure a Model's Mind?"},{"content":"Large Language Models (LLMs) like GPT or LLaMA are great at generating text. But there\u0026rsquo;s a catch:\nThey only know what they were trained on, and that knowledge is frozen at training time.\nSo what happens when you ask them something from after their training cutoff? Or something super niche, like a policy from your internal HR docs?\nEnter RAG – Retrieval-Augmented Generation.\nA technique that combines LLMs with a search engine, enabling them to look up facts on the fly.\n🧠 What is RAG? RAG (Retrieval-Augmented Generation) is a framework that augments the input to an LLM with retrieved documents or chunks from an external knowledge source.\nInstead of relying solely on the model’s internal weights, RAG pulls in real, current, or domain-specific content to ground its responses.\nThink of it like this:\n\u0026ldquo;Before answering, the model Googles the topic — and then responds.\u0026rdquo;\n🛠️ How Does RAG Work? The RAG architecture typically has three stages:\nQuery Understanding\nUser asks a question (e.g., \u0026ldquo;What are the leave policies at Acme Corp?\u0026rdquo;) Retrieval\nThe system converts the query into an embedding (via a model like all-MiniLM). Searches a vector database (like FAISS, Weaviate, Qdrant) for top relevant chunks. Generation\nThe LLM gets the original query + retrieved context. It generates a grounded, coherent answer. This creates a dynamic pipeline where the LLM can “look up” facts in real time.\n🔍 Why RAG is Important Without RAG With RAG ❌ Hallucinations ✅ Grounded answers ❌ Outdated knowledge ✅ Real-time / up-to-date info ❌ Model retraining needed for updates ✅ Just update documents ❌ Doesn’t know your internal data ✅ Custom knowledge injected dynamically It’s the backbone of many enterprise AI apps, chat-with-your-PDF, code assistants, and AI copilots.\n🧰 Tools and Frameworks for RAG LangChain – End-to-end pipelines with retrieval and LLM chaining (docs) Haystack – Search-native RAG framework from deepset LlamaIndex – Lightweight RAG with document loaders and query engines Pinecone / Weaviate / Qdrant – Vector DBs to store and retrieve embeddings FAISS – Facebook AI similarity search, blazing fast and open source Bonus: Use sentence-transformers to embed documents.\n📄 RAG for Custom Documents RAG is ideal for Q\u0026amp;A over:\n📝 Internal policies 📚 Academic PDFs 🧾 Tax or legal docs 💻 Codebases 🏢 HR manuals You chunk the docs (e.g., into 500-word segments), embed them, and index in a vector DB. When the user asks a question, the system retrieves relevant chunks and passes them to the LLM for answering.\n🧠 When to Use RAG vs Fine-Tuning Situation Technique Need accurate info from private docs ✅ RAG Need tone/style/domain adaptation 🔁 LoRA or finetune Need dynamic updates (e.g., news) ✅ RAG Have small structured data 🔄 Toolformer / APIs Want to reduce hallucinations ✅ RAG + prompt tuning Often, combining RAG + LoRA fine-tuning gives the best of both worlds.\n⚠️ RAG Challenges Chunking strategy matters a lot (sentence, paragraph, or overlap-based?) Embedding quality impacts retrieval quality Long documents can lead to token limits → need summarization or re-ranking LLM may still hallucinate within retrieved context (e.g., wrong interpretation) Tip: Always show the source in the final answer to improve trust.\n🔄 Variants of RAG Hybrid RAG: Combines semantic + keyword search Multi-hop RAG: Chain multiple retrieval steps for complex reasoning Self-RAG: LLM rewrites query before retrieval to improve results (Meta AI, 2023) Agentic RAG: Agents explore document tree and reason before answering 🧠 Final Thoughts RAG is one of the most practical, scalable, and production-ready ways to supercharge LLMs with real-world knowledge.\nInstead of hoping your LLM \u0026ldquo;remembers\u0026rdquo; something from training, just tell it what it needs to know.\nThe future of LLM applications isn’t just smarter models, it’s smarter context. And RAG is the backbone of that.\n— Akshat\n","permalink":"https://akshat4112.github.io/posts/rag-and-llms/","summary":"Large Language Models (LLMs) like GPT or LLaMA are great at generating text. But there\u0026rsquo;s a catch:\nThey only know what they were trained on, and that knowledge is frozen at training time.\nSo what happens when you ask them something from after their training cutoff? Or something super niche, like a policy from your internal HR docs?\nEnter RAG – Retrieval-Augmented Generation.\nA technique that combines LLMs with a search engine, enabling them to look up facts on the fly.","title":"RAG and LLMs: Teaching Large Models to Use External Knowledge"},{"content":"When people say \u0026ldquo;Transformers revolutionized NLP,\u0026rdquo; what they really mean is:\nAttention revolutionized NLP.\nFrom GPT and BERT to LLaMA and Claude, attention mechanisms are the beating heart of modern large language models.\nBut what exactly is attention? Why is it so powerful? And how many types are there?\nLet\u0026rsquo;s dive in.\n🧠 What is Attention? In the simplest sense, attention is a way for a model to focus on the most relevant parts of the input when generating output.\nIt answers:\n\u0026ldquo;Given this word, which other words should I pay attention to — and how much?\u0026rdquo;\n🔢 The Scaled Dot-Product Attention Let\u0026rsquo;s break it down mathematically.\nGiven:\nQuery matrix $Q \\in \\mathbb{R}^{n \\times d_k}$ Key matrix $K \\in \\mathbb{R}^{n \\times d_k}$ Value matrix $V \\in \\mathbb{R}^{n \\times d_v}$ The attention output is:\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V$$\n$QK^\\top$: Dot product measures similarity $\\sqrt{d_k}$: Scaling factor to avoid large softmax values softmax: Turns similarity into attention weights $V$: Weighted sum of value vectors 📖 Citation: Vaswani et al., 2017 (Attention is All You Need)\n🔁 Multi-Head Attention Instead of applying one attention function, we apply it multiple times in parallel with different projections.\n$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \u0026hellip;, \\text{head}_h)W^O$$ $$\\text{where head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$\nEach head learns different types of relationships (e.g., syntactic, semantic).\n🧩 Types of Attention in Transformers Let\u0026rsquo;s look at the key attention variations used in different transformer architectures.\n1. Self-Attention Query, Key, and Value come from the same input. Used in encoder and decoder blocks. Each token attends to every other token (or just previous ones in causal attention). $$\\text{SelfAttention}(X) = \\text{Attention}(X, X, X)$$\n2. Cross-Attention Used in encoder-decoder models like T5 or BART. Query comes from decoder, Key \u0026amp; Value come from encoder output. $$\\text{CrossAttention}(Q_{\\text{decoder}}, K_{\\text{encoder}}, V_{\\text{encoder}})$$\n3. Masked (Causal) Attention Used in autoregressive models like GPT. Prevents tokens from attending to future tokens. Enforced using a triangular mask. $$\\text{MaskedAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}} + M\\right)V$$ where $M$ is a mask with $-\\infty$ in upper triangle.\n4. Local Attention Each token only attends to a local window (e.g., ±128 tokens). Reduces compute from $O(n^2)$ to $O(nw)$ Used in models like Longformer (Beltagy et al., 2020)\n5. Sparse Attention Instead of full attention, use pre-defined patterns (e.g., strided, global). Reduces memory usage. Examples:\nBigBird (Zaheer et al., 2020) Reformer (Kitaev et al., 2020) 6. Linear / Kernelized Attention Approximates attention with linear complexity. Replace softmax with kernel function: $$\\text{Attention}(Q, K, V) = \\phi(Q)(\\phi(K)^\\top V)$$ Used in Performer (Choromanski et al., 2020)\n7. Memory-Augmented Attention Adds external memory vectors (e.g., key-value cache, documents). Popular in RAG and MoE systems. 🏗️ Attention Block in Transformers Each Transformer layer consists of:\nMulti-head Attention Add \u0026amp; Layer Norm Feed Forward Network Add \u0026amp; Layer Norm Input → [Multi-head Attention] → Add \u0026amp; Norm → [FFN] → Add \u0026amp; Norm → Output Transformers stack these layers 12–96 times depending on size (BERT-base vs GPT-4 scale).\n🧪 Why Attention Works Position-invariant: Doesn\u0026rsquo;t care where a word is, just what it\u0026rsquo;s related to Parallelizable: Unlike RNNs Interpretable: You can visualize what the model is \u0026ldquo;looking at\u0026rdquo; 🧠 Final Thoughts Attention isn\u0026rsquo;t just a component — it is the innovation that powers modern LLMs.\nFrom GPT\u0026rsquo;s self-attention to BERT\u0026rsquo;s bidirectional masking, every major NLP breakthrough builds on this core idea:\n\u0026ldquo;Pay attention to what matters — and learn how to pay attention.\u0026rdquo;\nIn upcoming posts, I\u0026rsquo;ll dive into positional encodings, attention visualization, and how LoRA modifies attention layers.\n— Akshat\n","permalink":"https://akshat4112.github.io/posts/attention-in-transformers/","summary":"When people say \u0026ldquo;Transformers revolutionized NLP,\u0026rdquo; what they really mean is:\nAttention revolutionized NLP.\nFrom GPT and BERT to LLaMA and Claude, attention mechanisms are the beating heart of modern large language models.\nBut what exactly is attention? Why is it so powerful? And how many types are there?\nLet\u0026rsquo;s dive in.\n🧠 What is Attention? In the simplest sense, attention is a way for a model to focus on the most relevant parts of the input when generating output.","title":"Understanding Attention in Transformers: The Core of Modern NLP"},{"content":"In the world of machine learning, especially with the rise of large language models (LLMs) and deep neural networks, model extraction attacks are a growing concern. These attacks aim to replicate the behavior of a machine learning model by querying it and then using the responses to reverse-engineer its underlying architecture and parameters.\nWhat is a Model Extraction Attack? A model extraction attack occurs when an adversary tries to replicate a machine learning model by making repeated queries to it and analyzing its responses. The goal of the attacker is to create a new model that mimics the target model’s functionality, often without direct access to its architecture or parameters.\nOnce the adversary has successfully replicated a model, they can use it for various malicious purposes, including:\nStealing intellectual property: By extracting a proprietary model, attackers can use it to bypass legal or technical protections or even re-sell it. Bypassing security measures: An extracted model might reveal vulnerabilities or ways to exploit the system, allowing attackers to bypass security checks. Creating competitive advantages: Competitors can replicate expensive and sophisticated models without having to invest in training them from scratch. Types of Model Extraction Attacks There are two primary categories of model extraction attacks:\n1. Black-box Attacks In a black-box attack, the adversary only has access to the model’s input-output behavior, without any information about its internals. The attacker can make queries to the model and receive outputs, which they will use to infer the model’s behavior and architecture.\nExample: Suppose an attacker wants to replicate a language model like GPT. They can send in a variety of text prompts to the model, such as \u0026ldquo;What’s the weather like today?\u0026rdquo; or \u0026ldquo;Tell me a joke,\u0026rdquo; and observe the responses. After making enough queries, the attacker can train their own model on these input-output pairs, essentially trying to reproduce the target model\u0026rsquo;s performance.\nTechniques used in Black-box Attacks:\nQuerying the model extensively: This is usually the most straightforward approach. The attacker queries the model with diverse inputs to gather enough data to approximate the model’s behavior. Model Distillation: The adversary can train a smaller surrogate model using the same input-output pairs. Although the extracted model will not match the target exactly, it can still replicate much of the functionality. Real-World Example: Researchers from the University of California, Berkeley demonstrated a black-box attack on image classification models. They used a model distillation technique where they queried a black-box image classifier (like Google Vision API) with thousands of images, extracting a surrogate model that performed similarly to the original model on classification tasks.\n2. White-box Attacks In a white-box attack, the adversary has full access to the model’s architecture, weights, and sometimes even its training data. This gives them an advantage in replicating the model, as they can directly inspect its components and behaviors.\nExample: If an attacker gains access to a model\u0026rsquo;s source code or API endpoint (e.g., through a vulnerable cloud service), they can directly extract information about the model’s structure. This could include its layers, weights, and biases, making it much easier to create a replica.\nTechniques used in White-box Attacks:\nExploiting exposed models: If a company or service exposes their model without adequate protection (like an open-source model or poorly secured API), an attacker can directly replicate it. Model stealing via backdoors: Some attackers try to inject vulnerabilities into the model itself that would allow them to extract its parameters without permission. Real-World Example: In NIPS 2016, researchers successfully conducted a white-box model extraction attack on a neural network model by reverse-engineering the model architecture and retraining a copy of the model on their own data. This demonstrated the feasibility of stealing a model from an exposed API.\nSteps Involved in Model Extraction Attacks Step 1: Querying the Target Model The adversary typically starts by querying the target model. In the case of black-box attacks, they don’t know the internal structure of the model, so they send a variety of queries, often including edge cases and adversarial inputs, to collect a wide range of responses.\nExample Implementation:\nimport requests def query_model(input_text): response = requests.post(\u0026#39;https://example.com/predict\u0026#39;, data={\u0026#39;input\u0026#39;: input_text}) return response.json() queries = [ \u0026#34;What is 2+2?\u0026#34;, \u0026#34;Tell me a story about a dragon.\u0026#34;, \u0026#34;What is the capital of France?\u0026#34; ] responses = [query_model(query) for query in queries] In this case, the attacker collects responses from the target model and stores them for further analysis.\nStep 2: Analyzing the Responses After gathering enough input-output pairs, the attacker will analyze the responses. They may look for patterns or anomalies that help them understand the model’s decision-making process.\nFor example, the attacker might notice that the model tends to classify certain types of input in a specific way, suggesting a particular feature in the underlying architecture.\nStep 3: Rebuilding the Model In the final step, the attacker will attempt to train a new model using the gathered data. This process involves feeding the input-output pairs into a new model and adjusting its parameters until the model closely replicates the behavior of the target model.\nExample Implementation:\nfrom sklearn.neural_network import MLPClassifier # Example: Train a simple MLP classifier on the extracted data X_train = [[1, 2], [2, 3], [3, 4]] # Example inputs y_train = [0, 1, 1] # Example outputs (targets) model = MLPClassifier(hidden_layer_sizes=(10,)) model.fit(X_train, y_train) # Predict on new data model.predict([[4, 5]]) This new model trained on the attacker’s collected data will likely approximate the behavior of the original model.\nRisks and Implications The risks posed by model extraction attacks are vast:\nIntellectual Property Theft: Large companies invest a lot in developing machine learning models, and model extraction attacks make it easier for malicious actors to replicate their models, potentially leading to loss of competitive advantage. Security Vulnerabilities: Once an attacker has replicated a model, they could use it to exploit weaknesses or gain unauthorized access to sensitive data, especially if the model is used in mission-critical systems like finance or healthcare. Reduction in Trust: If model extraction attacks become more prevalent, users may lose trust in machine learning systems, fearing that adversaries could easily replicate and misuse models. Mitigating Model Extraction Attacks 1. Limit Query Access Limiting the number of queries an external party can make to a model is a simple yet effective measure. Implementing rate limiting, CAPTCHA, or query restrictions can prevent an adversary from gathering enough data to replicate the model.\nImplementation Example:\nimport time from functools import wraps def limit_queries(rate_limit): def decorator(func): last_called = [0.0] @wraps(func) def wrapped(*args, **kwargs): elapsed = time.time() - last_called[0] if elapsed \u0026lt; rate_limit: time.sleep(rate_limit - elapsed) last_called[0] = time.time() return func(*args, **kwargs) return wrapped return decorator @limit_queries(1) # Only allow one query per second def query_model(input_text): response = requests.post(\u0026#39;https://example.com/predict\u0026#39;, data={\u0026#39;input\u0026#39;: input_text}) return response.json() 2. Model Watermarking Watermarking involves embedding unique markers within a model’s behavior. If an attacker replicates the model, these markers can be used to track and prove ownership.\n3. Obfuscating Model Outputs To make it harder for attackers to learn from the model’s behavior, you can introduce noise into the model’s outputs, making them less predictable and more difficult to replicate.\n4. Differential Privacy Differential privacy techniques can be applied to the model to ensure that individual data points cannot be reverse-engineered from the model’s responses. This reduces the effectiveness of model extraction attacks, as the model will not reveal sensitive information about specific data points.\nFinal Thoughts Model extraction attacks are a growing threat in the world of AI and machine learning. As models become more powerful and are deployed at scale, it’s crucial to understand the risks and employ countermeasures to protect intellectual property and secure sensitive systems.\nBy implementing appropriate defenses, including query limitations, model watermarking, and differential privacy, we can reduce the likelihood of successful model extraction and ensure that the benefits of AI are not overshadowed by malicious exploitation.\nStay tuned for more posts where we dive deeper into defense techniques and specific case studies.\n— Akshat\n","permalink":"https://akshat4112.github.io/posts/model-extraction-attacks/","summary":"In the world of machine learning, especially with the rise of large language models (LLMs) and deep neural networks, model extraction attacks are a growing concern. These attacks aim to replicate the behavior of a machine learning model by querying it and then using the responses to reverse-engineer its underlying architecture and parameters.\nWhat is a Model Extraction Attack? A model extraction attack occurs when an adversary tries to replicate a machine learning model by making repeated queries to it and analyzing its responses.","title":"Model Extraction Attacks: How Hackers Steal AI Models"},{"content":"Speaker anonymization refers to the process of modifying the characteristics of a speaker\u0026rsquo;s voice so that the speaker\u0026rsquo;s identity cannot be easily determined while preserving the speech\u0026rsquo;s intelligibility. With the increasing usage of speech data in virtual assistants, surveillance systems, and other applications, ensuring privacy in speech data has become critical.\nIn this post, we\u0026rsquo;ll dive into the technical details of speaker anonymization techniques, including implementation approaches using machine learning, deep learning models, and popular libraries.\nWhat is Speaker Anonymization? Speaker anonymization modifies the speaker\u0026rsquo;s voice using various methods while keeping the message intact. The primary goal is to hide the speaker\u0026rsquo;s identity, either by altering the speaker\u0026rsquo;s voice or replacing it with a synthetic one, while retaining the intelligibility and naturalness of the speech.\nCommon Techniques for Speaker Anonymization: Voice Conversion (VC): Alters the speaker\u0026rsquo;s voice to sound like another person or a synthetic target. Voice Modulation: Modifies pitch, speed, and tone of the voice. Speech Synthesis: Converts the original voice\u0026rsquo;s content into synthetic speech. Differential Privacy: Introduces noise to the speech data, preventing re-identification. Key Challenges: Preserving Speech Quality: Ensuring that the transformed speech is still intelligible and natural. Balancing Privacy and Utility: Anonymizing the voice while maintaining the ability to use the speech for analysis. Why is Speaker Anonymization Important? Speaker anonymization is crucial in multiple domains for privacy protection, regulatory compliance, and ethical AI development. Below are some key reasons:\n1. Privacy Protection Anonymizing speaker voices helps prevent the identification of individuals in sensitive applications such as medical conversations or voice assistants.\n2. Regulatory Compliance With regulations like GDPR and CCPA, anonymizing speech data ensures compliance with privacy laws that mandate the protection of personal data.\n3. Ethical AI Research Anonymized voice data helps researchers work with sensitive data without compromising privacy.\nTechniques for Speaker Anonymization Now, let\u0026rsquo;s dive into specific technical implementations of popular anonymization techniques:\n1. Voice Conversion (VC) Voice conversion is one of the most widely used techniques in speaker anonymization. The objective is to convert a speaker’s voice to sound like another person (or a synthetic voice) while preserving the speech content. Voice conversion is achieved through two major steps:\nSteps in Voice Conversion: Feature Extraction: Extract speech features such as Mel-frequency cepstral coefficients (MFCCs) or spectral features. Mapping Features to Target Voice: Map the extracted features from the source voice to those of the target voice. This is typically done using a regression model or deep neural networks. Implementation using a deep neural network (DNN):\nimport torch import torch.nn as nn import torch.optim as optim # Example: Simple neural network for feature transformation class VoiceConversionNN(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(VoiceConversionNN, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_dim, output_dim) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Sample feature data (MFCCs, etc.) source_features = torch.randn(100, 13) # Example: 100 samples, 13 features per sample target_features = torch.randn(100, 13) # Initialize and train the model model = VoiceConversionNN(13, 64, 13) # Input: 13 features, Output: 13 features criterion = nn.MSELoss() optimizer = optim.Adam(model.parameters(), lr=0.001) # Training loop (simple) for epoch in range(1000): model.train() optimizer.zero_grad() output = model(source_features) loss = criterion(output, target_features) loss.backward() optimizer.step() print(\u0026#34;Voice conversion model trained.\u0026#34;) In this example, the neural network learns to map the features from the source voice to the target voice’s features. In practice, these networks are trained on large datasets of voice pairs to create high-quality voice conversion systems.\n2. Voice Modulation (Pitch, Speed, and Timbre Adjustment) Voice modulation involves adjusting the speech characteristics like pitch, speed, and timbre. This method is simpler than voice conversion and can be implemented using signal processing techniques.\nImplementation: Pitch Shifting and Speed Adjustment import librosa import soundfile as sf def shift_pitch(audio_file, n_steps): # Load the audio file y, sr = librosa.load(audio_file) # Pitch shift using librosa y_shifted = librosa.effects.pitch_shift(y, sr, n_steps) return y_shifted def change_speed(audio_file, rate): # Load the audio file y, sr = librosa.load(audio_file) # Change speed using librosa y_fast = librosa.effects.time_stretch(y, rate) return y_fast # Apply pitch shifting shifted_audio = shift_pitch(\u0026#39;input_audio.wav\u0026#39;, 5) # Apply speed change faster_audio = change_speed(\u0026#39;input_audio.wav\u0026#39;, 1.2) # Save the processed audio sf.write(\u0026#39;shifted_audio.wav\u0026#39;, shifted_audio, 16000) sf.write(\u0026#39;faster_audio.wav\u0026#39;, faster_audio, 16000) In this code, Librosa is used for pitch shifting and time stretching. These techniques can be used individually or combined to anonymize the speaker’s voice.\n3. Speech Synthesis Speech synthesis is the process of generating synthetic speech from text. This method replaces the original speaker\u0026rsquo;s voice with a generated one, often using text-to-speech (TTS) systems.\nOne popular library for speech synthesis is Google TTS or pyttsx3, which can generate a new, anonymized voice.\nImplementation using pyttsx3: import pyttsx3 def synthesize_speech(text, output_file): engine = pyttsx3.init() engine.save_to_file(text, output_file) engine.runAndWait() # Example usage synthesize_speech(\u0026#34;Hello, this is an anonymized voice.\u0026#34;, \u0026#34;anonymized_speech.wav\u0026#34;) Here, pyttsx3 generates synthetic speech using the text provided, anonymizing the original speaker’s voice entirely.\n4. Differential Privacy for Speech Data Differential privacy is a technique that ensures that individual data points (in this case, the speaker’s identity) cannot be re-identified. This is achieved by adding noise to the data in a way that prevents overfitting to specific features of the data.\nWhile differential privacy is mostly used in machine learning models for training purposes, it can also be applied to anonymize voice data by introducing noise into the voice features before training.\nExample using noise addition: import numpy as np def add_noise_to_features(features, noise_level=0.05): noise = np.random.normal(0, noise_level, features.shape) noisy_features = features + noise return noisy_features # Example feature matrix (e.g., MFCCs) features = np.random.rand(100, 13) # Add noise for differential privacy noisy_features = add_noise_to_features(features) By adding Gaussian noise to the features, we can reduce the likelihood of identifying the speaker from the transformed data.\nReal-World Applications of Speaker Anonymization 1. Voice Assistants Companies like Google and Amazon collect speech data to improve their voice assistants. Speaker anonymization allows these companies to analyze the data while ensuring user privacy.\n2. Medical Records Anonymized audio of doctor-patient conversations is crucial in healthcare for training models or for use in AI-based diagnostic tools while protecting patient confidentiality.\n3. Surveillance Systems In environments such as public spaces or workplaces, speaker anonymization is used to ensure that surveillance audio does not compromise individual identities.\nFinal Thoughts Speaker anonymization is a critical step in ensuring privacy and security in speech-based applications. With techniques ranging from voice conversion to speech synthesis, it\u0026rsquo;s possible to anonymize voices while maintaining intelligibility. Implementing these techniques effectively, especially using machine learning and deep learning models, can ensure compliance with privacy regulations while preserving the utility of speech data.\nAs AI models continue to evolve, innovations in speaker anonymization will play an essential role in ethical AI development and maintaining user trust.\nStay tuned for more deep dives into the technical aspects of AI and speech processing.\n— Akshat\n","permalink":"https://akshat4112.github.io/posts/speaker-anonymization/","summary":"Speaker anonymization refers to the process of modifying the characteristics of a speaker\u0026rsquo;s voice so that the speaker\u0026rsquo;s identity cannot be easily determined while preserving the speech\u0026rsquo;s intelligibility. With the increasing usage of speech data in virtual assistants, surveillance systems, and other applications, ensuring privacy in speech data has become critical.\nIn this post, we\u0026rsquo;ll dive into the technical details of speaker anonymization techniques, including implementation approaches using machine learning, deep learning models, and popular libraries.","title":"Speaker Anonymization: Protecting Voice Identity in the AI Era"},{"content":"Understanding LLM Agents: The Future of Autonomous AI Systems Large Language Models (LLMs), like GPT-3, GPT-4, and others, have taken the world by storm due to their impressive language generation and understanding capabilities. However, when these models are augmented with decision-making capabilities, memory, and actions in specific environments, they become even more powerful. Enter LLM Agents — autonomous systems built on top of large language models to perform tasks, make decisions, and act autonomously based on user instructions.\nIn this post, we\u0026rsquo;ll explore what LLM agents are, how they work, and how to create and implement them in real-world applications.\n🧠 What Are LLM Agents? An LLM Agent refers to an autonomous system built using a large language model, capable of interacting with environments, performing complex tasks, and making decisions based on user input. Unlike traditional models, which are limited to generating text based on inputs, LLM agents can take actions in a dynamic setting, execute workflows, and interact with external APIs, databases, and services.\nAn LLM Agent can:\nInterpret goals or instructions from users Plan a sequence of actions to achieve those goals Execute actions using tools or APIs Observe the results of those actions Reflect and adjust based on feedback Maintain long-term memory across interactions Key Features of LLM Agents: Autonomy: LLM agents can take action without continuous user intervention, executing tasks in an automated fashion. Adaptability: LLM agents can adapt to different environments and instructions, adjusting their actions based on input. Decision-Making: These agents can reason about tasks and make decisions, including breaking down large problems into smaller, manageable tasks. Integration: LLM agents can interface with external systems, like databases, APIs, and the web, to retrieve information and perform actions in real-time. Unlike a standard chat interface, which generates text only, agents can interface with external systems, manipulate data, browse the web, and sometimes control physical devices.\n🛠️ How Do LLM Agents Work? LLM agents function by leveraging the capabilities of large language models in conjunction with external tools, frameworks, or environments. A fully-featured LLM agent typically consists of several key components:\n1. Foundation Model (Language Model Core) The core of an agent is the language model itself. The foundation model provides the reasoning, planning, and text generation capabilities. Models like GPT-4, Claude, or Llama 2 serve as the \u0026ldquo;brain\u0026rdquo; of the agent system. Its natural language understanding capabilities allow the agent to understand instructions in human language.\n2. Tool Use Framework Agents need to be able to use tools to interact with the world. This involves:\nTool definition: Specifying what tools are available Tool selection: Choosing which tool to use when Tool invocation: Properly formatting calls to tools Output processing: Interpreting the results of tool use Tools might include:\nWeb Services: Making HTTP requests to external APIs Databases: Querying or updating data in relational or NoSQL databases External Libraries: Accessing additional libraries or software packages to perform specialized tasks Code executors, calculators, and more 3. Memory Systems Effective agents require different types of memory:\nWorking memory: What\u0026rsquo;s relevant in the current context Semantic memory: Knowledge of facts, concepts, and relationships Episodic memory: Record of past conversations and actions Procedural memory: How to perform certain tasks In more advanced LLM agents, memory plays a crucial role. These agents may store information about past interactions, user preferences, or even the state of a task. Memory can either be:\nShort-term: Storing temporary information during a session. Long-term: Storing knowledge persistently, allowing the agent to retain information across sessions and continuously improve over time. 4. Planning and Reasoning Modules Agents must plan their actions and reason about their environment:\nTask decomposition: Breaking complex goals into manageable steps Decision making: Choosing between alternative approaches Self-reflection: Evaluating the success of actions and adjusting accordingly Meta-cognition: Reasoning about the agent\u0026rsquo;s own thought processes 💻 Implementing LLM Agents: Frameworks and Approaches Several approaches and frameworks have emerged for building LLM agents:\nReAct: Reasoning and Acting The ReAct framework combines reasoning (thinking through a problem) with acting (taking steps toward a solution). It\u0026rsquo;s based on the observation that LLMs can generate both reasoning traces and action plans.\ndef react_agent(query, tools, max_steps=5): context = f\u0026#34;User query: {query}\\n\\nAvailable tools: {tools_description(tools)}\u0026#34; for step in range(max_steps): # Think: Generate reasoning about the current state thought = llm.generate(f\u0026#34;{context}\\n\\nThought: Let me think about how to solve this...\u0026#34;) # Act: Decide on an action to take action = llm.generate(f\u0026#34;{context}\\n{thought}\\n\\nAction: \u0026#34;) # Observe: Execute the action and observe results if action.startswith(\u0026#34;FINISH\u0026#34;): return action.replace(\u0026#34;FINISH\u0026#34;, \u0026#34;\u0026#34;) tool_name, tool_input = parse_action(action) observation = execute_tool(tools, tool_name, tool_input) # Update context with the new information context += f\u0026#34;\\n{thought}\\n{action}\\n{observation}\u0026#34; # Final answer after max steps return llm.generate(f\u0026#34;{context}\\n\\nBased on the above, the final answer is:\u0026#34;) LangChain Agents LangChain provides a popular framework for building LLM agents with tool use capabilities:\nfrom langchain.agents import AgentType, initialize_agent from langchain.tools import Tool from langchain.llms import OpenAI # Define tools the agent can use tools = [ Tool( name=\u0026#34;Search\u0026#34;, func=search_function, description=\u0026#34;useful for when you need to search the internet\u0026#34; ), Tool( name=\u0026#34;Calculator\u0026#34;, func=calculator_function, description=\u0026#34;useful for when you need to perform calculations\u0026#34; ), ] # Initialize the agent llm = OpenAI(temperature=0) agent = initialize_agent( tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True ) # Run the agent agent.run(\u0026#34;What is the square root of the current temperature in San Francisco?\u0026#34;) AutoGPT and BabyAGI For more autonomous agents, frameworks like AutoGPT and BabyAGI implement goal-driven systems that can create and manage their own subtasks:\nfrom autogpt import AutoGPT agent = AutoGPT( ai_name=\u0026#34;Research Assistant\u0026#34;, ai_role=\u0026#34;Conduct research on quantum computing breakthroughs\u0026#34;, memory=VectorMemory(), tools=[WebBrowser(), FileWriter(), Calculator(), GitHubReader()] ) # Start the agent with an initial goal agent.start( initial_goal=\u0026#34;Create a comprehensive report on recent advancements in quantum error correction\u0026#34; ) Basic Weather Agent Example Let\u0026rsquo;s look at a simpler example of how to create an LLM agent that performs a specific task: using a language model to interact with a weather API.\nimport openai import requests openai.api_key = \u0026#34;your-api-key\u0026#34; def query_language_model(prompt): response = openai.Completion.create( engine=\u0026#34;gpt-4\u0026#34;, prompt=prompt, max_tokens=100 ) return response.choices[0].text.strip() def get_weather(city): api_key = \u0026#34;your-weather-api-key\u0026#34; url = f\u0026#34;http://api.openweathermap.org/data/2.5/weather?q={city}\u0026amp;appid={api_key}\u0026#34; response = requests.get(url) weather_data = response.json() if weather_data[\u0026#34;cod\u0026#34;] == 200: main = weather_data[\u0026#34;main\u0026#34;] temperature = main[\u0026#34;temp\u0026#34;] return f\u0026#34;The current temperature in {city} is {temperature - 273.15:.2f}°C.\u0026#34; else: return \u0026#34;Sorry, I couldn\u0026#39;t fetch the weather data right now.\u0026#34; def llm_agent_task(query): # Use GPT to decide if the user query is related to weather weather_prompt = f\u0026#34;Is the following query related to weather: \u0026#39;{query}\u0026#39;\u0026#34; is_weather_query = query_language_model(weather_prompt) if \u0026#34;yes\u0026#34; in is_weather_query.lower(): # Extract city name from query (simplified extraction) city = query.split(\u0026#34;in\u0026#34;)[-1].strip() weather = get_weather(city) return weather else: return \u0026#34;Sorry, I can only help with weather-related queries right now.\u0026#34; # Example interaction user_query = \u0026#34;What is the weather in Berlin today?\u0026#34; response = llm_agent_task(user_query) print(response) In this example:\nThe agent first asks the language model if the query is weather-related If it is, the agent extracts the city name and fetches weather data from the API It then returns the weather information to the user Simple Memory Implementation A more advanced LLM agent would use memory to recall past interactions:\nclass SimpleMemory: def __init__(self): self.memory = {} def remember(self, key, value): self.memory[key] = value def recall(self, key): return self.memory.get(key, \u0026#34;I don\u0026#39;t remember that.\u0026#34;) # Example usage memory = SimpleMemory() memory.remember(\u0026#34;favorite_color\u0026#34;, \u0026#34;blue\u0026#34;) # Retrieving memory favorite_color = memory.recall(\u0026#34;favorite_color\u0026#34;) print(favorite_color) # Outputs: blue 🌟 Advanced Agent Architectures Research and development in agent architectures has produced several sophisticated approaches:\n1. Multi-Agent Systems Multiple LLM agents can collaborate, each with specialized roles:\nCritic agents that evaluate plans and outputs Expert agents with domain-specific knowledge Coordinator agents that manage task distribution Debate agents that explore different perspectives For example, a system might use one agent as a researcher, one as a writer, one as an editor, and one as a fact-checker.\n# Pseudocode for a simple multi-agent debate def multi_agent_debate(question, num_rounds=3): agents = [ create_agent(\u0026#34;Advocate\u0026#34;), create_agent(\u0026#34;Critic\u0026#34;), create_agent(\u0026#34;Mediator\u0026#34;) ] discussion = f\u0026#34;Question: {question}\\n\u0026#34; for round in range(num_rounds): for agent in agents: response = agent.generate(discussion) discussion += f\u0026#34;\\n{agent.name}: {response}\u0026#34; # Final synthesis by the mediator conclusion = agents[2].generate( f\u0026#34;{discussion}\\n\\nBased on this discussion, the final answer is:\u0026#34; ) return conclusion 2. Hierarchical Planning Complex tasks often benefit from hierarchical planning, where high-level goals are decomposed into subgoals:\ndef hierarchical_agent(goal): # High-level planning plan = llm.generate(f\u0026#34;To achieve the goal: {goal}, I should break it down into steps:\u0026#34;) steps = parse_steps(plan) results = [] for step in steps: # For each step, either decompose further or execute directly if is_complex(step): sub_result = hierarchical_agent(step) # Recursively handle complex steps results.append(sub_result) else: action_result = execute_simple_action(step) results.append(action_result) # Synthesize results into a cohesive output return synthesize_results(results) 3. Reflexion: Self-Reflection and Improvement Agents can become more effective by reflecting on their performance and learning from mistakes:\ndef reflexion_agent(query, feedback_model, max_attempts=3): attempts = [] for attempt in range(max_attempts): # Generate a response response = agent.run(query) attempts.append(response) # Self-evaluate the response reflection = feedback_model.evaluate( query=query, response=response, criteria=[\u0026#34;accuracy\u0026#34;, \u0026#34;completeness\u0026#34;, \u0026#34;reasoning\u0026#34;] ) # If the response is satisfactory, return it if reflection.score \u0026gt; 0.8: return response # Otherwise, learn from the reflection agent.update_with_feedback(reflection) # Return the best attempt according to the feedback model best_attempt = max(attempts, key=lambda a: feedback_model.evaluate(query, a).score) return best_attempt 📊 Evaluating LLM Agents Evaluating agent performance is challenging due to the complexity and diversity of tasks they might perform. Some evaluation approaches include:\nBenchmark Tasks WebShop: Testing if an agent can follow instructions to purchase items online ALFWorld: Having agents navigate text-based environments HotPotQA: Multi-step question answering requiring reasoning Evaluation Metrics Success rate: Did the agent accomplish the goal? Efficiency: How many steps or how much time was required? Autonomy: How much human intervention was needed? Exploration: How effectively did the agent explore alternatives? Robustness: How well does the agent handle unexpected situations? 🚀 Applications of LLM Agents The potential applications of LLM agents span numerous domains:\nResearch Assistants Agents can help researchers by searching literature, summarizing papers, generating hypotheses, and designing experiments.\nPersonal Assistants Agents can manage calendars, book appointments, organize information, and automate routine tasks based on user preferences.\nSoftware Development Coding agents can write, test, and debug code, as well as implement features based on specifications.\nBusiness Automation Agents can process documents, generate reports, analyze data, and automate workflows in business contexts.\nEducation and Tutoring Educational agents can provide personalized tutoring, answer questions, and adapt content to a student\u0026rsquo;s learning style.\nCustomer Support These agents can handle customer service tasks, from answering FAQs to resolving complaints and managing customer interactions.\nHealthcare LLM agents can assist in diagnosing conditions, interpreting medical records, and scheduling appointments.\n🧪 Challenges and Future Directions Despite rapid progress, several challenges remain in developing effective LLM agents:\n1. Reliability and Safety Agents need guardrails to ensure they don\u0026rsquo;t:\nTake harmful actions Execute unintended operations Leak sensitive information Generate misleading content 2. Scalability Current approaches often struggle with:\nVery long-term planning Complex multi-step tasks Large-scale knowledge integration Computational efficiency 3. Integration with Specialized Tools Building agents that can effectively:\nInterface with domain-specific software Control robotic systems Work with specialized scientific tools Handle multimodal inputs and outputs 4. Ambiguity in Queries LLM agents often face difficulties when handling ambiguous queries. Without additional context, they may generate incorrect or irrelevant responses.\n5. Resource Consumption Running LLM agents, especially those that require continuous interaction with large models and external services, can be resource-intensive.\n6. Ethical Concerns There are ethical implications regarding the use of LLM agents in decision-making, privacy concerns, and the potential for misuse. Responsible AI practices and transparency in the model\u0026rsquo;s actions are essential.\n💼 Building Your Own Agent: A Practical Example Let\u0026rsquo;s build a simple research agent that can search for information and summarize findings:\nimport os from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser from langchain.prompts import StringPromptTemplate from langchain import OpenAI, SerpAPIWrapper, LLMChain from typing import List, Union, Dict, Any import re # Set up search tool search = SerpAPIWrapper() tools = [ Tool( name=\u0026#34;Search\u0026#34;, func=search.run, description=\u0026#34;useful for when you need to search for information on the internet\u0026#34; ) ] # Set up the prompt template template = \u0026#34;\u0026#34;\u0026#34;You are a research assistant. You have access to the following tools: {tools} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Question: {input} Thought: \u0026#34;\u0026#34;\u0026#34; class CustomPromptTemplate(StringPromptTemplate): template: str tools: List[Tool] def format(self, **kwargs) -\u0026gt; str: intermediate_steps = kwargs.pop(\u0026#34;intermediate_steps\u0026#34;, []) thoughts = \u0026#34;\u0026#34; for action, observation in intermediate_steps: thoughts += action.log thoughts += f\u0026#34;\\nObservation: {observation}\\nThought: \u0026#34; kwargs[\u0026#34;tools\u0026#34;] = \u0026#34;\\n\u0026#34;.join([f\u0026#34;{tool.name}: {tool.description}\u0026#34; for tool in self.tools]) kwargs[\u0026#34;tool_names\u0026#34;] = \u0026#34;, \u0026#34;.join([tool.name for tool in self.tools]) return self.template.format(**kwargs) + thoughts prompt = CustomPromptTemplate( template=template, tools=tools, input_variables=[\u0026#34;input\u0026#34;, \u0026#34;intermediate_steps\u0026#34;] ) class CustomOutputParser(AgentOutputParser): def parse(self, llm_output: str) -\u0026gt; Union[Dict[str, Any], str]: if \u0026#34;Final Answer:\u0026#34; in llm_output: return {\u0026#34;output\u0026#34;: llm_output.split(\u0026#34;Final Answer:\u0026#34;)[-1].strip()} regex = r\u0026#34;Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\u0026#34; match = re.search(regex, llm_output, re.DOTALL) if not match: return {\u0026#34;output\u0026#34;: llm_output} action = match.group(1).strip() action_input = match.group(2).strip(\u0026#34; \u0026#34;) return {\u0026#34;action\u0026#34;: action, \u0026#34;action_input\u0026#34;: action_input} output_parser = CustomOutputParser() llm = OpenAI(temperature=0) llm_chain = LLMChain(llm=llm, prompt=prompt) agent = LLMSingleActionAgent( llm_chain=llm_chain, output_parser=output_parser, stop=[\u0026#34;\\nObservation:\u0026#34;], allowed_tools=[tool.name for tool in tools] ) agent_executor = AgentExecutor.from_agent_and_tools( agent=agent, tools=tools, verbose=True ) # Example usage result = agent_executor.run(\u0026#34;What are the latest developments in quantum computing?\u0026#34;) print(result) 🧠 Final Thoughts LLM agents represent a step change in AI capabilities, moving from models that merely respond to queries to systems that can actively pursue goals. While we\u0026rsquo;re still in the early days of this technology, the rapid progress suggests we\u0026rsquo;re approaching an era of increasingly autonomous and capable AI assistants.\nThe most effective agents will likely combine the strengths of LLMs—reasoning, world knowledge, and flexibility—with specialized tools and robust planning frameworks. As these technologies mature, we can expect to see agents that can tackle increasingly complex and open-ended tasks, from scientific research to creative projects and beyond.\nBuilding effective agents remains a challenging engineering problem, balancing power with reliability, autonomy with control, and capability with safety. The frameworks and approaches outlined in this post provide a starting point, but there\u0026rsquo;s still significant room for innovation in the design and implementation of these systems.\nWith advancements in large language models and AI technology, the potential for LLM agents will continue to expand, bringing us closer to fully autonomous, intelligent systems that can understand our needs and act on our behalf with increasing competence and reliability.\n— Akshat\n","permalink":"https://akshat4112.github.io/posts/llm-agents/","summary":"Understanding LLM Agents: The Future of Autonomous AI Systems Large Language Models (LLMs), like GPT-3, GPT-4, and others, have taken the world by storm due to their impressive language generation and understanding capabilities. However, when these models are augmented with decision-making capabilities, memory, and actions in specific environments, they become even more powerful. Enter LLM Agents — autonomous systems built on top of large language models to perform tasks, make decisions, and act autonomously based on user instructions.","title":"LLM Agents: Building AI Systems That Can Reason and Act"},{"content":"I\u0026rsquo;m a machine learning engineer and my research interests are in generative modelling, speech processing, text analysis, machine learning, and deep learning. Currently pursuing a master\u0026rsquo;s from the University of Stuttgart, my major is in computational linguistics.\nCurrent work Research work includes denoising diffusion on speaker embeddings, explainability of CRF-LSTM-based NER models, temporal and spatial word embeddings, and homoglyph detection using attention-based CNNs.\nMy work includes Before this, I worked at Quantiphi, Scanta, and Mobile Programming LLC as a machine learning engineer and worked on speaker diarization, multimodal sentiment analysis, NLP augmentor, clinical ner, chatbot using AWS Lex and Dialogflow, customer care call analytics, neural machine translation.\nBackground and history Previously I was an undergrad at APJ Abdul Kalam Technical University studying computers and maths. my major project was on \u0026ldquo;Automatic Catalogue Tagging using Deep Learning\u0026rdquo;. Other minor projects include an online examination system, school ERP, seawater quality monitoring system, and, online certificate generator. I\u0026rsquo;m from Agra and currently living in Stuttgart.\nOther Works Intel Software Innovator AAAI Reviewer Member of the Board of Studies at OP Jindal University Author with Packt publications Kaggle 3x expert, Top 1% of 300k+ Kaggle users Interests Table Tennis, Chess, Financial Markets, Quantum Physics ","permalink":"https://akshat4112.github.io/about/","summary":"I\u0026rsquo;m a machine learning engineer and my research interests are in generative modelling, speech processing, text analysis, machine learning, and deep learning. Currently pursuing a master\u0026rsquo;s from the University of Stuttgart, my major is in computational linguistics.\nCurrent work Research work includes denoising diffusion on speaker embeddings, explainability of CRF-LSTM-based NER models, temporal and spatial word embeddings, and homoglyph detection using attention-based CNNs.\nMy work includes Before this, I worked at Quantiphi, Scanta, and Mobile Programming LLC as a machine learning engineer and worked on speaker diarization, multimodal sentiment analysis, NLP augmentor, clinical ner, chatbot using AWS Lex and Dialogflow, customer care call analytics, neural machine translation.","title":"About"},{"content":"","permalink":"https://akshat4112.github.io/publication/","summary":"","title":"GlyphNet: Homoglyph domains dataset and detection using attention-based Convolutional Neural Networks"}]